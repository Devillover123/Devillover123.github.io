<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习</title>
      <link href="/posts/29145/"/>
      <url>/posts/29145/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a><p align="center">深度学习</p></h1><h2 id="1、单个网络层上的前向传播"><a href="#1、单个网络层上的前向传播" class="headerlink" title="1、单个网络层上的前向传播"></a>1、单个网络层上的前向传播</h2><p>​阅读代码，理解内容，继续使用咖啡烘培模型</p><p>​<strong>前向传播参数随机初始化，后面进行反向传播调参修正。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308061730483.webp" alt="image-20230806173016220"></p><blockquote><p>​w2_1 &#x3D; np.array ( [-7，8，<strong>1</strong>] )        #应该为3个元素的 1D</p><p>​               –</p><p>​a2_1 &#x3D; sigmod ( z2_1 )   # z2_1</p></blockquote><p>​</p><p>​这就是如何使用<code>python</code>和<code>numpy</code>实现前向传播。</p><h2 id="2、前向传播的一般实现"><a href="#2、前向传播的一般实现" class="headerlink" title="2、前向传播的一般实现"></a>2、前向传播的一般实现</h2><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308062044889.webp" alt="image-20230806204441625"></p><p>​</p><blockquote><ol><li><p><code>units = W.shape[1]</code>: 获取权重矩阵 <code>W</code> 的输出维度（神经元数量），保存在变量 <code>units</code> 中。</p><p>通过将 <code>w.shape[1]</code> 赋值给 <code>units</code>，你可以根据权重矩阵 <code>w</code> 的形状自动设置神经网络层的输出维度，使其与权重矩阵的列数相匹配。这样在构建神经网络时就不需要手动指定输出维度，更加方便和灵活。</p></li><li><p><code>a_out = np.zeros(units)</code>: 创建一个一维数组 <code>a_out</code>，用于保存输出结果，初始值全部设为 0。</p></li><li><p><code>for j in range(units):</code>: 遍历每个输出神经元的索引 <code>j</code>。</p></li><li><p><code>w = W[:, j]</code>: 从权重矩阵 <code>W</code> 中获取第 <code>j</code> 列，即与第 <code>j</code> 个神经元连接的权重向量。</p></li><li><p><code>z = np.dot(w, a_in) + b[j]</code>: 计算加权输入 <code>z</code>，使用权重向量 <code>w</code> 与输入向量 <code>a_in</code> 的点积，并加上对应的偏置 <code>b[j]</code>。</p></li><li><p><code>a_out[j] = g(z)</code>: 将加权输入 <code>z</code> 经过激活函数 <code>g</code> 进行非线性变换，得到第 <code>j</code> 个输出神经元的输出值，并保存在 <code>a_out[j]</code> 中。</p></li><li><p>循环结束后，<code>a_out</code> 中存储了整个层的输出结果，即一个由 <code>units</code> 个元素组成的一维数组，表示该层所有神经元的输出。</p></li></ol></blockquote><p>​</p><p>​后面的代码时将几个隐藏层串在一起，以便在神经网络中实现前向传播。</p><p>​<strong>这里使用   W  ,因为根据线性代数，矩阵使用大写字母，小写字母代表向量和标量。</strong></p><p>​       这就是底层工作原理， <code>在实验室中练习。</code></p><hr><p>​神经网络与AI、AGI、通用人工智能之间是什么关系？</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308062119965.webp" alt="image-20230806211930637"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deeplearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习</title>
      <link href="/posts/29144/"/>
      <url>/posts/29144/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a><p align="center">深度学习</p></h1><h2 id="1、如何用代码实现推理"><a href="#1、如何用代码实现推理" class="headerlink" title="1、如何用代码实现推理"></a>1、如何用代码实现推理</h2><p>​神经网络的一个显着特点是可以将相同的算法应用于许多不同的应用程序。下面使用一个例子来说明推理。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051709246.webp" alt="image-20230805170902992"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051717349.webp" alt="image-20230805171740094"></p><p>​这行代码是使用 Keras API 创建一个神经网络的一层。在这个例子中，它创建了一个具有 3 个神经元的全连接层（也称为稠密层），并且使用 sigmoid 激活函数。</p><ul><li><p><code>Dense</code>：这是 Keras 中创建全连接层的类。</p></li><li><p><code>units=3</code>：指定这一层有 3 个神经元。这意味着该层的输出将是一个包含 3 个值的向量。</p></li><li><p><code>activation=&#39;sigmoid&#39;</code>：这是设置该层的激活函数，这里使用的是 sigmoid 函数。Sigmoid 函数将输入的值映射到 0 到 1 之间的范围，通常用于处理二分类问题。</p><p>以下是一个简单的示例，演示如何使用这个层来构建一个简单的 Keras 模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 Sequential 模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加第一层</span></span><br><span class="line">model.add(Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, input_shape=(input_dim,)))  <span class="comment"># 替换 input_dim 为你的输入数据维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加其他层（这里省略）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型并进行训练</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">10</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上代码只是一个简单示例，实际构建的模型可能更复杂，并且在编译和训练模型时需要使用适当的优化器、损失函数和评估指标。</p><p>这就是使用TensorFlow在神经网络中进行推理的方式。</p></li></ul><hr><p>​例子：手写数字分类问题</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051728039.webp" alt="image-20230805172849814"></p><h2 id="2、Tensorflow-中的数据形式"><a href="#2、Tensorflow-中的数据形式" class="headerlink" title="2、Tensorflow 中的数据形式"></a>2、Tensorflow 中的数据形式</h2><p>​在  <strong>Numpy</strong>  和  <strong>TensorFlow</strong>  中表示数据，  <strong>Numpy</strong>  和<strong>TensorFlow</strong>  中的数据表示方式存在一些不一致。</p><p>​先看在  <strong>TensorFlow</strong>  中表示数据：利用上次的  <strong>coffee</strong>  数据，<strong>为什么是双方括号？</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051749685.webp" alt="image-20230805174906489"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051932344.webp" alt="image-20230805193231104"></p><p>​矩阵维数为行数<em>列数，每行数据一个</em>*[ ]<strong>，整个矩阵表示完后加</strong>（[ ]）**。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051933971.webp" alt="image-20230805193326774"></p><p>​搞清楚生成的矩阵为什么样的形式。行向量，列向量，单个方括号则表示一维向量，没有行或者列的一维数组。一维向量可以用于表示一组单一维度的数据，例如：</p><ul><li><p>温度的时间序列数据：[25.5, 26.3, 24.8, 26.0, …]</p></li><li><p>学生的成绩：[90, 85, 78, 92, …]</p></li><li><p>产品的价格：[10.5, 15.2, 12.0, 9.8, …]</p><blockquote><p>在编程语言中，一维数组通常是一种简单的线性数据结构，可以通过列表（list）来表示。二维数组通常是嵌套的一维数组，可以通过列表的列表（list of lists）来表示。例如，Python中的列表可以用于表示一维数组和二维数组。</p><ol><li>一维数组（一维向量）：<ul><li>定义：一维数组是一组按顺序排列的数据元素，每个元素都有唯一的索引。</li><li>表示：一维数组是一个线性结构，数据按照单一维度组织，并且可以用一个索引来访问其中的每个元素。</li><li>示例：[1, 2, 3, 4, 5] 或者 [‘apple’, ‘banana’, ‘orange’]</li></ul></li><li>二维数组（二维矩阵）：<ul><li>定义：二维数组是一组按行和列排列的数据元素，每个元素由两个索引（行索引和列索引）来确定。</li><li>表示：二维数组是一个矩形结构，数据按照行和列的方式组织，每个元素需要两个索引来定位。</li><li>示例：[[1, 2, 3], [4, 5, 6], [7, 8, 9]] 或者 [[‘a’, ‘b’, ‘c’], [‘d’, ‘e’, ‘f’]]</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维数组（列表）</span></span><br><span class="line">one_dimensional_array = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组（列表的列表）</span></span><br><span class="line">two_dimensional_array = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在数据分析、机器学习和深度学习中，二维数组常用于表示数据集、图像、矩阵等数据结构。一维数组通常用于表示单一维度的特征向量或标签。</p></blockquote></li></ul><p>​</p><p>​<strong>处理线性回归和逻辑回归的课程中，使用一维向量来表示输入特征  <em>x。</em></strong></p><p>​<strong>TensorFlow</strong>  中表示数据的惯例是使用矩阵来表示数据， 事实证明，<strong>TensorFlow</strong>  旨在处理非常大的数据集，并且通过在矩阵而不是一维数组中表示数据，它让<strong>TensorFlow</strong>  在内部的计算效率更高一些。</p><p>​所以：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051954453.webp" alt="image-20230805195427271"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051959072.webp" alt="image-20230805195912840"></p><p>​</p><blockquote><ol><li><code>x = np.array([[200.0, 17.0]])</code>：这行代码创建了一个包含两个元素的二维 NumPy 数组，并将其赋值给变量 <code>x</code>。这个数组实际上是一个包含两个浮点数的一维向量（或一维数组）。注意，这里的代码应该是没有问题的，我之前提到的问题已经修复了。</li><li><code>layer_1 = Dense(units=3, activation=&#39;sigmoid&#39;)</code>：这行代码定义了一个全连接层（Dense Layer），该层有 3 个神经元，并使用 Sigmoid 激活函数。这里的 <code>units</code> 参数表示该层的输出维度，即输出具有 3 个值的向量。</li><li><code>a1 = layer_1(x)</code>：这行代码将输入向量 <code>x</code> 通过 <code>layer_1</code> 这个定义好的神经网络层进行前向传播。这就是将输入数据通过神经网络进行处理得到输出的过程。<code>a1</code> 将保存经过 <code>layer_1</code> 处理后的输出结果。</li><li><code>tf.Tensor([[0.2 0.7 0.3]], shape=(1, 3), dtype=float32)</code>：这是代码的输出结果。这表示经过前向传播后，<code>layer_1</code> 处理输入向量 <code>x</code> 后得到的输出向量 <code>a1</code>。这里的输出向量 <code>a1</code> 是一个包含三个浮点数的一维向量，具体数值是 <code>[0.2, 0.7, 0.3]</code>。</li></ol></blockquote><p>​</p><p><strong>Tensor</strong>：张量，这里的张量是TensorFlow团队为了有效地存储和执行矩阵计算而创建的一种数据类型。</p><blockquote><p><code>tf.Tensor</code> 是 TensorFlow 中表示张量（tensor）的数据类型。张量是多维数组，可以看作是标量（0 维）、向量（1 维）、矩阵（2 维）等的扩展。</p><p>在 TensorFlow 中，所有的数据都以张量的形式进行处理。张量可以包含任意维度的数据，并且可以存储数值、字符串等类型的数据。它是 TensorFlow 的核心数据结构之一，用于表示计算图中的数据流动。</p><p><code>tf.Tensor</code> 是不可变的，意味着创建后无法直接修改其值。在 TensorFlow 中，所有的操作（例如张量运算）都会返回新的 <code>tf.Tensor</code> 对象，而不会修改原始张量。</p></blockquote><p>​事实上，如果你想获取张量a1并将其转换回NumPy数组，你可以使用函数  <strong>a1.numpy</strong>  来实现。它将获取相同的数据并以NumPy数组的形式返回，而不是以TensorFlow数组或TensorFlow矩阵的形式返回。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052013348.webp" alt="image-20230805201312150"></p><h2 id="3、搭建一个神经网络"><a href="#3、搭建一个神经网络" class="headerlink" title="3、搭建一个神经网络"></a>3、搭建一个神经网络</h2><ol><li>顺序框架张量流（Sequential Model in TensorFlow）是 TensorFlow 中一种简单且常用的神经网络模型构建方式。它是 TensorFlow 的高级 API，旨在帮助用户快速构建神经网络模型，特别适用于那些层按顺序堆叠的简单模型。</li><li>顺序框架张量流的主要特点是它按照顺序一层一层地添加神经网络层，形成一个线性的模型结构。每一层都连接到上一层，并且数据从第一层流动到最后一层。这种顺序的层叠结构使得模型的构建和理解更加直观和简单。</li></ol><p>​以下是一个简单的例子，展示了如何使用顺序框架张量流构建一个简单的全连接神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建顺序框架</span></span><br><span class="line">model = tf.keras.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加第一层（输入层）</span></span><br><span class="line">model.add(Dense(units=<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(input_dim,)))  <span class="comment"># 替换 input_dim 为输入数据的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加第二层</span></span><br><span class="line">model.add(Dense(units=<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加输出层</span></span><br><span class="line">model.add(Dense(units=num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>))  <span class="comment"># 替换 num_classes 为分类类别的数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​上述代码中，我们创建了一个顺序框架 <code>model</code>，并按照顺序添加了三个全连接层（输入层、隐藏层和输出层）。最后，<strong>我们通过 <code>compile</code> 方法对模型进行编译，指定优化器、损失函数和评估指标，以便在训练时进行模型优化和评估。</strong></p><p>​通过顺序框架张量流，我们可以快速构建和训练神经网络模型，适用于很多简单的深度学习任务。对于更复杂的模型结构或需要非顺序连接的情况，可以使用函数式 API 或子类化 API 来构建定制化的模型。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052049104.webp" alt="image-20230805204953874"></p><ul><li>如果你想训练这个神经网络，你需要做的就是调用你需要用一些参数调用模型点编译的函数   <em><strong>compile</strong></em>   。</li><li><strong>fit</strong>  告诉张量流采用这个神经网络在数据  <strong>x</strong>  上对  <strong>y</strong>  进行训练。</li><li><strong>predict</strong>  进行预测。</li></ul><p>​模型预测使用顺序函数编译的神经网络进行前向传播并为您进行推理。</p><hr><p>​为数字分类示例重做此操作：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052102013.webp" alt="image-20230805210237748"></p><p>​<strong>通常可以将隐藏层直接放入顺序函数中</strong>：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052103009.webp" alt="image-20230805210336777"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052106426.webp" alt="image-20230805210654045"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deeplearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习</title>
      <link href="/posts/29143/"/>
      <url>/posts/29143/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a><p align="center">深度学习</p></h1><h2 id="1、神经元与大脑"><a href="#1、神经元与大脑" class="headerlink" title="1、神经元与大脑"></a>1、神经元与大脑</h2><p>​一些废话：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022202498.webp" alt="image-20230802220229291"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022203876.webp" alt="image-20230802220306686"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022203564.webp" alt="image-20230802220323363"></p><h2 id="2、需求检测"><a href="#2、需求检测" class="headerlink" title="2、需求检测"></a>2、需求检测</h2><p>​eg：销售T恤，想知道一件特定的T恤是否会成为畅销品。以最简单的逻辑回归为例。</p><p>​a：activation，激活。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022211456.webp" alt="image-20230802221110210"></p><p>​鉴于对单个神经元的这种描述，构建神经网络只需要将这些神经元串在一起并将它们连接在一起。</p><p>​以下复杂案例：输入为四个特征；但T恤是否畅销实际取决于 </p><ul><li>负担能力： <em><strong>affordability</strong></em>   </li><li>认可度：   <em><strong>awareness</strong></em></li><li>感知质量：   <em><strong>perceive quality</strong></em></li></ul><p>​创建一个人工神经网络来估计这件T恤被认为非常实惠的概率。其中   <em><strong>affordability</strong></em>   是   <em><strong>price</strong></em>   与  <em><strong>shipping cost</strong></em>  的函数。 这里使用一个小神经元，一个逻辑回归单元来输入   <em><strong>price</strong></em>   和  <em><strong>shipping cost</strong></em>  来预测    <em><strong>affordability</strong></em>；同理， <em><strong>awareness</strong></em>   主要是   <em><strong>marketing</strong></em>   的函数。   <em><strong>perceive quality</strong></em>   是   <em><strong>material</strong></em>   与  <em><strong>price</strong></em>   的函数。最后将三个输出的数字作为输入连接到右侧的一个逻辑回归单元的神经元，并输出这件T恤成为畅销品的概率。</p><blockquote><p><strong>中间的三个神经元组合在一起，形成层，称为隐藏层。右边的单个神经元也是一层，称为输出层。</strong></p><p>在神经网络的术语中，将 <em><strong>affordability</strong></em> 、 <em><strong>awareness</strong></em>、<em><strong>perceive quality</strong></em> 称为<strong>激活</strong>。所代表的数字为相应神经元的激活值。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022227563.webp" alt="image-20230802222720365"></p><hr><p>​<strong>神经网络所做的是不需要我们自己手动设计特征，它可以学习其要制作的功能。</strong>尽管之前将此神经网络描述为： <em><strong>affordability</strong></em> 、 <em><strong>awareness</strong></em>、<em><strong>perceive quality</strong></em>，但神经网络应该会自行计算出它想在隐藏层中使用的特征是哪些。多层神经网络称为<strong>多层感知器</strong>。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308031701601.webp" alt="image-20230803170154420"></p><p>​<strong>当构建自己的神经网络时，需要做出的决定之一是想要多少个隐藏层以及希望每个隐藏层有多少个神经元。即神经网络要选择合适的架构。选择正确的隐藏层数和每层隐藏单元数也会对学习算法的性能产生影响。</strong></p><h2 id="3、图像感知（数字图像处理）"><a href="#3、图像感知（数字图像处理）" class="headerlink" title="3、图像感知（数字图像处理）"></a>3、图像感知（数字图像处理）</h2><p>​人脸识别的应用程序，需要训练一个神经网络将图片作为输入，人名作为输出。</p><p>​输入为百万像素亮度值的特征向量，输出为人名。</p><blockquote><p>图为吴恩达老师的老婆</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041513381.webp" alt="image-20230804151308079"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041545360.webp" alt="image-20230804154526002"></p><p>​在神经网络的最早层中，可能会发现神经元正在寻找图像中非常短的线或非常短的边缘。如果查看下一个隐藏层，会发现这些神经元可能会学习将许多小的短线和小的短边段组合在一起，以寻找面部的各个部分。当查看此示例中的下一个隐藏层时，神经网络会聚合面部的不同部分，然后尝试检测是否存在更大、更粗糙的面部形状。</p><h2 id="4、神经网络中的网络层"><a href="#4、神经网络中的网络层" class="headerlink" title="4、神经网络中的网络层"></a>4、神经网络中的网络层</h2><p>​具体数学和实现细节关于构建一个或多个神经网络层。依旧选择需求检测中的示例，放大隐藏层查看其计算。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041613506.webp" alt="image-20230804161357270"></p><ul><li><p>隐藏层输入为四个数字，三个神经元每个都是一个小逻辑回归单元。以第一个神经元为例，有两个参数    <em><strong>w</strong></em>   和   <em><strong>b</strong></em>   ，下标表示为第几个隐藏单元，   <em><strong>a</strong></em>   为激活值，下标表示第几个激活值。激活   <em><strong>a</strong></em>   向量由三个激活值构成。然后传递到输出层，成为输出层的输入。</p></li><li><p>构建多层神经网络时，为各层指定不同的数字。形成 <strong>[n]</strong>  上表，<strong>n</strong>  表示神经网络层数。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041628082.webp" alt="image-20230804162805875"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041628216.webp" alt="image-20230804162854034"></p><p>​<strong>每一层输入数字向量并对其应用一堆逻辑回归单元，然后计算另一个数字向量，然后从一层传递到另一层，直到你到达最终的输出层计算，这就是神经网络的原理。然后可以设置阈值，得出最终预测。</strong></p><h2 id="5、复杂的神经网络"><a href="#5、复杂的神经网络" class="headerlink" title="5、复杂的神经网络"></a>5、复杂的神经网络</h2><p>​为了更加清楚的了解神经网络的符号。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041850938.webp" alt="image-20230804185043652"></p><p>​当我们描述一个神经网络为四层时，它包括输出层和所有隐藏层，不包括输入层。</p><p>​相信对我们来说，这节的知识很容易理解。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042016788.webp" alt="image-20230804201615524"></p><h2 id="6、神经网络的前向传播"><a href="#6、神经网络的前向传播" class="headerlink" title="6、神经网络的前向传播"></a>6、神经网络的前向传播</h2><p>​让我们将我们学到的东西组合成一个算法，让神经网络进行推理或预测，称为前向传播算法。</p><p>​将使用手写数字识别作为一个激励性的例子。为了简单，只区分0和1。</p><p>​分析：这是一个二位分类的问题，我们要输入图像进行分类，区分0和1。<strong>实验室可以自己玩一下。</strong></p><p>​使用8*8即64像素强度的网格或矩阵，255白色，0黑色，不同数字即灰度。鉴于64个输入特征，使用具有两个隐藏层的神经网络。其中第一个隐藏层由25个神经元，第二个隐藏层有15个神经元，最后输出层一个神经元输出这是 “ 1 ” 的概率。以下过程按照所学展开推导：</p><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042031152.webp" alt="image-20230804203118903"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042033904.webp" alt="image-20230804203301644"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042034562.webp" alt="image-20230804203430374"></p><p>​</p><p>​进行到第三层，此时的   <em><strong>a3</strong></em>  是标量，设置其阈值为4.5以得出二位分类标签。所以计算顺序先取   <em><strong>x</strong></em>   然后计算   <em><strong>a1</strong></em>   ，然后计算   <em><strong>a2</strong></em>   ，再计算   <em><strong>a3</strong></em>   ，这也是神经网络的输出。也可写为   <em><strong>f(x)</strong></em>   , 当我们学习线性回归和逻辑回归时，我们使用   <em><strong>f(x)</strong></em>  来表示线性回归或逻辑回归的输出。</p><p>​因为这个激活计算过程是从左到右进行，从  <em><strong>x</strong></em>   到   <em><strong>a1</strong></em>   、   <em><strong>a2</strong></em>   、   <em><strong>a3</strong></em>   。所以又称<strong>前向传播</strong>。<strong>这与用于学习的称为反向传播或反向传播的不同算法形成对比。</strong></p><p>​顺便说一句，这种类型的神经网络架构最初有更多的隐藏单元，然后随着离输出层越来越近，隐藏单元的数量会减少。</p><p>​<strong>以上就是前向传播算法的神经网络推理。有了这个，你就可以下载别人训练过并发布在互联网上的神经网络参数。你将能够使用他们的神经网络对你的新数据进行推理。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042046047.webp" alt="image-20230804204559700"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deeplearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29142/"/>
      <url>/posts/29142/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="1、过拟合问题"><a href="#1、过拟合问题" class="headerlink" title="1、过拟合问题"></a>1、过拟合问题</h2><ul><li>模型对训练数据 <strong>欠拟合</strong> (  <em><strong>underfit</strong></em>  ) or 算法具有 <strong>高偏差</strong>（ <em><strong>high bias</strong></em> ）</li><li>希望  <em><strong>generalization</strong></em> :  <strong>泛化</strong></li><li><strong>过拟合</strong>（ <em><strong>overfitting</strong></em> ） or  <strong>高方差</strong>（ <em><strong>high variance</strong></em> ）</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291641912.webp" alt="image-20230729164112592"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291650553.webp" alt="image-20230729165008193"></p><h2 id="2、解决过拟合问题"><a href="#2、解决过拟合问题" class="headerlink" title="2、解决过拟合问题"></a>2、解决过拟合问题</h2><ol><li>收集更多的训练集数据。</li><li>不要使用太多特征，使用特征的子集即选择一部分最合适的特征。</li><li><em><strong>Regularization</strong></em>  : 正则化</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291657447.webp" alt="image-20230729165758247"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291658119.webp" alt="image-20230729165852938"></p><h2 id="3、正则化"><a href="#3、正则化" class="headerlink" title="3、正则化"></a>3、正则化</h2><ul><li><p>正则化是一种更<strong>温和地减少某些特征影响</strong>的方法，而不像彻底消除它那样严厉。</p></li><li><p>正则化的作用是<strong>鼓励学习算法缩小参数值</strong>，而不是把参数变为0。</p></li></ul><p>​基于正则化为学习算法设计的代价函数，通过调整代价函数来调整参数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291711733.webp" alt="image-20230729171147458"></p><p>​<strong>正则化的典型实现方式是惩罚所有的特征，惩罚所有的 <em>w</em> 参数</strong>。最后一项可以忽略。</p><ul><li><em><strong>λ</strong></em> 设置为0，则根本没有使用正则化项。</li><li><em><strong>λ</strong></em> 设置非常大，参数非常接近0，例如对于线性回归模型则   <em><strong>f&#x3D;b</strong></em>   ，拟合水平直线，欠拟合。</li></ul><h2 id="4、用于线性回归的正则方法"><a href="#4、用于线性回归的正则方法" class="headerlink" title="4、用于线性回归的正则方法"></a>4、用于线性回归的正则方法</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292206887.webp" alt="image-20230729220649628"></p><blockquote><p><strong>注意：正则化项求偏导后无求和符号。</strong></p><p>求导是对某一项的参数求导，此时其他参数为常数。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292212627.webp" alt="image-20230729221220333"></p><h3 id="高数"><a href="#高数" class="headerlink" title="高数"></a><p align="center">高数</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292217945.webp" alt="image-20230729221710700"></p><h2 id="5、用于逻辑回归的正则化方法"><a href="#5、用于逻辑回归的正则化方法" class="headerlink" title="5、用于逻辑回归的正则化方法"></a>5、用于逻辑回归的正则化方法</h2><p><strong><p align="center">正则化逻辑与线性回归相似</p></strong></p><p>​修改代价函数实现正则化：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292222803.webp" alt="image-20230729222206533"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292224550.webp" alt="image-20230729222403346"></p><p>​<strong>完全相同的方程，除了   <em>f</em>   的定义不再是线性函数，而是应用于   <em>z</em>   的逻辑函数。</strong></p><p>​参考代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 马上学</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292229255.webp" alt="image-20230729222925905"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29141/"/>
      <url>/posts/29141/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><p>​分类：输出变量为少数可能值中的一个，而不是无限范围内的任何数字。</p><h3 id="1-2逻辑回归（用于二元分类）"><a href="#1-2逻辑回归（用于二元分类）" class="headerlink" title="1.2逻辑回归（用于二元分类）"></a>1.2逻辑回归（用于二元分类）</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061024791.webp" alt="image-20230706102408627"></p><p>​Sigmoid函数是一个在生物学中常见的<a href="https://baike.baidu.com/item/S%E5%9E%8B%E5%87%BD%E6%95%B0/19178062?fromModule=lemma_inlink">S型函数</a>，也称为<a href="https://baike.baidu.com/item/S%E5%9E%8B%E7%94%9F%E9%95%BF%E6%9B%B2%E7%BA%BF/5581189?fromModule=lemma_inlink">S型生长曲线</a>。 在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的<a href="https://baike.baidu.com/item/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2520792?fromModule=lemma_inlink">激活函数</a>，将变量映射到0,1之间。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061059720.webp" alt="image-20230706105911494"></p><p>​<strong>多变量整合为单变量，再用sigmoid分类</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061104571.webp" alt="image-20230706110452346"></p><h3 id="1-3逻辑回归、决策边界"><a href="#1-3逻辑回归、决策边界" class="headerlink" title="1.3逻辑回归、决策边界"></a>1.3逻辑回归、决策边界</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081441602.webp" alt="image-20230708144100042"></p><h3 id="2-1逻辑回归中的代价函数"><a href="#2-1逻辑回归中的代价函数" class="headerlink" title="2.1逻辑回归中的代价函数"></a>2.1逻辑回归中的代价函数</h3><p>​<strong>代价函数提供了一种衡量特定参数与训练数据拟合程度的方法</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081448958.webp" alt="image-20230708144831744"></p><hr><p>​<strong>单个训练示例的损失函数</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101450133.webp" alt="image-20230710145008957"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081510170.webp" alt="image-20230708151030003"></p><p>​<em>f</em>  的值总是在0-1之间，越接近于1，损失越小。<strong>上图损失函数漏掉了一个负号</strong></p><hr><p>​<strong>代价函数是整个训练集的函数，因此是单个训练示例的损失函数之和的平均值</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081519147.webp" alt="image-20230708151933785"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes cost</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)) : target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)) : model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)       : model parameter</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      cost (scalar): cost</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">m = X.shape[<span class="number">0</span>]</span><br><span class="line">cost = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    z_i = np.dot(X[i],w) + b</span><br><span class="line">    f_wb_i = sigmoid(z_i)</span><br><span class="line">    cost +=  -y[i]*np.log(f_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-f_wb_i)</span><br><span class="line">         </span><br><span class="line">cost = cost / m</span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="2-2简化逻辑回归代价函数"><a href="#2-2简化逻辑回归代价函数" class="headerlink" title="2.2简化逻辑回归代价函数"></a>2.2简化逻辑回归代价函数</h3><p>​目的：使用梯度下降来拟合逻辑回归模型的参数时，实现简单。</p><p>​二元<strong>交叉熵损失函数</strong>，多分类只需要多乘几个多项式即可</p><p> <img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101459266.webp" alt="image-20230710145927025"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101504651.webp" alt="image-20230710150413331"></p><p>​这个特定的成本函数是使用最大似然估计的统计原理推导出来的，这是统计学中关于如何有效的找到不同模型的参数的想法。<strong>它有一个很好的特性，它是凸的。</strong></p><h3 id="3-1实现梯度下降"><a href="#3-1实现梯度下降" class="headerlink" title="3.1实现梯度下降"></a>3.1实现梯度下降</h3><p>​为了拟合逻辑回归模型的参数，将尝试找到使 <em>w</em> 和 <em>b</em> 的成本  <em>J</em>  最小的参数值，使用梯度下降来实现。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121410281.webp" alt="image-20230712141007965"></p><p>​<strong>与线性回归模型类似，注意同时更新。</strong></p><p>推导过程：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121500696.webp" alt="image-20230712150021478"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121519309.webp" alt="image-20230712151912073"></p><p>​<strong>尽管线性回归和逻辑回归所写的算法看起来相同，但实际上它们是两种截然不同的算法，因为 <em>f</em> 的定义不同。</strong></p><p>​<strong>ps：上述逻辑回归的 <em>f</em> 的负号位置错了</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121526385.webp" alt="image-20230712152606243"></p><hr><p>​<strong>梯度函数实现代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the gradient for linear regression </span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)): target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)): model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)      : model parameter</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. </span></span><br><span class="line"><span class="string">      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))                           <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i],w) + b)          <span class="comment">#(n,)(n,)=scalar</span></span><br><span class="line">        err_i  = f_wb_i  - y[i]                       <span class="comment">#scalar</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      <span class="comment">#scalar</span></span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw/m                                   <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = dj_db/m                                   <span class="comment">#scalar</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw  </span><br></pre></td></tr></table></figure><p>​<strong>检查梯度函数的实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_tmp = np.array([[<span class="number">0.5</span>, <span class="number">1.5</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1.5</span>, <span class="number">0.5</span>], [<span class="number">3</span>, <span class="number">0.5</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2.5</span>]])</span><br><span class="line">y_tmp = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">w_tmp = np.array([<span class="number">2.</span>,<span class="number">3.</span>])</span><br><span class="line">b_tmp = <span class="number">1.</span></span><br><span class="line">dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dj_db: <span class="subst">&#123;dj_db_tmp&#125;</span>&quot;</span> )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dj_dw: <span class="subst">&#123;dj_dw_tmp.tolist()&#125;</span>&quot;</span> )</span><br></pre></td></tr></table></figure><p>​<strong>结果为：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dj_db: <span class="number">0.49861806546328574</span></span><br><span class="line">dj_dw: [<span class="number">0.498333393278696</span>, <span class="number">0.49883942983996693</span>]</span><br></pre></td></tr></table></figure><hr><p>​<strong>梯度下降代码:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w_in, b_in, alpha, num_iters</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs batch gradient descent</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)   : Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,))   : target values</span></span><br><span class="line"><span class="string">      w_in (ndarray (n,)): Initial values of model parameters  </span></span><br><span class="line"><span class="string">      b_in (scalar)      : Initial values of model parameter</span></span><br><span class="line"><span class="string">      alpha (float)      : Learning rate</span></span><br><span class="line"><span class="string">      num_iters (scalar) : number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      w (ndarray (n,))   : Updated values of parameters</span></span><br><span class="line"><span class="string">      b (scalar)         : Updated value of parameter </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span></span><br><span class="line">    J_history = []</span><br><span class="line">    w = copy.deepcopy(w_in)  <span class="comment">#avoid modifying global w within function</span></span><br><span class="line">    b = b_in</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># Calculate the gradient and update the parameters</span></span><br><span class="line">        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update Parameters using w, b, alpha and gradient</span></span><br><span class="line">        w = w - alpha * dj_dw               </span><br><span class="line">        b = b - alpha * dj_db               </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Save cost J at each iteration</span></span><br><span class="line">        <span class="keyword">if</span> i&lt;<span class="number">100000</span>:      <span class="comment"># prevent resource exhaustion </span></span><br><span class="line">            J_history.append( compute_cost_logistic(X, y, w, b) )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span></span><br><span class="line">        <span class="keyword">if</span> i% math.ceil(num_iters / <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;i:4d&#125;</span>: Cost <span class="subst">&#123;J_history[-<span class="number">1</span>]&#125;</span>   &quot;</span>)</span><br><span class="line">    </span><br><span class="line">     <span class="keyword">return</span> w, b, J_history         <span class="comment">#return final w,b and J history for graphing</span></span><br></pre></td></tr></table></figure><p>​<strong>在数据集上运行：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_tmp  = np.zeros_like(X_train[<span class="number">0</span>])</span><br><span class="line">b_tmp  = <span class="number">0.</span></span><br><span class="line">alph = <span class="number">0.1</span></span><br><span class="line">iters = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nupdated parameters: w:<span class="subst">&#123;w_out&#125;</span>, b:<span class="subst">&#123;b_out&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>​<strong>结果：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Iteration    <span class="number">0</span>: Cost <span class="number">0.684610468560574</span>   </span><br><span class="line">Iteration <span class="number">1000</span>: Cost <span class="number">0.1590977666870457</span>   </span><br><span class="line">Iteration <span class="number">2000</span>: Cost <span class="number">0.08460064176930078</span>   </span><br><span class="line">Iteration <span class="number">3000</span>: Cost <span class="number">0.05705327279402531</span>   </span><br><span class="line">Iteration <span class="number">4000</span>: Cost <span class="number">0.04290759421682</span>   </span><br><span class="line">Iteration <span class="number">5000</span>: Cost <span class="number">0.03433847729884557</span>   </span><br><span class="line">Iteration <span class="number">6000</span>: Cost <span class="number">0.02860379802212006</span>   </span><br><span class="line">Iteration <span class="number">7000</span>: Cost <span class="number">0.02450156960879306</span>   </span><br><span class="line">Iteration <span class="number">8000</span>: Cost <span class="number">0.02142370332569295</span>   </span><br><span class="line">Iteration <span class="number">9000</span>: Cost <span class="number">0.019030137124109114</span>   </span><br><span class="line"></span><br><span class="line">updated parameters: w:[<span class="number">5.28</span> <span class="number">5.08</span>], b:-<span class="number">14.222409982019837</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29140/"/>
      <url>/posts/29140/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="1-多维特征"><a href="#1-多维特征" class="headerlink" title="1.多维特征"></a>1.多维特征</h2><p><strong>多元线性回归模型</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100942878.webp" alt="image-20230610094203639"></p><hr><hr><h2 id="2-矢量化"><a href="#2-矢量化" class="headerlink" title="2.矢量化"></a>2.矢量化</h2><p><strong>缩短代码，提高运行效率</strong></p><p><strong>NumPy</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100951696.webp" alt="image-20230610095105404"></p><p><strong>用空间换时间，用连续的结构可以省去查找数据的时间</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002307.webp" alt="image-20230610100241038"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002970.webp" alt="image-20230610100256599"></p><hr><hr><h2 id="3-用于多元线性回归的梯度下降算法"><a href="#3-用于多元线性回归的梯度下降算法" class="headerlink" title="3.用于多元线性回归的梯度下降算法"></a>3.用于多元线性回归的梯度下降算法</h2><p><strong>带有矢量化的多元线性回归实现梯度下降</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101013171.webp" alt="image-20230610101334864"></p><p><strong>核心</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101018363.webp" alt="image-20230610101851161"></p><hr><hr><h2 id="4-特征缩放-数据预处理"><a href="#4-特征缩放-数据预处理" class="headerlink" title="4.特征缩放(数据预处理)"></a>4.特征缩放(数据预处理)</h2><p><strong>归一化</strong> ：拥有不同特征时，它们得取值范围非常不同时，可能会导致梯度下降运行缓慢，重新缩放不同得特征，使它们都具有可比较的取值范围，效果更显著。</p><ol><li><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110919219.webp" alt="image-20230611091955007"></li><li>均值归一化：特征值减平均值（样本平均值）再除以方差<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110925805.webp" alt="image-20230611092559538"></li><li>Z-score 归一化：需要计算每个特征的标准差。<strong>概率论</strong></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110938294.webp" alt="image-20230611093829964"></p><p><strong>按需缩放</strong>：在一个数量级上的特征可以不考虑缩放</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110941080.webp" alt="image-20230611094136864"></p><h2 id="5-判断梯度下降是否收敛"><a href="#5-判断梯度下降是否收敛" class="headerlink" title="5.判断梯度下降是否收敛"></a>5.判断梯度下降是否收敛</h2><p>如果有 <em>J</em> 在一次迭代后增加，意味着 Alpha选择不当，通常意味着Alpha太大，或者代码中可能存在错误。</p><ol><li><strong>迭代次数</strong>，创建学习曲线尝试找出。</li><li>自动收敛测试：假设 <em>epsilon</em> 是一个代表小数变量，例如 0.001 或 10^-3。如果成本 <em>J</em> 在一次迭代中减少的幅度小于这个数字 <em>epsilon</em> ，那么曲线可能趋于平坦。</li></ol><h2 id="6-选择合适的学习率"><a href="#6-选择合适的学习率" class="headerlink" title="6.选择合适的学习率"></a>6.选择合适的学习率</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120950291.webp" alt="image-20230612095038996"></p><hr><p>技巧：如果学习率足够小，成本函数应该在每次迭代中减少。通过将Alpha设置为一个非常小的数字，看看这是否会导致每次迭代时成本降低。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120956944.webp" alt="image-20230612095616640"></p><p><strong>省流：凭感觉去试，调参。</strong></p><h2 id="7-特征工程"><a href="#7-特征工程" class="headerlink" title="7.特征工程"></a>7.特征工程</h2><p><strong>特征衍生</strong>：通常通过转换或组合原始特征来使学习算法更容易做出准确的预测。</p><p><strong>多项式回归</strong></p><p><strong>Scikit-learn</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121042502.webp" alt="image-20230612104233179"></p><p>不仅可以拟合直线，还可以拟合曲线、非线性函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121050229.webp" alt="image-20230612105041878">、</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29139/"/>
      <url>/posts/29139/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="两种主要类型"><a href="#两种主要类型" class="headerlink" title="两种主要类型"></a><p align="left">两种主要类型</p></h2><h3 id="1-监督学习-Supervised-Learning"><a href="#1-监督学习-Supervised-Learning" class="headerlink" title="1.监督学习(Supervised Learning)"></a><p align="left">1.监督学习(Supervised Learning)</p></h3><p><img src="https://api2.mubu.com/v3/document_image/3413a5cb-b522-42e8-ae40-3a988a00321d-12774614.jpg" alt="image"></p><p>1、Regression-回归：从无限多种可能输出数字中预测数字。</p><p>2、Classfication-分类：预测类别，可能的输出都是一小组。</p><hr><h3 id="2-无监督学习-Unsupervised-Learning"><a href="#2-无监督学习-Unsupervised-Learning" class="headerlink" title="2.无监督学习(Unsupervised Learning)"></a><p align="left">2.无监督学习(Unsupervised Learning)</p></h3><p>没有试图监督算法而为了给每个输入提供一些正确的答案，相反，为了弄清数据中有什么模式或者结构。</p><p><img src="https://api2.mubu.com/v3/document_image/ab575484-7afb-4726-b350-0697e0a4cf12-12774614.jpg" alt="image"></p><p>1、Clustering-聚类：获取没有标签的数据并尝试将他们自动分组到集群中。例如相似推荐，就是把相似的内容归类后处理。</p><p>2、Anomaly detection-异常检测：用于检测异常事件。</p><p>3、Dimensionality reduction-降维：压缩大数据集。</p><hr><h3 id="3-线性回归模型-linear-regression"><a href="#3-线性回归模型-linear-regression" class="headerlink" title="3.线性回归模型(linear regression)"></a><p align="left">3.线性回归模型(linear regression)</p></h3><p>Training set-训练集、features-输入变量 <em>x</em>（特征或输入特征）、targets-目标变量  <em>y</em></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021634673.webp" alt="image-20230602163455564"></p><p>Univariate linear regression-单变量线性回归</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021647760.webp" alt="image-20230602164734648"></p><p><strong>1.Cost function-代价函数</strong></p><p>平方误差代价函数</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021702346.webp" alt="image-20230602170247191"></p><p>如何使用代价函数为模型找到最佳参数？</p><p>使  <em>J</em>  越小越好。  </p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021719541.webp" alt="image-20230602171953431"></p><p>先找最优的权重 <em>w</em> ，令 <em>b</em> 为 0。做代价函数图—二维</p><p><strong>多元函数求极值的问题。</strong></p><p>可视化代价函数：</p><p>回到原始问题，<em>b</em> 不为 0 时：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021801888.webp" alt="image-20230602180127719"></p><hr><h3 id="4-梯度下降（Gradient-descent）"><a href="#4-梯度下降（Gradient-descent）" class="headerlink" title="4.梯度下降（Gradient descent）"></a><p align="left">4.梯度下降（Gradient descent）</p></h3><p>高效算法：代码编写自动找到 <em>w</em> 和 <em>b</em>，实现最好的拟合。</p><p>梯度下降是一种尝试最小化任何函数的方法。而不局限于线性回归的成本函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031154023.webp" alt="image-20230603115414788"></p><p><strong>同时更新参数（代码顺序）</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031745415.webp" alt="image-20230603174503106"></p><p><strong>学习率</strong></p><p>太小：下降步幅小，速度慢。</p><p>太大：步幅大，但可能会使结果更糟，在最低点附近震荡，过充，甚至离最低点越来越远，发散。</p><hr><h3 id="5-线性回归算法"><a href="#5-线性回归算法" class="headerlink" title="5.线性回归算法"></a><p align="left">5.线性回归算法</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306071640865.webp" alt="image-20230607164007657"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081554888.webp" alt="image-20230608155457007"></p><p><strong>使用线性回归的平方误差成本函数时，成本函数永远不会有多个局部最小值。凸函数</strong></p><hr><h3 id="6-运行梯度下降"><a href="#6-运行梯度下降" class="headerlink" title="6.运行梯度下降"></a><p align="left">6.运行梯度下降</p></h3><p><strong>C1_W2_Lab03_Gradient_Descent_Soln</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081609512.webp" alt="image-20230608160911171"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
