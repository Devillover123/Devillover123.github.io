<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29140/"/>
      <url>/posts/29140/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29141/"/>
      <url>/posts/29141/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><p>分类：输出变量为少数可能值中的一个，而不是无限范围内的任何数字。</p><h3 id="1-2逻辑回归（用于二元分类）"><a href="#1-2逻辑回归（用于二元分类）" class="headerlink" title="1.2逻辑回归（用于二元分类）"></a>1.2逻辑回归（用于二元分类）</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061024791.webp" alt="image-20230706102408627"></p><p>Sigmoid函数是一个在生物学中常见的<a href="https://baike.baidu.com/item/S%E5%9E%8B%E5%87%BD%E6%95%B0/19178062?fromModule=lemma_inlink">S型函数</a>，也称为<a href="https://baike.baidu.com/item/S%E5%9E%8B%E7%94%9F%E9%95%BF%E6%9B%B2%E7%BA%BF/5581189?fromModule=lemma_inlink">S型生长曲线</a>。 在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的<a href="https://baike.baidu.com/item/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2520792?fromModule=lemma_inlink">激活函数</a>，将变量映射到0,1之间。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061059720.webp" alt="image-20230706105911494"></p><p><strong>多变量整合为单变量，再用sigmoid分类</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061104571.webp" alt="image-20230706110452346"></p><h3 id="1-3逻辑回归、决策边界"><a href="#1-3逻辑回归、决策边界" class="headerlink" title="1.3逻辑回归、决策边界"></a>1.3逻辑回归、决策边界</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081441602.webp" alt="image-20230708144100042"></p><h3 id="2-1逻辑回归中的代价函数"><a href="#2-1逻辑回归中的代价函数" class="headerlink" title="2.1逻辑回归中的代价函数"></a>2.1逻辑回归中的代价函数</h3><p><strong>代价函数提供了一种衡量特定参数与训练数据拟合程度的方法</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081448958.webp" alt="image-20230708144831744"></p><hr><p><strong>单个训练示例的损失函数</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101450133.webp" alt="image-20230710145008957"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081510170.webp" alt="image-20230708151030003"></p><p><em>f</em>  的值总是在0-1之间，越接近于1，损失越小。<strong>上图损失函数漏掉了一个负号</strong></p><hr><p><strong>代价函数是整个训练集的函数，因此是单个训练示例的损失函数之和的平均值</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081519147.webp" alt="image-20230708151933785"></p><pre><code class="python">def compute_cost_logistic(X, y, w, b):    &quot;&quot;&quot;    Computes cost    Args:      X (ndarray (m,n)): Data, m examples with n features      y (ndarray (m,)) : target values      w (ndarray (n,)) : model parameters        b (scalar)       : model parameter      Returns:      cost (scalar): cost     &quot;&quot;&quot;m = X.shape[0]cost = 0.0for i in range(m):    z_i = np.dot(X[i],w) + b    f_wb_i = sigmoid(z_i)    cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)         cost = cost / mreturn cost</code></pre><h3 id="2-2简化逻辑回归代价函数"><a href="#2-2简化逻辑回归代价函数" class="headerlink" title="2.2简化逻辑回归代价函数"></a>2.2简化逻辑回归代价函数</h3><p>目的：使用梯度下降来拟合逻辑回归模型的参数时，实现简单。</p><p>二元<strong>交叉熵损失函数</strong>，多分类只需要多乘几个多项式即可</p><p> <img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101459266.webp" alt="image-20230710145927025"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101504651.webp" alt="image-20230710150413331"></p><p>这个特定的成本函数是使用最大似然估计的统计原理推导出来的，这是统计学中关于如何有效的找到不同模型的参数的想法。<strong>它有一个很好的特性，它是凸的。</strong></p><h3 id="3-1实现梯度下降"><a href="#3-1实现梯度下降" class="headerlink" title="3.1实现梯度下降"></a>3.1实现梯度下降</h3><p>为了拟合逻辑回归模型的参数，将尝试找到使 <em>w</em> 和 <em>b</em> 的成本  <em>J</em>  最小的参数值，使用梯度下降来实现。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121410281.webp" alt="image-20230712141007965"></p><p>与线性回归模型类似，注意同时更新。</p><p>推导过程：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121500696.webp" alt="image-20230712150021478"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121519309.webp" alt="image-20230712151912073"></p><p><strong>尽管线性回归和逻辑回归所写的算法看起来相同，但实际上它们是两种截然不同的算法，因为 <em>f</em> 的定义不同。</strong></p><p><strong>ps：上述逻辑回归的 <em>f</em> 的负号位置错了</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121526385.webp" alt="image-20230712152606243"></p><hr><p><strong>梯度函数实现代码：</strong></p><pre><code class="python">def compute_gradient_logistic(X, y, w, b):     &quot;&quot;&quot;    Computes the gradient for linear regression      Args:      X (ndarray (m,n): Data, m examples with n features      y (ndarray (m,)): target values      w (ndarray (n,)): model parameters        b (scalar)      : model parameter    Returns      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.       dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b.     &quot;&quot;&quot;    m,n = X.shape    dj_dw = np.zeros((n,))                           #(n,)    dj_db = 0.    for i in range(m):        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar        err_i  = f_wb_i  - y[i]                       #scalar        for j in range(n):            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar        dj_db = dj_db + err_i    dj_dw = dj_dw/m                                   #(n,)    dj_db = dj_db/m                                   #scalar            return dj_db, dj_dw  </code></pre><p><strong>检查梯度函数的实现：</strong></p><pre><code class="python">X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])y_tmp = np.array([0, 0, 0, 1, 1, 1])w_tmp = np.array([2.,3.])b_tmp = 1.dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)print(f&quot;dj_db: &#123;dj_db_tmp&#125;&quot; )print(f&quot;dj_dw: &#123;dj_dw_tmp.tolist()&#125;&quot; )</code></pre><p><strong>结果为：</strong></p><pre><code class="python">dj_db: 0.49861806546328574dj_dw: [0.498333393278696, 0.49883942983996693]</code></pre><hr><p><strong>梯度下降代码:</strong></p><pre><code class="python">def gradient_descent(X, y, w_in, b_in, alpha, num_iters):     &quot;&quot;&quot;    Performs batch gradient descent    Args:      X (ndarray (m,n)   : Data, m examples with n features      y (ndarray (m,))   : target values      w_in (ndarray (n,)): Initial values of model parameters        b_in (scalar)      : Initial values of model parameter      alpha (float)      : Learning rate      num_iters (scalar) : number of iterations to run gradient descent      Returns:      w (ndarray (n,))   : Updated values of parameters      b (scalar)         : Updated value of parameter     &quot;&quot;&quot;    # An array to store cost J and w&#39;s at each iteration primarily for graphing later    J_history = []    w = copy.deepcopy(w_in)  #avoid modifying global w within function    b = b_in    for i in range(num_iters):        # Calculate the gradient and update the parameters        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)           # Update Parameters using w, b, alpha and gradient        w = w - alpha * dj_dw                       b = b - alpha * dj_db                         # Save cost J at each iteration        if i&lt;100000:      # prevent resource exhaustion             J_history.append( compute_cost_logistic(X, y, w, b) )        # Print cost every at intervals 10 times or as many iterations if &lt; 10        if i% math.ceil(num_iters / 10) == 0:            print(f&quot;Iteration &#123;i:4d&#125;: Cost &#123;J_history[-1]&#125;   &quot;)         return w, b, J_history         #return final w,b and J history for graphing</code></pre><p><strong>在数据集上运行：</strong></p><pre><code class="python">w_tmp  = np.zeros_like(X_train[0])b_tmp  = 0.alph = 0.1iters = 10000w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) print(f&quot;\nupdated parameters: w:&#123;w_out&#125;, b:&#123;b_out&#125;&quot;)</code></pre><p><strong>结果：</strong></p><pre><code class="python">Iteration    0: Cost 0.684610468560574   Iteration 1000: Cost 0.1590977666870457   Iteration 2000: Cost 0.08460064176930078   Iteration 3000: Cost 0.05705327279402531   Iteration 4000: Cost 0.04290759421682   Iteration 5000: Cost 0.03433847729884557   Iteration 6000: Cost 0.02860379802212006   Iteration 7000: Cost 0.02450156960879306   Iteration 8000: Cost 0.02142370332569295   Iteration 9000: Cost 0.019030137124109114   updated parameters: w:[5.28 5.08], b:-14.222409982019837</code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29140/"/>
      <url>/posts/29140/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="1-多维特征"><a href="#1-多维特征" class="headerlink" title="1.多维特征"></a>1.多维特征</h2><p><strong>多元线性回归模型</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100942878.webp" alt="image-20230610094203639"></p><hr><hr><h2 id="2-矢量化"><a href="#2-矢量化" class="headerlink" title="2.矢量化"></a>2.矢量化</h2><p><strong>缩短代码，提高运行效率</strong></p><p><strong>NumPy</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100951696.webp" alt="image-20230610095105404"></p><p><strong>用空间换时间，用连续的结构可以省去查找数据的时间</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002307.webp" alt="image-20230610100241038"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002970.webp" alt="image-20230610100256599"></p><hr><hr><h2 id="3-用于多元线性回归的梯度下降算法"><a href="#3-用于多元线性回归的梯度下降算法" class="headerlink" title="3.用于多元线性回归的梯度下降算法"></a>3.用于多元线性回归的梯度下降算法</h2><p><strong>带有矢量化的多元线性回归实现梯度下降</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101013171.webp" alt="image-20230610101334864"></p><p><strong>核心</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101018363.webp" alt="image-20230610101851161"></p><hr><hr><h2 id="4-特征缩放-数据预处理"><a href="#4-特征缩放-数据预处理" class="headerlink" title="4.特征缩放(数据预处理)"></a>4.特征缩放(数据预处理)</h2><p><strong>归一化</strong> ：拥有不同特征时，它们得取值范围非常不同时，可能会导致梯度下降运行缓慢，重新缩放不同得特征，使它们都具有可比较的取值范围，效果更显著。</p><ol><li><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110919219.webp" alt="image-20230611091955007"></li><li>均值归一化：特征值减平均值（样本平均值）再除以方差<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110925805.webp" alt="image-20230611092559538"></li><li>Z-score 归一化：需要计算每个特征的标准差。<strong>概率论</strong></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110938294.webp" alt="image-20230611093829964"></p><p><strong>按需缩放</strong>：在一个数量级上的特征可以不考虑缩放</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110941080.webp" alt="image-20230611094136864"></p><h2 id="5-判断梯度下降是否收敛"><a href="#5-判断梯度下降是否收敛" class="headerlink" title="5.判断梯度下降是否收敛"></a>5.判断梯度下降是否收敛</h2><p>如果有 <em>J</em> 在一次迭代后增加，意味着 Alpha选择不当，通常意味着Alpha太大，或者代码中可能存在错误。</p><ol><li><strong>迭代次数</strong>，创建学习曲线尝试找出。</li><li>自动收敛测试：假设 <em>epsilon</em> 是一个代表小数变量，例如 0.001 或 10^-3。如果成本 <em>J</em> 在一次迭代中减少的幅度小于这个数字 <em>epsilon</em> ，那么曲线可能趋于平坦。</li></ol><h2 id="6-选择合适的学习率"><a href="#6-选择合适的学习率" class="headerlink" title="6.选择合适的学习率"></a>6.选择合适的学习率</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120950291.webp" alt="image-20230612095038996"></p><hr><p>技巧：如果学习率足够小，成本函数应该在每次迭代中减少。通过将Alpha设置为一个非常小的数字，看看这是否会导致每次迭代时成本降低。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120956944.webp" alt="image-20230612095616640"></p><p><strong>省流：凭感觉去试，调参。</strong></p><h2 id="7-特征工程"><a href="#7-特征工程" class="headerlink" title="7.特征工程"></a>7.特征工程</h2><p><strong>特征衍生</strong>：通常通过转换或组合原始特征来使学习算法更容易做出准确的预测。</p><p><strong>多项式回归</strong></p><p><strong>Scikit-learn</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121042502.webp" alt="image-20230612104233179"></p><p>不仅可以拟合直线，还可以拟合曲线、非线性函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121050229.webp" alt="image-20230612105041878"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29139/"/>
      <url>/posts/29139/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="两种主要类型"><a href="#两种主要类型" class="headerlink" title="两种主要类型"></a><p align="left">两种主要类型</p></h2><h3 id="1-监督学习-Supervised-Learning"><a href="#1-监督学习-Supervised-Learning" class="headerlink" title="1.监督学习(Supervised Learning)"></a><p align="left">1.监督学习(Supervised Learning)</p></h3><p><img src="https://api2.mubu.com/v3/document_image/3413a5cb-b522-42e8-ae40-3a988a00321d-12774614.jpg" alt="image"></p><p>1、Regression-回归：从无限多种可能输出数字中预测数字。</p><p>2、Classfication-分类：预测类别，可能的输出都是一小组。</p><hr><h3 id="2-无监督学习-Unsupervised-Learning"><a href="#2-无监督学习-Unsupervised-Learning" class="headerlink" title="2.无监督学习(Unsupervised Learning)"></a><p align="left">2.无监督学习(Unsupervised Learning)</p></h3><p>没有试图监督算法而为了给每个输入提供一些正确的答案，相反，为了弄清数据中有什么模式或者结构。</p><p><img src="https://api2.mubu.com/v3/document_image/ab575484-7afb-4726-b350-0697e0a4cf12-12774614.jpg" alt="image"></p><p>1、Clustering-聚类：获取没有标签的数据并尝试将他们自动分组到集群中。例如相似推荐，就是把相似的内容归类后处理。</p><p>2、Anomaly detection-异常检测：用于检测异常事件。</p><p>3、Dimensionality reduction-降维：压缩大数据集。</p><hr><h3 id="3-线性回归模型-linear-regression"><a href="#3-线性回归模型-linear-regression" class="headerlink" title="3.线性回归模型(linear regression)"></a><p align="left">3.线性回归模型(linear regression)</p></h3><p>Training set-训练集、features-输入变量 <em>x</em>（特征或输入特征）、targets-目标变量  <em>y</em></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021634673.webp" alt="image-20230602163455564"></p><p>Univariate linear regression-单变量线性回归</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021647760.webp" alt="image-20230602164734648"></p><p><strong>1.Cost function-代价函数</strong></p><p>平方误差代价函数</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021702346.webp" alt="image-20230602170247191"></p><p>如何使用代价函数为模型找到最佳参数？</p><p>使  <em>J</em>  越小越好。  </p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021719541.webp" alt="image-20230602171953431"></p><p>先找最优的权重 <em>w</em> ，令 <em>b</em> 为 0。做代价函数图—二维</p><p><strong>多元函数求极值的问题。</strong></p><p>可视化代价函数：</p><p>回到原始问题，<em>b</em> 不为 0 时：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021801888.webp" alt="image-20230602180127719"></p><hr><h3 id="4-梯度下降（Gradient-descent）"><a href="#4-梯度下降（Gradient-descent）" class="headerlink" title="4.梯度下降（Gradient descent）"></a><p align="left">4.梯度下降（Gradient descent）</p></h3><p>高效算法：代码编写自动找到 <em>w</em> 和 <em>b</em>，实现最好的拟合。</p><p>梯度下降是一种尝试最小化任何函数的方法。而不局限于线性回归的成本函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031154023.webp" alt="image-20230603115414788"></p><p><strong>同时更新参数（代码顺序）</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031745415.webp" alt="image-20230603174503106"></p><p><strong>学习率</strong></p><p>太小：下降步幅小，速度慢。</p><p>太大：步幅大，但可能会使结果更糟，在最低点附近震荡，过充，甚至离最低点越来越远，发散。</p><hr><h3 id="5-线性回归算法"><a href="#5-线性回归算法" class="headerlink" title="5.线性回归算法"></a><p align="left">5.线性回归算法</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306071640865.webp" alt="image-20230607164007657"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081554888.webp" alt="image-20230608155457007"></p><p><strong>使用线性回归的平方误差成本函数时，成本函数永远不会有多个局部最小值。凸函数</strong></p><hr><h3 id="6-运行梯度下降"><a href="#6-运行梯度下降" class="headerlink" title="6.运行梯度下降"></a><p align="left">6.运行梯度下降</p></h3><p><strong>C1_W2_Lab03_Gradient_Descent_Soln</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081609512.webp" alt="image-20230608160911171"></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
