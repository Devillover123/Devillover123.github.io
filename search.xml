<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>无监督学习</title>
      <link href="/posts/29154/"/>
      <url>/posts/29154/</url>
      
        <content type="html"><![CDATA[<h1 id="聚类、异常检测"><a href="#聚类、异常检测" class="headerlink" title="聚类、异常检测"></a><p align="center">聚类、异常检测</p></h1><h2 id="1、聚类（Clustering）"><a href="#1、聚类（Clustering）" class="headerlink" title="1、聚类（Clustering）"></a>1、聚类（Clustering）</h2><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>​K均值（K-means）是一种常见的聚类算法，用于机器学习和数据分析，根据数据点的相似性将它们分成不同的组或簇。K均值的主要目标是将数据点分配到K个簇中，以使簇内的方差最小化。</p><p>​K均值算法的工作原理如下：</p><ol><li><strong>初始化</strong>：选择K个初始簇中心点（簇质心），这些可以是随机选择的数据点，或者使用其他方法如k-means++确定。</li><li><strong>分配</strong>：将每个数据点分配给最近的簇中心，创建K个簇。</li><li><strong>更新中心点</strong>：通过计算分配给该簇的所有数据点的<strong>平均值</strong>，重新计算每个簇的中心点。</li><li><strong>重复</strong>：重复步骤2和3，直到达到收敛，也就是说<strong>中心点不再显著变化</strong>，或者达到预定的迭代次数。</li><li><strong>最终聚类</strong>：算法终止后，得到K个簇，每个数据点属于其中之一  。</li></ol><p>​K的选择，即簇的数量，是K均值中的关键参数，可以显著影响结果。可以使用各种方法，如肘部法和轮廓分数，来确定K的合适值。</p><p>​K均值是高效的，当簇大致呈球形且大小相近时效果良好。但是，它有一些局限性，比如对初始中心点位置敏感，不适用于具有非凸形状或不均匀密度的簇。为了解决一些这些限制，不同情况下会使用其他聚类算法，如层次聚类和DBSCAN。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309202006691.webp" alt="image-20230920200617406"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_closest_centroids</span>(<span class="params">X, centroids</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set K</span></span><br><span class="line">    K = centroids.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    idx = np.zeros(X.shape[<span class="number">0</span>], dtype=<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="comment"># Array to hold distance between X[i] and each centroids[j]</span></span><br><span class="line">        distance = [] </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(centroids.shape[<span class="number">0</span>]):</span><br><span class="line">            norm_ij = np.linalg.norm(X[i] - centroids[j]) 、</span><br><span class="line">            <span class="comment"># norm 范数，liner线性+algebra代数</span></span><br><span class="line">            <span class="comment"># Your code to calculate the norm between (X[i] - centroids[j])</span></span><br><span class="line">            distance.append(norm_ij) <span class="comment"># append 函数用于向列表的末尾添加一个元素</span></span><br><span class="line">            </span><br><span class="line">        idx[i] = np.argmin(distance)</span><br><span class="line">        <span class="comment">#aigmin给出最小值下标</span></span><br><span class="line">        <span class="comment"># Your code here to calculate index of minimum value in distance</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309212023475.webp" alt="image-20230921202352790"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_centroids</span>(<span class="params">X, idx, K</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns the new centroids by computing the means of the </span></span><br><span class="line"><span class="string">    data points assigned to each centroid.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X (ndarray):   (m, n) Data points</span></span><br><span class="line"><span class="string">        idx (ndarray): (m,) Array containing index of closest centroid for each </span></span><br><span class="line"><span class="string">                       example in X. Concretely, idx[i] contains the index of </span></span><br><span class="line"><span class="string">                       the centroid closest to example i</span></span><br><span class="line"><span class="string">        K (int):       number of centroids</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        centroids (ndarray): (K, n) New centroids computed</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Useful variables</span></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># You need to return the following variables correctly</span></span><br><span class="line">    centroids = np.zeros((K, n))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):   </span><br><span class="line">            </span><br><span class="line">            points = X[idx == k]</span><br><span class="line">            <span class="comment"># Your code here to get a list of all data points in X assigned to centroid k  </span></span><br><span class="line">            centroids[k] = np.mean(points, axis = <span class="number">0</span>) <span class="comment"># Your code here to compute the mean of the points assigned</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ## </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure><h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>​K均值（K-means）的代价函数通常被称为簇内平方和（Inertia）或误差平方和（SSE，Sum of Squared Errors）。这个代价函数的目标是度量簇内数据点与其所属簇中心之间的距离的总和，从而评估聚类的质量。K均值算法的目标是最小化这个代价函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309202020954.webp" alt="image-20230920202028690"></p><p>​在这个代价函数中，1&#x2F;m用于将平方和除以数据点的总数 m，以计算平均簇内平方和。这个归一化的代价函数更容易解释和比较，因为它不受数据集大小的影响，从而使不同规模的数据集的K均值结果可以进行有效比较。</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>​随机以训练示例为质心初始化，计算选择代价最小的聚类即可。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309202043689.webp" alt="image-20230920204313438"></p><h3 id="选择聚类数"><a href="#选择聚类数" class="headerlink" title="选择聚类数"></a>选择聚类数</h3><p>​选择聚类数量（K的值）是K均值聚类中的一个重要决策，它会直接影响到聚类结果的质量。以下是一些常见的方法和技巧来选择合适的聚类数量：</p><ol><li><strong>肘部法（Elbow Method）</strong>：肘部法是一种直观的方法，它涉及尝试不同的K值，然后绘制K值与代价函数的关系图。通常，代价函数（簇内平方和）会随着K的增加而减小，但在某个点后下降速度会减慢，形成一个肘部状的曲线。选择肘部处对应的K值作为最佳聚类数量。</li><li><strong>轮廓分数（Silhouette Score）</strong>：轮廓分数是一种量化聚类效果的方法，它考虑了簇内数据点的紧密度和簇间数据点的分离度。通过计算不同K值下的轮廓分数，可以选择使轮廓分数最大化的K值作为最佳聚类数量。</li><li><strong>Gap统计量（Gap Statistics）</strong>：Gap统计量是一种比较实际数据集的代价函数与随机数据集的代价函数之间的差异来选择K值的方法。通过比较观察到的数据集代价函数与一组生成的随机数据集代价函数的差异，可以找到最佳的K值。</li><li><strong>专业领域知识</strong>：在某些情况下，您可能具有关于数据的领域知识，可以帮助您估计合适的聚类数量。例如，如果您了解数据的特性，并且知道数据应该被分成多少个明显的群组，那么您可以选择相应的K值。</li><li><strong>可视化</strong>：对数据进行可视化分析也可以帮助您选择聚类数量。通过绘制数据点的散点图、簇中心的位置和簇的分布情况，您可以直观地观察到数据的聚类结构，并估计适当的K值。</li><li><strong>交叉验证</strong>：在某些情况下，您可以使用交叉验证来选择聚类数量。将数据集分成训练集和测试集，然后在训练集上使用不同的K值进行聚类，然后在测试集上评估聚类性能，选择性能最佳的K值。</li></ol><p>​最终，选择聚类数量通常是一个权衡过程，需要考虑数据的特性以及您的分析目标。不同的方法可能会产生不同的结果，因此通常建议尝试多种方法来选择最佳的K值。</p><h2 id="2、异常检测"><a href="#2、异常检测" class="headerlink" title="2、异常检测"></a>2、异常检测</h2><p>​异常检测是一种数据分析和机器学习技术，用于识别数据集中的异常或异常行为。<strong>异常通常指的是与大多数数据点不同或不符合正常模式的数据点或事件</strong>。异常检测在许多领域中都有广泛的应用，包括金融欺诈检测、网络安全、制造业质量控制、医疗诊断等。以下是一些常见的异常检测方法和技术：</p><ol><li><strong>统计方法</strong>：统计方法是最简单的异常检测方法之一，它们基于数据的统计特性来识别异常。例如，通过计算数据点的均值和标准差，可以使用Z分数方法来检测异常值。</li><li><strong>机器学习方法</strong>：机器学习方法包括使用监督学习、无监督学习或半监督学习来构建模型来识别异常。常见的算法包括支持向量机 (SVM)、随机森林、神经网络和聚类方法（如K均值聚类）。</li><li><strong>基于距离的方法</strong>：这些方法使用数据点之间的距离来度量异常程度。例如，K近邻算法（K-Nearest Neighbors）可以通过计算数据点与其最近邻居之间的距离来识别异常。</li><li><strong>基于密度的方法</strong>：这些方法尝试识别数据中的低密度区域作为异常。LOF（局部离群因子）和DBSCAN（基于密度的空间聚类应用噪声）是一些常见的基于密度的异常检测方法。</li><li><strong>时间序列异常检测</strong>：用于时间序列数据的特定方法，如季节性分解、指数平滑和ARIMA模型，可用于检测时间序列数据中的异常。</li><li><strong>深度学习方法</strong>：深度学习方法，尤其是循环神经网络（RNN）和长短时记忆网络（LSTM），在时间序列数据和文本数据中的异常检测方面表现出色。</li><li><strong>无监督异常检测</strong>：无监督方法不需要标记的异常数据，而是试图从数据中学习正常模式，并识别与之不符的模式。</li><li><strong>集成方法</strong>：这些方法将多个异常检测模型组合起来，以提高检测性能。例如，Isolation Forest和One-Class SVM是集成方法的示例。</li><li><strong>领域知识</strong>：在某些情况下，领域知识和专业知识对于异常检测非常重要。专家可以定义哪些情况下应该报告异常，并帮助改进模型。</li></ol><h3 id="高斯正太分布"><a href="#高斯正太分布" class="headerlink" title="高斯正太分布"></a>高斯正太分布</h3><p>​高斯分布，也称为正态分布，是统计学中非常重要的概率分布之一，通常用于建模自然界和人类行为中的许多现象。高斯分布的概率密度函数（Probability Density Function，PDF）如下：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221443295.webp" alt="image-20230922144310142"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221448406.webp" alt="image-20230922144849103"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221456419.webp" alt="image-20230922145613176"></p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>​使用高斯正态分布（也称为高斯分布）对特征建模是一种常用的方法来检测异常。这个方法通常被称为基于概率的异常检测，它假设正常数据点符合高斯分布，而异常点则位于分布的尾部。以下是使用高斯分布进行异常检测的一般步骤：</p><ol><li><strong>数据准备</strong>：<ul><li>收集和准备要用于异常检测的数据。</li><li>确保数据的特征（属性）是数值型的。</li></ul></li><li><strong>特征工程</strong>：<ul><li>对数据进行必要的特征工程，包括缺失值处理、数据标准化等，以确保数据满足高斯分布的假设。</li></ul></li><li><strong>建立高斯模型</strong>：<ul><li>对每个特征（属性）独立地建立高斯分布模型。对于每个特征，估计其均值（$\mu$）和方差（$\sigma^2$）。</li><li>这可以通过计算每个特征的样本均值和样本方差来完成。</li></ul></li><li><strong>计算概率密度</strong>：<ul><li>对于给定的数据点，计算每个特征的概率密度值，使用高斯分布的概率密度函数。</li><li>对于多维数据，将每个特征的概率密度值相乘得到整体概率密度。</li></ul></li><li><strong>确定异常阈值</strong>：<ul><li>选择一个适当的异常阈值，通常是一个小概率值（例如，0.01% 或 0.1%）。</li><li>如果数据点的整体概率密度低于此阈值，则将其标记为异常点。</li></ul></li><li><strong>进行异常检测</strong>：<ul><li>对于新的数据点，将其输入到已建立的高斯分布模型中，计算其概率密度。</li><li>如果概率密度低于异常阈值，则将该数据点标记为异常。</li></ul></li><li><strong>可视化和解释</strong>：<ul><li>可视化结果以便于理解和解释。可以使用直方图、概率密度图、散点图等方法来展示异常点。</li></ul></li><li><strong>模型评估</strong>：<ul><li>对异常检测模型进行评估，通常使用准确率、召回率、F1分数等指标。</li><li>还可以使用交叉验证来评估模型性能。</li></ul></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221522643.webp" alt="image-20230922152247373"></p><p> <img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221530235.webp" alt="image-20230922153026007"></p><p>​ complete the <code>select_threshold</code> function below to find the best threshold to use for selecting outliers based on the results from a validation set (<code>p_val</code>) and the ground truth (<code>y_val</code>).</p><p>​<code>np.sum</code> 是NumPy库中的一个函数，用于计算数组或矩阵中元素的和。它的用法非常简单，可以对数组中的所有元素或者指定的轴进行求和操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对整个数组或矩阵的元素求和</span></span><br><span class="line">result = np.<span class="built_in">sum</span>(array)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿指定轴(axis)对数组或矩阵的元素进行求和</span></span><br><span class="line">result = np.<span class="built_in">sum</span>(array, axis=<span class="number">0</span>)  <span class="comment"># 对列求和</span></span><br><span class="line">result = np.<span class="built_in">sum</span>(array, axis=<span class="number">1</span>)  <span class="comment"># 对行求和</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><ul><li><code>array</code> 可以是一个NumPy数组或矩阵。</li><li><code>axis</code> 是一个可选参数，表示要沿着哪个轴进行求和操作。如果不指定 <code>axis</code> 参数，将对整个数组中的元素进行求和。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个示例数组</span></span><br><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对整个数组的元素求和</span></span><br><span class="line">total_sum = np.<span class="built_in">sum</span>(arr)</span><br><span class="line"><span class="built_in">print</span>(total_sum)  <span class="comment"># 输出：21</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿列(axis=0)求和</span></span><br><span class="line">column_sum = np.<span class="built_in">sum</span>(arr, axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(column_sum)  <span class="comment"># 输出：[5 7 9]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿行(axis=1)求和</span></span><br><span class="line">row_sum = np.<span class="built_in">sum</span>(arr, axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(row_sum)  <span class="comment"># 输出：[ 6 15]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: estimate_gaussian</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">estimate_gaussian</span>(<span class="params">X</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculates mean and variance of all features </span></span><br><span class="line"><span class="string">    in the dataset</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X (ndarray): (m, n) Data matrix</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        mu (ndarray): (n,) Mean of all features</span></span><br><span class="line"><span class="string">        var (ndarray): (n,) Variance of all features</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    mu = <span class="number">1</span> / m * np.<span class="built_in">sum</span>(X, axis = <span class="number">0</span>)</span><br><span class="line">    var = <span class="number">1</span> / m * np.<span class="built_in">sum</span>((X - mu) ** <span class="number">2</span>, axis = <span class="number">0</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> mu, var</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: select_threshold</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">select_threshold</span>(<span class="params">y_val, p_val</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Finds the best threshold to use for selecting outliers </span></span><br><span class="line"><span class="string">    based on the results from a validation set (p_val) </span></span><br><span class="line"><span class="string">    and the ground truth (y_val)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        y_val (ndarray): Ground truth on validation set</span></span><br><span class="line"><span class="string">        p_val (ndarray): Results on validation set</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        epsilon (float): Threshold chosen </span></span><br><span class="line"><span class="string">        F1 (float):      F1 score by choosing epsilon as threshold</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span> </span><br><span class="line"></span><br><span class="line">    best_epsilon = <span class="number">0</span></span><br><span class="line">    best_F1 = <span class="number">0</span></span><br><span class="line">    F1 = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    step_size = (<span class="built_in">max</span>(p_val) - <span class="built_in">min</span>(p_val)) / <span class="number">1000</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epsilon <span class="keyword">in</span> np.arange(<span class="built_in">min</span>(p_val), <span class="built_in">max</span>(p_val), step_size):</span><br><span class="line">    </span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        </span><br><span class="line">        predictions = (p_val &lt; epsilon)</span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line">        tp = np.<span class="built_in">sum</span>((predictions == <span class="number">1</span>) &amp; (y_val == <span class="number">1</span>))</span><br><span class="line">        fp = np.<span class="built_in">sum</span>((predictions == <span class="number">1</span>) &amp; (y_val == <span class="number">0</span>))</span><br><span class="line">        </span><br><span class="line">        fn = np.<span class="built_in">sum</span>((predictions == <span class="number">0</span>) &amp; (y_val == <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        prec = tp / (tp + fp)</span><br><span class="line">        rec = tp / (tp + fn)</span><br><span class="line">        </span><br><span class="line">        F1 = <span class="number">2</span> * prec * rec / (prec + rec)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ### </span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> F1 &gt; best_F1:</span><br><span class="line">            best_F1 = F1</span><br><span class="line">            best_epsilon = epsilon</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_epsilon, best_F1</span><br></pre></td></tr></table></figure><h3 id="开发与评估"><a href="#开发与评估" class="headerlink" title="开发与评估"></a>开发与评估</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221538992.webp" alt="image-20230922153806772"></p><p>​如果已知少量的异常示例，将它放在交叉验证集里效果是很好的。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221632262.webp" alt="image-20230922163254088"></p><h3 id="异常检测与监督学习对比"><a href="#异常检测与监督学习对比" class="headerlink" title="异常检测与监督学习对比"></a>异常检测与监督学习对比</h3><p>​异常检测（无监督学习）和监督学习是两种不同的机器学习方法，用于解决不同类型的问题。以下是异常检测与监督学习的主要对比：</p><ol><li><strong>目标</strong>：<ul><li>异常检测：目标是识别数据中的异常或不寻常的模式，通常假设正常样本比异常样本多得多。</li><li>监督学习：目标是通过已知标签的训练数据来学习一个模型，以便对新数据进行分类或回归预测。</li></ul></li><li><strong>标签</strong>：<ul><li>异常检测：通常不需要标签，因为异常点通常很难获取或成本较高。</li><li>监督学习：需要已知的标签来训练模型，这些标签指示了每个训练样本的正确类别或值。</li></ul></li><li><strong>数据分布</strong>：<ul><li>异常检测：通常假设正常数据和异常数据具有不同的数据分布，异常点位于分布的尾部或稀疏区域。</li><li>监督学习：通常假设训练数据和测试数据的分布相似，模型根据已知的标签进行分类或回归。</li></ul></li><li><strong>模型训练</strong>：<ul><li>异常检测：通常使用无监督方法，如基于距离、密度或概率的算法。</li><li>监督学习：使用带有标签的训练数据来训练监督学习模型，如决策树、神经网络、支持向量机等。</li></ul></li><li><strong>评估</strong>：<ul><li>异常检测：通常使用异常检测指标，如精确度、召回率、F1分数等来评估性能。</li><li>监督学习：通常使用分类或回归的评估指标，如准确率、均方误差、ROC曲线等。</li></ul></li><li><strong>数据量</strong>：<ul><li>异常检测：通常适用于少量的异常数据点。</li><li>监督学习：通常需要大量的标记数据来训练监督学习模型。</li></ul></li><li><strong>应用领域</strong>：<ul><li>异常检测：适用于金融欺诈检测、网络安全、工业质量控制、医疗诊断等需要检测罕见事件的领域。</li><li>监督学习：广泛应用于分类、回归和预测任务，如图像分类、文本分类、自然语言处理等。</li></ul></li><li><strong>可解释性</strong>：<ul><li>异常检测方法通常更容易解释，因为它们通常不依赖于复杂的特征工程或模型。</li><li>监督学习方法可能会涉及更复杂的模型和特征工程，因此可能较难解释。</li></ul></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221658698.webp" alt="image-20230922165806451"></p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><ul><li><p>特征的选择对于构建异常检测系统时非常重要。</p></li><li><p>一种选择是确保特征服从高斯分布。非高斯特征将其进行变换，使其基本符合高斯分布。（炼丹）无论对训练集数据作何种转换，也要记得对交叉验证和测试集作相同的转换。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221705244.webp" alt="image-20230922170528968"></p><ul><li>训练模型，查看算法未能检测到的检查验证集中的异常。尝试创建新的功能，允许算法识别。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309221742106.webp" alt="image-20230922174200903"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习</title>
      <link href="/posts/29153/"/>
      <url>/posts/29153/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a><p align="center">深度学习</p></h1>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deeplearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/posts/29152/"/>
      <url>/posts/29152/</url>
      
        <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a><p align="center">决策树</p></h1><p>猫分类作为示例：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309141742921.webp" alt="image-20230914174201119"></p><p align="center">根节点  >>>  决策节点  >>>  叶节点 </p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309141749242.webp" alt="image-20230914174911889"></p><ul><li>决策树的工作算法：从所有可能的决策树中，尝试选择一个在训练集上表先良好的树，然后理想的泛化到新数据。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309141751202.webp" alt="image-20230914175149945"></p><h2 id="学习过程："><a href="#学习过程：" class="headerlink" title="学习过程："></a>学习过程：</h2><ul><li>决定根节点使用什么特征。    </li><li>只关注左侧部分，决定将哪些节点放在那，特别是要拆分的特征。</li><li>右侧部分重复上述操作。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309142029321.webp" alt="image-20230914202951995"></p><ol><li><p>如何选择每个节点上使用哪些特征进行拆分？</p><p>决策树将要选择拆分的特征以尝试<strong>最大化纯度</strong>。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309142035225.webp" alt="image-20230914203540840"></p></li><li><p>构建决策树的第二个关键是决定何时停止分裂。</p></li></ol><ul><li>当一个节点一定是一类时。</li><li>当分出的节点到达树的最大深度时。（限制决策树深度的一个原因时确保树不会太大和太笨重；通过保持树小，使其不太容易过拟合）</li><li>优先级分数的改进，该分数应该低于某个阈值。</li><li>一个节点的示例数量低于某个阈值，也可能决定停止分裂。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309142050213.webp" alt="image-20230914205023932"></p><h2 id="纯度"><a href="#纯度" class="headerlink" title="纯度"></a>纯度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">在决策树中，熵（entropy）是一种用于衡量数据集的不纯度（impurity）的常用度量方式之一。 </span></span><br><span class="line"><span class="string">熵的概念来源于信息论，它用于衡量系统的混乱程度或不确定性。在决策树中，熵用于衡量一个数据集中不同类别的混合程度。如果一个数据集中的样本都属于同一类别，那么熵为0，表示数据集非常纯净。如果一个数据集中的样本均匀分布在不同的类别中，那么熵会较高，表示数据集不纯，即存在较大的不确定性。</span></span><br><span class="line"><span class="string">熵越高，数据集的不纯度越高，表示需要更多的决策来对数据进行分类。决策树算法会尝试使用不同的特征来分割数据，以降低数据集的熵，即增加数据集的纯度。通过选择使得熵降低最多的特征作为分割特征，决策树可以逐步构建出一个能够有效分类数据的树形结构。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309142134287.webp" alt="image-20230914213441134"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309142128263.webp" alt="image-20230914212832975"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309142133414.webp" alt="image-20230914213310196"></p><h2 id="选择拆分信息增益"><a href="#选择拆分信息增益" class="headerlink" title="选择拆分信息增益"></a>选择拆分信息增益</h2><ul><li><p>在构建决策树时，决定在节点上拆分哪个特征的方式将基于哪种特征选择能够减少熵。</p></li><li><p>计算出熵之后，鉴于要在根节点使用的功能的这三个选项，哪个最有效？</p></li><li><p><strong>如果一个节点有很多示例且具有高熵，相比只有几个示例的高熵要好一点。</strong></p></li><li><p>下图三种情况的根节点已经计算出了左右分支的熵，为了从中挑选，可以将两个数字组合成一个数字。要看每个分支下示例的数量，如果使得示例多的分支的熵低一些，才会是正确的选择。**<code>计算加权平均熵</code>**</p></li><li><p><strong>需要计算的不是加权平均熵，而是与根本没有分裂的熵减少</strong>：转到根节点，开始是10个示例，5猫5狗，p_1实际为0.5，而0.5对应的熵为1。然后减去计算出来的分裂后的加权平均熵，计算出<strong>熵减</strong>。这些被称为<strong>信息增益</strong>。</p></li><li><p>事实证明，决定何时不再进一步分裂的停止标准之一是熵减的大小 。<strong>熵减越大的越好</strong>。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151425110.webp" alt="image-20230915142506755"></p><h3 id="信息增益的一般定义："><a href="#信息增益的一般定义：" class="headerlink" title="信息增益的一般定义："></a><strong>信息增益</strong>的一般定义：</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151429224.webp" alt="image-20230915142929927"></p><blockquote><p>其中   <strong>p_1 ^ left</strong>   表示左分支中正标签样本所占示例分数；</p><p><strong>w ^ left</strong>  表示转到左分支的样本占根节点的样本分数；  </p><p> <strong>p_1 ^ root</strong>    表示根节点中正样本的分数。</p></blockquote><h2 id="给定训练集的情况下构建决策树的整体算法"><a href="#给定训练集的情况下构建决策树的整体算法" class="headerlink" title="给定训练集的情况下构建决策树的整体算法"></a>给定训练集的情况下构建决策树的整体算法</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151502324.webp" alt="image-20230915150159977"></p><ol><li>从树的根节点处的所有训练样本开始。</li><li>计算所有可能特征的信息增益，并选择要拆分的特征以提供最高的信息增益。</li><li>将数据集划分为两个子集，创建树的左右分支，将训练样本发送到左右分支。</li><li>重复上述拆分过程直至：<ul><li>当一个节点100%为一个类别时，熵为0时；</li><li>达到树的最大深度时；</li><li>拆分的信息增益小于阈值时；</li><li>拆分出的节点的样本小于阈值时；</li></ul></li></ol><p>构建决策树的方式是构建较小的子决策树然后将它们放在一起来构建整体决策树。<strong>递归算法</strong>。</p><h2 id="决策树的改进："><a href="#决策树的改进：" class="headerlink" title="决策树的改进："></a>决策树的改进：</h2><h3 id="one-hot编码："><a href="#one-hot编码：" class="headerlink" title="one-hot编码："></a>one-hot编码：</h3><ul><li>解决两个以上的特征：换成k个二进制特征来替换。所有特征中恰有一个为1，剩下的都为0，所以称为：one-hot编码。<strong>该方法也适用于神经网络。</strong></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151615856.webp" alt="image-20230915161502495"></p><h3 id="连续值数字特征的处理："><a href="#连续值数字特征的处理：" class="headerlink" title="连续值数字特征的处理："></a>连续值数字特征的处理：</h3><p>例如在小猫分类器中加入体重这个特征，它是离散的数值。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151720456.webp" alt="image-20230915172018220"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151722745.webp" alt="image-20230915172224452"></p><ul><li>如上图所示，尝试一些值来拆分数据集，并计算信息增益，选择信息增益最高的一次作为拆分依据。一般会沿着x轴取多个值，一种惯例是根据此特征的值对所有示例进行排序，并取所有排序列表之间的中点的值。考虑此处作为阈值。（例如10个训练样本，就要测试9个不同的值，然后选择提供最高信息增益的值作为阈值）</li></ul><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><ul><li>上述讨论了决策树分类算法，将决策树泛化，得到回归树，将可以预测数字等离散的值。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309151804527.webp" alt="image-20230915180453195"></p><ul><li>将输入的特征变为三个，而将体重变为输出。这就变成了一个回归问题。</li><li>拆分的方法变成了计算<strong>加权方差</strong>，然后计算方差的减少。选择最大的方差减少的特征作为拆分特征。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171315779.webp" alt="image-20230917131506332"></p><h2 id="使用多个决策树"><a href="#使用多个决策树" class="headerlink" title="使用多个决策树"></a>使用多个决策树</h2><ul><li>使用单个决策树的缺点之一是该决策树可能对数据中的微笑变化高度敏感。解决方法是构建多个决策树。</li><li>修改一个训练样本就导致算法再根部产生了不同的根部分裂。使用决策树时，通常会得到一个更好的结果，因为可以训练一大堆决策树而不是仅训练一个，这样得到的预测更加准确。</li><li>如果有一个想要分类的新预测样本，可以在新样本上运行所有的这三棵树，并选取票数最多结果的作为最终预测。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171351850.webp" alt="image-20230917135121683"></p><h3 id="有放回抽样"><a href="#有放回抽样" class="headerlink" title="有放回抽样"></a>有放回抽样</h3><ul><li>这么多种不同的决策树是如何生成的？<strong>进行有放回的抽样来替换训练集。</strong></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171405685.webp" alt="image-20230917140521502"></p><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><ul><li>进行有放回的抽样来生成与原始训练集大小一样的新训练集，训练决策树。然后尝试预测时，依旧采用投票的形式确定最终预测。</li><li>事实证明，如果将抽样产生新训练集的次数调的很大，实际没有什么太大改进。</li><li>关键思想是：有些抽样产生的新训练集所产生的决策树在进行拆分时与之前的决策树相似。</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171423104.webp" alt="image-20230917142329649"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171440713.webp" alt="image-20230917144048529"></p><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><ul><li><p>总结为<strong>刻意练习</strong>四个字，主要训练表现差的那部分。在抽样时，多多选择预测错误的训练集进行产生新的决策树。</p></li><li><p>梯度提升：XGBoost是梯度提升框架的一种实现，这是一种集成学习方法。梯度提升按顺序构建一组决策树，每棵树都纠正了前一棵树的错误，从而产生了一个高度准确的模型。</p></li><li><p>正则化：XGBoost在其目标函数中包括了内置的L1（Lasso回归）和L2（Ridge回归）正则化项。这有助于防止过拟合，使模型更加稳健。</p></li><li><p>处理缺失值：XGBoost具有处理缺失数据的强大方法。它可以在训练过程中自动学习如何最好地填补缺失值，减少了预处理的需求。</p></li><li><p>树剪枝：XGBoost采用一种称为“剪枝”的技术来控制集成中各个树的生长，防止它们变得过于深并过拟合数据。</p></li><li><p>并行处理：XGBoost被设计为高效处理，可以利用并行处理，适用于大型数据集和分布式计算环境。</p></li><li><p>交叉验证：该算法提供了内置的交叉验证功能，使超参数调整和模型性能评估更加容易。</p></li><li><p>特征重要性：XGBoost提供特征重要性分数，有助于识别数据集中最重要的特征，帮助特征选择和模型解释。</p></li><li><p>各种损失函数支持：XGBoost支持不同类型任务的各种损失函数，包括回归、二元分类和多类分类。</p></li><li><p>GPU加速支持：XGBoost可以使用GPU进行加速，进一步提高训练速度。</p></li><li><p>多功能性：XGBoost可用于广泛的机器学习任务，包括结构化数据、自然语言处理和推荐系统。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171456145.webp" alt="image-20230917145605950"></p><h2 id="何时选择使用决策树？"><a href="#何时选择使用决策树？" class="headerlink" title="何时选择使用决策树？"></a>何时选择使用决策树？</h2><ul><li>决策树和树通常适用于表格数据，也称为结构化数据。</li><li>不太适用于非结构化数据，例如图像、视频、音频、文本等</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171512326.webp" alt="image-20230917151232045"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309171515722.webp" alt="image-20230917151522517"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29151/"/>
      <url>/posts/29151/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习开发的迭代过程"><a href="#机器学习开发的迭代过程" class="headerlink" title="机器学习开发的迭代过程"></a><p align="center">机器学习开发的迭代过程</p></h1><ol><li>选择系统的框架：模型、数据、超参数等等。</li><li>训练模型。</li><li>诊断模型。</li><li>做出决定：是否扩大神经网络、更改正则化参数、添加数据、增加或者减少功能。</li><li>持续上述过程。</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131410115.webp" alt="image-20230913141043879"></p><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>​        手动检查算法的错误分类或错误标记的示例，尝试找出错误原因并解决。（根据误差收集更多数据）</p><h2 id="添加更多数据"><a href="#添加更多数据" class="headerlink" title="添加更多数据"></a>添加更多数据</h2><p>​        根据问题，添加对解决问题最有用的数据。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>​        机器学习数据增强是一种用于改善训练数据集的技术，旨在提高机器学习模型的性能和鲁棒性。数据增强的主要思想是通过对原始数据进行一系列变换或处理，生成新的训练样本，从而扩充训练数据集。这有助于模型更好地泛化到未见过的数据，并减少过拟合的风险。以下是一些常见的机器学习数据增强技术：</p><ol><li>图像数据增强：<ul><li>随机旋转：对图像进行随机旋转，以增加视角的多样性。</li><li>镜像翻转：水平或垂直翻转图像，以生成镜像样本。</li><li>随机裁剪：随机裁剪图像的一部分，以改变图像的大小和内容。</li><li>调整亮度和对比度：随机调整图像的亮度和对比度，以模拟不同的光照条件。</li><li>增加噪声：向图像中添加随机噪声，以提高模型对噪声的鲁棒性。</li></ul></li><li>文本数据增强：<ul><li>同义词替换：替换文本中的一些词汇为其同义词，以生成新的句子。</li><li>随机删除和插入：随机删除文本中的词语或随机插入新的词语，以改变句子结构。</li><li>扰动：对文本进行随机扰动，如字符级别的替换或重排。</li><li>生成对抗样本：使用生成对抗网络（GANs）生成对抗样本，以使模型更加鲁棒。</li></ul></li><li>音频数据增强：<ul><li>变速和变调：改变音频的播放速度和音调，以增加多样性。</li><li>加噪声：向音频中添加随机噪声，以提高模型对噪声的鲁棒性。</li><li>时间剪辑：随机剪辑音频的一部分，以改变音频的长度和内容。</li></ul></li></ol><p>​        数据增强技术的选择取决于任务和数据类型。在训练机器学习模型之前，通常需要仔细考虑数据增强策略，以确保生成的样本仍然保持与实际数据的一致性。数据增强可以帮助提高模型的性能，特别是在数据有限的情况下，但也需要小心以避免引入不良影响。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131449503.webp" alt="image-20230913144953300"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131506848.webp" alt="image-20230913150623575"></p><h2 id="迁移学习-使用其他任务中的数据"><a href="#迁移学习-使用其他任务中的数据" class="headerlink" title="迁移学习_使用其他任务中的数据"></a>迁移学习_使用其他任务中的数据</h2><p>​        针对神经网络模型的参数训练：</p><ol><li><strong>仅训练输出层的参数</strong>：<ul><li>这种方式被称为特征提取或冻结底层特征表示。在这种策略中，模型的底层（通常是卷积神经网络或预训练的特征提取器）的权重通常被冻结，不再进行训练，而<strong>只有输出层的参数</strong>（通常是分类器）被训练以适应新任务。</li><li>这种方法适用于源任务和目标任务之间的特征表示相似，但分类或预测的任务不同。冻结底层特征表示可以防止在目标任务上过拟合，并且通常需要较少的训练数据。</li></ul></li><li><strong>训练所有层的参数</strong>：<ul><li>这种方式被称为微调。在微调中，除了输出层之外，模型的底层参数也会被训练，（以之前的参数值为初始值）但训练速度通常比较慢，因为这些底层参数已经包含了预训练任务的知识。</li><li>这种方法适用于源任务和目标任务之间的特征表示相似性较高，或者需要在目标任务上进行更多的模型调整以适应新任务。</li></ul></li></ol><p>​        选择哪种策略取决于迁移学习任务的特性。通常情况下，如果源任务和目标任务之间有很大的相似性，微调整个模型的参数可能会更有效，因为模型可以从源任务中受益。然而，如果源任务和目标任务之间差异很大，只训练输出层的参数可能会更合适，因为这可以减少过拟合的风险。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131538345.webp" alt="image-20230913153802116"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131559030.webp" alt="image-20230913155954813"></p><p>​        迁移学习是一项重要的机器学习技术，允许将从一个任务中学到的知识迁移到另一个相关任务中，以改善模型性能和加速训练过程。以下是迁移学习的主要要点和总结：</p><ol><li><strong>核心思想</strong>：迁移学习的核心思想是利用已有知识来帮助解决新任务，即使新任务的数据分布或特性与原任务不完全相同。</li><li><strong>两种主要策略</strong>：迁移学习通常包括两种主要策略：<ul><li><strong>基于特征的迁移学习</strong>：重用底层特征表示，通常冻结底层，只训练输出层的参数。</li><li><strong>基于模型的迁移学习</strong>：重用整个或部分预训练模型，包括底层特征表示，可能微调整个模型的参数。</li></ul></li><li><strong>适用场景</strong>：选择迁移学习策略取决于源任务和目标任务之间的相似性和差异性。<ul><li>如果源任务和目标任务之间相似，微调整个模型可能更有效。</li><li>如果源任务和目标任务之间差异大，只训练输出层的参数可能更合适。</li></ul></li><li><strong>应用领域</strong>：迁移学习广泛应用于计算机视觉、自然语言处理、语音识别等领域，用于提高模型性能和泛化能力。</li><li><strong>数据有限性</strong>：迁移学习特别有用，当目标任务的数据量有限或难以获得大规模标记数据时，可以借助已有知识来加速模型的学习过程。</li><li><strong>实验和调整</strong>：在应用迁移学习时，通常需要进行实验和调整，以确定哪种策略对特定任务最有效。有时可以结合不同策略以获得最佳性能。</li></ol><h2 id="机器学习项目的完整周期"><a href="#机器学习项目的完整周期" class="headerlink" title="机器学习项目的完整周期"></a>机器学习项目的完整周期</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131626841.webp" alt="image-20230913162659552"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131635991.webp" alt="image-20230913163549719"></p><h2 id="倾斜数据集的误差指标"><a href="#倾斜数据集的误差指标" class="headerlink" title="倾斜数据集的误差指标"></a>倾斜数据集的误差指标</h2><p>​        误差最小的预测可能不是特别有用的预测。在处理倾斜数据集的问题时，我们通常使用不同的误差度量，而不仅仅是分类误差。</p><p>​         一对常见的错误指标是<strong>精确率</strong>和<strong>召回率</strong>，</p><p>​        引入<strong>混淆矩阵</strong>：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309131906587.webp" alt="image-20230913190612369"></p><ul><li><p><strong>精确率定义为：预测结果中真阳性的数量与分类为阳性数量的比值。</strong></p></li><li><p><strong>召回率定义为：预测结果中真阳性的数量与实际为阳性数量的比值。</strong></p></li><li><p>一般来说，零准确率和零召回率的算法是无用算法。</p></li><li><p><strong>精确率很高：说话靠谱。</strong></p></li><li><p><strong>召回率很高：遗漏率很低。</strong></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&quot;准确率&quot;（Accuracy）和&quot;精确率&quot;（Precision）是两个不同的性能评估指标，通常用于衡量分类模型的性能，特别是在二分类问题中。它们有不同的计算方法和重要性。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. 准确率（Accuracy）： 准确率是一个分类模型的总体性能度量，它衡量了模型正确分类样本的比例。准确率的计算方法如下： </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">准确率 = 正确分类的样本数/总样本数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">准确率的优点是简单易懂，但它有一个问题，就是对于不平衡的数据集来说，可能会给出误导性的结果。如果数据集中某个类别的样本比其他类别多得多，模型只需预测多数类别，也能获得高准确率。因此，在不平衡数据集中，准确率可能不是一个很好的性能指标。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. 精确率（Precision）： 精确率衡量了模型在预测为正类别的样本中，实际为正类别的比例。精确率的计算方法如下： </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">精确率 = 真正例/(真正例+假正例)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">其中，真正例是模型正确预测为正类别的样本数，假正例是模型错误预测为正类别的样本数。精确率强调了模型对正类别的预测准确性，对于那些需要高度准确性的应用，精确率是一个重要的指标。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="精确率与召回率的权衡"><a href="#精确率与召回率的权衡" class="headerlink" title="精确率与召回率的权衡"></a>精确率与召回率的权衡</h2><ul><li><p>提高分类阈值会提高精确度，但是会导致较低的召回率。另一种相反的情况例如做核酸，就是为了提高召回率，“宁可错杀一千，也不会放过一个“，这样召回率高，但精确率变低。<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309132013257.webp" alt="image-20230913201349928">     </p></li><li><p>当   <em><strong>f</strong></em>   高于某个阈值时，我们通过设置这个阈值，可以很好的在精确度和召回率之间做出不同的权衡。</p></li><li><p>notice：选择与阈值并不是要通过交叉验证做的事情，手动选择阈值权衡是我们要做的。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># F1分数</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">F1分数是一个综合考虑精确率（Precision）和召回率（Recall）的性能评估指标，通常用于评估二分类模型的性能。它衡量了模型在同时考虑精确性和完整性的情况下的性能。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">F1分数的计算方法如下：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">F1=2⋅(精确率⋅召回率)/(精确率+召回率)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">F1分数通过精确率和召回率的调和平均来综合考虑两者，这有助于平衡模型的精确性和完整性。F1分数的取值范围在0和1之间，值越高表示模型性能越好。当模型的精确率和召回率都很高时，F1分数也会很高，反之亦然。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">F1分数特别适用于处理不平衡数据集，其中一个类别的样本数量远远多于另一个类别。在这种情况下，如果只使用精确率或召回率来评估模型，可能会导致误导，因为模型可能会倾向于预测为占主导地位的类别。F1分数帮助平衡了这两个指标，使其更公平地考虑了正类别和负类别的性能。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p align="center">`**P和R的调和平均数**`</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309132048000.webp" alt="image-20230913204842787"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Classification</title>
      <link href="/posts/29150/"/>
      <url>/posts/29150/</url>
      
        <content type="html"><![CDATA[<h1 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a><p align="center">Image Classification</p></h1><ul><li><p>本次主要实践内容是在图像分类任务中训练和评估不同的神经网络模型，目前尝试<strong>MobileNetV2、MobileNetV3_small、ResNet18、ResNet34、ShuffleNetV1、ShuffleNetV2、VGG11、VGG11_bn</strong>，并对模型进行了评估。</p></li><li><p>通过实践，发现在不同模型上的性能达到了一定的高度，但还有许多提升空间，后续会继续<strong>使用不同模型、修改超参数、数据预处理、学习率更新策略、优化器等方面进行优化。添加类别激活图可视化、学习策略可视化、数据增强策略可视化。</strong></p></li><li><p>本次实践具体内容为对花卉图像进行分类，以下内容为不同模型的性能指标和混淆矩阵。</p></li></ul><hr><p>​涉及模型的内容，在 <strong>model_cfg</strong> 中，一个完整的模型由 <strong>backbone</strong>、<strong>neck</strong> 、<strong>head</strong>；损失的计算集成在<strong>head</strong>中。</p><p>​<code>eval()</code> 是Python内置函数之一，用于计算传递给它的表达式并返回结果。它将接受的字符串作为一个有效的Python表达式进行解析和计算。<code>eval()</code> 函数的使用案例包括：</p><ul><li><strong>计算数学表达式：</strong> 可以用于执行数学表达式，例如计算数学运算。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = <span class="built_in">eval</span>(<span class="string">&quot;2 + 3 * 4&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># 输出：14</span></span><br></pre></td></tr></table></figure><ul><li><strong>动态执行代码：</strong> 可以用于执行动态生成的代码。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">5</span></span><br><span class="line">y = <span class="number">10</span></span><br><span class="line">expression = <span class="string">&quot;x + y&quot;</span></span><br><span class="line">result = <span class="built_in">eval</span>(expression)</span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># 输出：15</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><strong>创建自定义函数： 可以用于动态创建函数。</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dynamic_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">eval</span>(<span class="string">&quot;x * 2 + 1&quot;</span>)</span><br><span class="line"></span><br><span class="line">result = dynamic_function(<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># 输出：7</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/train.py models/  /</span><br></pre></td></tr></table></figure><p>评估：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools/evaluation.py models/  /</span><br></pre></td></tr></table></figure><h3 id="数据预处理-数据增强"><a href="#数据预处理-数据增强" class="headerlink" title="数据预处理|数据增强"></a>数据预处理|数据增强</h3><p>五种花卉图片，共计3670张图片。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121723220.webp" alt="image-20230912172315065"></p><ul><li><p>标签文件制作</p></li><li><p>数据集划分</p></li><li><p>数据集信息文件制作</p></li><li><p>转换与编辑图像标注文件</p></li></ul><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul><li><strong><a href="https://ieeexplore.ieee.org/document/6795724">LeNet5</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124461111">AlexNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124477080">VGG</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124630832">DenseNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124477575">ResNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124754437">Wide-ResNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124477919">ResNeXt</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124478157">SEResNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124478347">SEResNeXt</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124478426">RegNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124478681">MobileNetV2</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124478770">MobileNetV3</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124479156">ShuffleNetV1</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124479336">ShuffleNetV2</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124754493">EfficientNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124479644">RepVGG</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124479467">Res2Net</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124481466">ConvNeXt</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124481590">HRNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124481766">ConvMixer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124481930">CSPNet</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124538198">Swin-Transformer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124567953">Vision-Transformer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124596023">Transformer-in-Transformer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124596093">MLP-Mixer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124591888">DeiT</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124596343">Conformer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124596425">T2T-ViT</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124596619">Twins</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124596740">PoolFormer</a></strong></li><li><strong><a href="https://blog.csdn.net/zzh516451964zzh/article/details/124630541">VAN</a></strong></li><li><strong><a href="https://arxiv.org/pdf/2207.14284v2.pdf">HorNet</a></strong></li><li><strong><a href="https://arxiv.org/abs/2206.01191">EfficientFormer</a></strong></li><li><strong><a href="https://arxiv.org/abs/2111.09883.pdf">Swin Transformer V2</a></strong></li><li><strong><a href="http://openaccess.thecvf.com//content/CVPR2022/papers/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf">MViT V2</a></strong></li><li><strong><a href="https://arxiv.org/abs/2110.02178">MobileViT</a></strong></li><li><strong><a href="https://arxiv.org/abs/2204.03645v1">DaViT</a></strong></li><li><strong><a href="https://arxiv.org/abs/2203.06717">replknet</a></strong></li><li><strong><a href="https://arxiv.org/abs/2106.08254">BEiT</a></strong></li><li><strong><a href="https://arxiv.org/abs/2211.07636">EVA</a></strong></li><li><strong><a href="https://arxiv.org/abs/2205.13137">MixMIM</a></strong></li><li><strong><a href="https://arxiv.org/abs/2104.00298">EfficientNetV2</a></strong></li></ul><h3 id="预训练权重"><a href="#预训练权重" class="headerlink" title="预训练权重"></a>预训练权重</h3><h3 id="学习率更新策略-优化器"><a href="#学习率更新策略-优化器" class="headerlink" title="学习率更新策略|优化器"></a>学习率更新策略|优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@repo&#123;2020mmclassification,</span></span><br><span class="line">    title=&#123;OpenMMLa<span class="string">b&#x27;s Image Classification Toolbox and Benchmark&#125;,</span></span><br><span class="line"><span class="string">    author=&#123;MMClassification Contributors&#125;,</span></span><br><span class="line"><span class="string">    howpublished = &#123;\url&#123;https://github.com/open-mmlab/mmclassification&#125;&#125;,</span></span><br><span class="line"><span class="string">    year=&#123;2020&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><h2 id="1、mobilenet-v3-small"><a href="#1、mobilenet-v3-small" class="headerlink" title="1、mobilenet_v3_small"></a>1、mobilenet_v3_small</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/image-20230816230759363.webp" alt="image-20230816230759363"></p><ul><li><strong>Backbone（骨干网络）：</strong> 骨干网络是模型的基础部分，用于从输入数据中提取特征。在这里，使用了MobileNetV3 作为骨干网络。MobileNetV3 是一种轻量级的卷积神经网络架构，适用于移动设备和计算资源受限的场景。</li><li><strong>Neck（特征提取部分）：</strong> 特征提取部分是在骨干网络之后添加的层，用于进一步提取抽象特征。在这里，您使用了 Global Average Pooling，这是一种用于减少特征图尺寸的操作，将每个特征通道的平均值作为全局特征表示。</li><li><strong>Head（分类头部）：</strong> 分类头部是模型的最后一层，用于将从特征提取部分得到的特征映射到不同类别的预测。使用了 Stacked Linear Classification Head，这是一个堆叠的线性分类层，用于生成类别预测。</li><li><strong>Loss（损失函数）：</strong> 损失函数用于衡量模型的预测与真实标签之间的差距。使用了 Cross Entropy Loss（交叉熵损失），这是分类任务中常用的损失函数，用于优化模型参数，使预测结果更接近真实标签。</li></ul><p><strong>三种情况下保存权重：</strong></p><ul><li>最近一次周期</li><li>损失最小的周期</li><li>验证准确率最高的周期</li></ul><p><strong>模型评估：</strong></p><p><strong>batch_size &#x3D; 16；loss_weight&#x3D;1.0；lr&#x3D;0.001；loss&#x3D;0.059</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308162319981.webp" alt="image-20230816231903794"></p><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171228867.webp" alt="image-20230817122804701"></p><blockquote><p>以第一行为例介绍混淆矩阵：</p><ul><li><strong>daisy 类别被正确预测（True Positive）：</strong> 在第一行的第一个格子中，数字 117 表示实际为 “daisy” 的样本中，被正确预测为 “daisy” 的数量。这些样本被成功地分类到了正确的类别。</li><li><strong>daisy 类别被错误预测为其他类别（False Positives）：</strong> 在第一行的其他列中的数字，例如 2、3、1、3，表示实际为 “daisy” 的样本被错误地预测为其他类别。例如，有 2 个样本被错误地预测为 “dandelion”，有 3 个样本被错误地预测为 “roses”，等等。</li><li><strong>daisy 类别的召回率（Recall）：</strong> 召回率表示实际属于某个类别的样本中，被模型预测为该类别的比例。在这里，召回率可以计算为 (117 &#x2F; (117 + 2 + 3 + 1 + 3)) &#x3D; 0.9286，即约 92.86%。这表示模型对于实际为 “daisy” 的样本中，有约 92.86% 被成功预测为 “daisy”。</li><li><strong>daisy 类别的精确率（Precision）：</strong> 精确率表示模型预测为某个类别的样本中，实际属于该类别的比例。在这里，精确率可以计算为 (117 &#x2F; (117 + 2 + 3 + 1 + 3)) &#x3D; 0.8931，即约 89.31%。这表示在模型预测为 “daisy” 的样本中，有约 89.31% 真正属于 “daisy” 类别。</li><li><strong>daisy 类别的 F1 分数：</strong> F1 分数是精确率和召回率的调和平均值，可以综合考虑模型的准确性和覆盖率。在这里，您的数据没有提供 “daisy” 类别的精确率和召回率，因此无法计算 F1 分数。</li></ul></blockquote><hr><p><strong>batch_size &#x3D; 16；loss_weight&#x3D;0.9；lr&#x3D;0.001；loss&#x3D;0.053</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171226672.webp" alt="image-20230817122643492"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171227745.webp" alt="image-20230817122731555"></p><hr><p><strong>batch_size &#x3D; 16；loss_weight&#x3D;0.9；lr&#x3D;0.0003；loss&#x3D;0.0040</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171310185.webp" alt="image-20230817131038989"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171312541.webp" alt="image-20230817131200362"></p><hr><ol><li><strong>数据增强策略的添加：</strong> 代码引入了一个名为 <code>policies</code> 的列表，其中包含了多种不同的数据增强策略。这些策略是一系列数据增强操作的组合，将按顺序应用于训练图像。第一段代码没有明确包含这种策略，可能需要在代码中添加数据增强操作。</li><li><strong>数据预处理策略的不同：</strong> 第二段代码中的数据预处理策略与第一段代码不同。第一段代码中，训练数据的预处理包括随机裁剪、随机翻转、标准化等。而第二段代码中，使用了一系列复杂的数据增强操作，例如 Posterize、Rotate、Solarize 等。这些操作的目的是提高模型的鲁棒性和泛化能力。</li></ol><p>更改后的优点：</p><ol><li><strong>增强数据多样性：</strong>数据增强策略更加丰富多样，涵盖了多种增强操作。这可以帮助模型在训练过程中更好地适应不同的变化和场景，从而提高模型的泛化能力。</li><li><strong>提高模型鲁棒性：</strong> 复杂的数据增强操作，如 Solarize、AutoContrast 等，可以在一定程度上模拟现实世界中的异常情况，从而使模型更具鲁棒性，能够更好地处理异常数据。</li><li><strong>减少过拟合风险：</strong> 数据增强可以有效减少过拟合风险，因为它引入了更多的变化和噪声，使得模型不仅仅学习训练集中的特定样本。</li><li><strong>减少数据不平衡影响：</strong> 一些增强操作可以平衡不同类别之间的样本数量，从而减少因类别不平衡导致的模型偏见。</li></ol><p>​        总体而言，通过引入更丰富的数据增强操作，可以提高模型的性能和泛化能力，使其在不同情况下表现得更好。然而，增强操作的选择和配置需要谨慎考虑，因为过多或不恰当的增强操作可能会影响训练稳定性和效果。</p><p><strong>batch_size &#x3D; 16；loss_weight&#x3D;1.0；lr&#x3D;0.001；loss&#x3D;0.133</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171357536.webp" alt="image-20230817135723285"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171359328.webp" alt="image-20230817135908146"></p><hr><p><strong>batch_size &#x3D; 32；loss_weight&#x3D;1.0；lr&#x3D;0.001；loss&#x3D;0.080</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171817878.webp" alt="image-20230817181654647"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171819853.webp" alt="image-20230817181920681"></p><h2 id="2、mobilenet-v2"><a href="#2、mobilenet-v2" class="headerlink" title="2、mobilenet_v2"></a>2、mobilenet_v2</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171906213.webp" alt="image-20230817190617893"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308171909695.webp" alt="image-20230817190901400"></p><p>批量图片检测</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181029513.webp" alt="image-20230818102906307"></p><h2 id="3、VGG11"><a href="#3、VGG11" class="headerlink" title="3、VGG11"></a>3、VGG11</h2><p>​</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">主要不同在于 `norm_cfg` 参数。在第一个示例中，使用了批归一化（BN）作为规范化层，而在第二个示例中没有使用任何规范化层。批归一化可以帮助网络更快地训练，提高模型的稳定性和收敛性。没有使用规范化层可能会导致训练过程较慢，收敛较慢。</span></span><br><span class="line"><span class="string">两个示例都构建了一个具有11层深度的VGG主干网络，用于分类任务，并指定了相同的类别数量。不同之处在于是否使用批归一化作为规范化层。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>epoches&#x3D;100</strong></p><p><strong>batch_size &#x3D; 16；num_workers &#x3D; 2；loss_weight&#x3D;1.0；lr&#x3D;0.1*16&#x2F;256 ；loss&#x3D;0.051</strong></p><p>预训练权重：<strong>vgg11_batch256_imagenet_20210208-4271cd6c</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181055345.webp" alt="image-20230818105554203"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181154012.webp" alt="image-20230818115440746"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181156758.webp" alt="image-20230818115602491"></p><hr><p><code>norm_cfg=dict(type=&#39;BN&#39;)</code>：指定了使用批归一化（Batch Normalization）作为规范化层。</p><p><strong>batch_size &#x3D; 16；num_workers &#x3D; 2；loss_weight&#x3D;1.0；lr&#x3D;0.1*16&#x2F;256 ；loss&#x3D;0.046</strong></p><p>预训练权重：<strong>vgg11_bn_batch256_imagenet_20210207-f244902c.pth</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181312688.webp" alt="image-20230818131242421"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181314878.webp" alt="image-20230818131404573"></p><h2 id="4、Resnet"><a href="#4、Resnet" class="headerlink" title="4、Resnet"></a>4、Resnet</h2><h3 id="Resnet18"><a href="#Resnet18" class="headerlink" title="Resnet18"></a>Resnet18</h3><p><strong>epoches&#x3D;100</strong></p><p><strong>batch_size &#x3D; 16；num_workers &#x3D; 2；loss_weight&#x3D;1.0；lr&#x3D;0.1*16&#x2F;256 ；loss&#x3D;0.133</strong></p><p>预训练权重：<strong>resnet18_8xb32_in1k_20210831-fbbb1da6.pth</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181705618.webp" alt="image-20230818170518446"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181757520.webp" alt="image-20230818175735297"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308181759064.webp" alt="image-20230818175913802"></p><hr><p><strong>batch_size &#x3D; 8；num_workers &#x3D; 2；loss_weight&#x3D;1.0；lr&#x3D;0.1*8&#x2F;256 ；loss&#x3D;0.163</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191107220.webp" alt="image-20230819110657946"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191108065.webp" alt="image-20230819110828762"></p><h3 id="Renet34"><a href="#Renet34" class="headerlink" title="Renet34"></a>Renet34</h3><p>预训练权重：<strong>resnet34_8xb32_in1k_20210831-f257d4e6.pth</strong></p><p><strong>batch_size &#x3D; 16；num_workers &#x3D; 2；loss_weight&#x3D;1.0；lr&#x3D;0.1*16&#x2F;256 ；loss&#x3D;0.122</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191124185.webp" alt="image-20230819112428988"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191222114.webp" alt="image-20230819122207852"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191223392.webp" alt="image-20230819122314098"></p><h2 id="5、ShuffleNet"><a href="#5、ShuffleNet" class="headerlink" title="5、ShuffleNet"></a>5、ShuffleNet</h2><h3 id="ShuffleNetV1"><a href="#ShuffleNetV1" class="headerlink" title="ShuffleNetV1"></a>ShuffleNetV1</h3><p>预训练权重：<strong>shufflenet_v1_batch1024_imagenet_20200804-5d6cec73 .pth</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191335390.webp" alt="image-20230819133512216"></p><p><strong>batch_size &#x3D; 16；num_workers &#x3D; 2；loss_weight&#x3D;1.0；lr&#x3D;0.5*16&#x2F;256 ；loss&#x3D;0.044</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191429702.webp" alt="image-20230819142944418"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191430293.webp" alt="image-20230819143054972"></p><hr><h3 id="ShuffleNetV2"><a href="#ShuffleNetV2" class="headerlink" title="ShuffleNetV2"></a>ShuffleNetV2</h3><p>预训练权重：<strong>shufflenet_v2_batch1024_imagenet_20200812-5bf4721e.pth</strong></p><p><strong>batch_size &#x3D; 32；num_workers &#x3D; 4；loss_weight&#x3D;1.0；lr&#x3D;0.5*32&#x2F;256 ；loss&#x3D;0.0</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191647044.webp" alt="image-20230819164730799"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308191648317.webp" alt="image-20230819164840103"></p>]]></content>
      
      
      <categories>
          
          <category> Image Classification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29149/"/>
      <url>/posts/29149/</url>
      
        <content type="html"><![CDATA[<h1 id="下一步"><a href="#下一步" class="headerlink" title="下一步"></a><p align="center">下一步</p></h1><h2 id="模型诊断"><a href="#模型诊断" class="headerlink" title="模型诊断"></a>模型诊断</h2><p>​在机器学习中，模型诊断是指通过分析模型的性能、预测结果以及其他相关指标来评估模型的质量和表现。这有助于发现模型中的问题、改进模型并了解其如何作出预测。以下是一些常见的机器学习诊断方法和技术：</p><ol><li><strong>性能指标分析</strong>：分析模型的性能指标，如准确率、精确率、召回率、F1分数等，以了解模型在不同类别上的表现如何。</li><li><strong>学习曲线分析</strong>：绘制模型在训练集和验证集上的学习曲线，以检查模型是否过拟合或欠拟合。</li><li><strong>特征重要性分析</strong>：通过分析模型所学习的特征权重，可以了解哪些特征对模型的预测贡献较大。</li><li><strong>错误分析</strong>：分析模型在预测中犯错的情况，了解模型常见的错误类型和原因。</li><li><strong>可视化分析</strong>：利用可视化工具展示模型的预测结果、决策边界等，帮助理解模型的工作原理。</li><li><strong>交叉验证</strong>：通过交叉验证方法来估计模型在未见数据上的性能，以避免过度拟合。</li><li><strong>模型比较</strong>：将不同模型的性能进行比较，以确定哪个模型在特定任务上表现最佳。</li><li><strong>超参数调整</strong>：调整模型的超参数，例如学习率、正则化参数等，以优化模型的性能。</li><li><strong>集成方法分析</strong>：分析集成方法（如随机森林、梯度提升等）的预测结果，了解它们如何结合多个基模型。</li><li><strong>时间序列分析</strong>：对于时间序列数据，可以分析模型在不同时间点上的预测表现，探索模型对趋势和季节性的捕捉能力。</li></ol><p>​        总之，机器学习诊断是一个多方面的过程，涉及对模型、数据和结果的深入分析，以便更好地理解模型的行为并做出必要的改进。</p><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><ul><li><p>获取一个数据集并将其分为<strong>训练集</strong>和<strong>测试集</strong>，可以系统的评估学习成果如何。</p></li><li><p>测试集和训练集的均方误差。（不包括正则化项）通常对计算机有用的是训练误差，它是衡量学习算法在训练集上的表现。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309072026399.webp" alt="image-20230907202650328"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309072035534.webp" alt="image-20230907203521249"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309072104582.webp" alt="image-20230907210404387"></p><p>​    <code>可以在测试集中计算**y_hat**不等于测试集中实际标签的示例分数。</code></p><h2 id="模型选择，交叉评估验证"><a href="#模型选择，交叉评估验证" class="headerlink" title="模型选择，交叉评估验证"></a>模型选择，交叉评估验证</h2><p>​         测试不同的阶数，比较   <em><strong>J_test</strong></em>   大小，得到性能好坏。但<code>**有缺陷。**</code>可能是乐观估计。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309102109004.webp" alt="image-20230910210906708"></p><p>​        可以将数据分为三个不同的子集：训练集、<strong>交叉验证集</strong>、测试集、</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309102115905.webp" alt="image-20230910211506635"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309102119998.webp" alt="image-20230910211953702"></p><ul><li><p>通常，不包括正则化项。中间这项除了被称为<strong>交叉验证错误</strong>外，通常也简称为<strong>验证错误</strong>。有了这些标准，就可以进行模型选择。</p></li><li><p>在<strong>交叉验证集上</strong>评估这些参数，选择   <em><strong>J_cv</strong></em>  最小的模型。</p></li><li><p>最后，如果想要获取<strong>泛化误差</strong>，再使用测试集计算   <em><strong>J_test</strong></em>   进行操作。</p></li><li><p>相当于两个测试集，cv用来判断项数，测试集用来判断误差。</p><p>​        <code>可以把训练集，验证集合，测试集合想成平时作业，月考，期末考，当你做了作业之后，通过月考成绩去调整平时作业的参数，最后通过期末考测试你这个学习成绩怎么样/</code></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309102211376.webp" alt="image-20230910221121073"></p></li></ul><hr><p>​        它可以自动做出决定，例如为线性回归模型选择什么阶多项式。也可为其他类型的模型进行选择。例如下面的神经网络结构：手写数字模型。</p><p>​        如下为一些不同大小的神经网络，可以使用   <em><strong>J_cv</strong></em>  评估神经网络性能，这是分类问题，最常见的选择是将其计算为<strong>算法错误分类的交叉验证示例的分数。</strong></p><p>​         使用三个模型进行计算，然后取交叉验证误差最低的模型。<strong>如果想要计算泛化误差，</strong>则使用测试集来评估选择的神经网络的性能。</p><p>​        <code>只有在选出一个模型作为最终模型之后。才可以在测试集上进行评估。选择模型或学习算法的过程并不需要查看测试集。</code></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309102232182.webp" alt="image-20230910223223741"></p><p>​        现在已经有了评估学习算法甚至自动选择模型的方法，接下来更深一步研究一些诊断示例。</p><h2 id="通过偏差的方法进行诊断"><a href="#通过偏差的方法进行诊断" class="headerlink" title="通过偏差的方法进行诊断"></a>通过偏差的方法进行诊断</h2><ul><li>高偏差（欠拟合）</li><li>高方差（过拟合）</li><li>诊断算法是否具有高偏差或者高方差的方法是查看算法在训练集和交叉验证集上的性能。看   <em><strong>J_cv</strong></em> 和   <em><strong>J_train</strong></em>   的大小。</li></ul><blockquote><p>中间这个图，   <em><strong>J_train</strong></em>  不太高表明它没有高偏差问题，而   <em><strong>J_cv</strong></em>  并不比   <em><strong>J_train</strong></em>  差很多，这表明它也没有高方差问题。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309111540688.webp" alt="image-20230911154042487"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309111643897.webp" alt="image-20230911164308628"></p><blockquote><ul><li>学习算法有高偏差或者欠拟合，关键是  <em><strong>J_train</strong></em>  是否高，如图的左部分，此时  <em><strong>J_cv</strong></em>  与   <em><strong>J_train</strong></em>  很接近。</li><li>高方差或者过拟合，  <em><strong>J_cv</strong></em>  远大于   <em><strong>J_train</strong></em>  ，同时   <em><strong>J_train</strong></em>  可能很低。</li></ul><p>训练神经网络时，<strong>有时</strong>也会看到高偏差和高方差。   <em><strong>J_train</strong></em>  高且   <em><strong>J_cv</strong></em>  远大于   <em><strong>J_train</strong></em>  。可能如小图部分所示，部分数据过拟合，部分欠拟合。</p></blockquote><p>​        <strong><code>高阶多项式在训练次数不够的时候就会既有高偏差和高方差</code></strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309111650909.webp" alt="image-20230911165022631"></p><p>​         正则化如何影响学习算法的偏差和方差？更好的理解何时应该使用正则化。</p><h2 id="正则化参数如何影响偏差、方差？"><a href="#正则化参数如何影响偏差、方差？" class="headerlink" title="正则化参数如何影响偏差、方差？"></a>正则化参数如何影响偏差、方差？</h2><p>​        <code>目的：使用交叉验证来为选择一个适合的正则化参数   ***λ***   </code></p><p>​        首先回顾一下正则化参数对模型的影响，以及此时对应的  <em><strong>J_train</strong></em>  和  <em><strong>J_cv</strong></em>  的情况。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121552461.webp" alt="image-20230912155219136"></p><p>​        按照之前的方法进行尝试   <em><strong>λ</strong></em>   取不同值时，模型参数以及  <em><strong>J_cv</strong></em>  的大小。</p><blockquote><p>左侧过拟合，右侧欠拟合。与上个图像刚好对称。</p></blockquote><p> <img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121611475.webp" alt="image-20230912161149154"></p><h2 id="制定一个用于性能评估的基准"><a href="#制定一个用于性能评估的基准" class="headerlink" title="制定一个用于性能评估的基准"></a>制定一个用于性能评估的基准</h2><ul><li><p>目的：为了判断  <em><strong>J_train</strong></em>  和  <em><strong>J_cv</strong></em>  何谓之高，何谓之低。</p></li><li><p>语音识别运行示例：查看训练误差是否远高于人的表现水平更有用。</p></li><li><p>基准性能水平指的是合理希望学习算法最终达到的误差水平。常见的方法就是衡量人类在这项任务上的表现。也可以根据 <em><strong>baseline</strong></em> 建立基准。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121634778.webp" alt="image-20230912163424534"></p><blockquote><p>比较 <em><strong>baseline performance</strong></em> 与  <em><strong>J_train</strong></em>  之间的差距，如果效果差，可能是高偏差。继续比较  <em><strong>J_train</strong></em>   与  <em><strong>J_cv</strong></em>  之间的差距，如果差距很大，可能遇到了高方差问题。</p></blockquote><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>​        在机器学习中，学习曲线（Learning Curve）是一种可视化工具，用于帮助我们理解模型的性能如何随着训练数据量的增加而变化。学习曲线通常显示了训练数据大小与模型的性能之间的关系。</p><p>学习曲线一般包括两个主要方面的信息：</p><ol><li><strong>训练集性能</strong>：这是指模型在用于训练的数据上的性能表现。通常，随着训练数据量的增加，模型在训练集上的性能会逐渐提高。这是因为更多的数据有助于模型更好地学习数据的模式和特征。训练集性能通常以训练误差（训练集上的损失）来衡量。</li><li><strong>验证集性能</strong>：这是指模型在用于验证的独立数据集上的性能表现。验证集通常用于评估模型的泛化能力，即模型对新数据的适应能力。随着训练数据量的增加，验证集性能可能会出现以下情况：<ul><li>随着数据量的增加，验证集性能逐渐提高，这表明模型的泛化能力在改善。</li><li>随着数据量的增加，验证集性能趋于稳定，这可能表示模型已经充分学习了可用数据的模式。</li><li>验证集性能在训练数据量增加后开始下降，这可能是由于过拟合（Overfitting）导致的，即模型在训练集上表现很好，但在验证集上表现较差。</li></ul></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121757190.webp" alt="image-20230912175728901"></p><p>​        <code>如果算法具有高偏差，如果仅仅添加更多的训练数据，不会降低错误率。</code>首先得拟合好，其次增加数据集数量才有用。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121807204.webp" alt="image-20230912180725889"></p><p>​        高方差和高偏差不同，增加数据集大小可以有助于改善模型性能。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121817233.webp" alt="image-20230912181714954"></p><h2 id="偏差和方差如何帮助决定下一步要做什么？"><a href="#偏差和方差如何帮助决定下一步要做什么？" class="headerlink" title="偏差和方差如何帮助决定下一步要做什么？"></a>偏差和方差如何帮助决定下一步要做什么？</h2><p>​        以下描述的很清楚了：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121949024.webp" alt="image-20230912194907612"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309121955743.webp" alt="image-20230912195521360"></p><h2 id="神经网络中的方差和偏差"><a href="#神经网络中的方差和偏差" class="headerlink" title="神经网络中的方差和偏差"></a>神经网络中的方差和偏差</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309122006372.webp" alt="image-20230912200611174"></p><p>​        平衡，存在于万物之间。神经网络是我们摆脱了必须权衡偏差和方差的困境。如果神经网络足够大，几乎可以很好地适应训练集。<strong>如下图是神经网络处理偏差的具体方法，也是神经网络如此强大的原因。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309122015567.webp" alt="image-20230912201550334"></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">精心涉及的正则化大型网络通常与较小的神经网络一样好甚至更好（这里指的是正则化后的大型神经网络可能没有高方差的问题）。大的神经网络使得过拟合的风险增加，适当的正则化使得神经网络变得卓越，但会使计算量增加。</span><br></pre></td></tr></table></figure><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202309122032797.webp" alt="image-20230912203210570"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29148/"/>
      <url>/posts/29148/</url>
      
        <content type="html"><![CDATA[<h1 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a><p align="center">优化方法</p></h1><h2 id="高级优化方法"><a href="#高级优化方法" class="headerlink" title="高级优化方法"></a>高级优化方法</h2><p>​<code>比梯度下降更快的训练神经网络</code></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131725457.webp" alt="image-20230813172540182"></p><p>​Adam（Adaptive Moment Estimation）是一种用于优化神经网络和深度学习模型训练的优化算法。它结合了随机梯度下降（SGD）的优点和<strong>自适应学习率</strong>方法，旨在提供更快的收敛速度和更稳定的训练过程。</p><p>​Adam算法的工作原理如下：</p><ol><li><strong>梯度计算</strong>：在每个训练批次中，计算模型参数关于损失函数的梯度，以了解参数如何影响损失。</li><li><strong>一阶矩估计（均值）</strong>：Adam维护了每个模型参数的指数移动平均梯度，称为一阶矩估计或梯度的均值。这有助于捕捉梯度的整体趋势。</li><li><strong>二阶矩估计（方差）</strong>：类似地，Adam还维护了每个参数的指数移动平均平方梯度，称为二阶矩估计或梯度的方差。这有助于估计参数更新的变化幅度。</li><li><strong>学习率调整</strong>：Adam根据一阶矩估计和二阶矩估计对每个参数的更新幅度进行调整。学习率被自适应地缩放，从而在不同参数和时间步骤上提供更好的平衡。</li><li><strong>参数更新</strong>：使用调整后的学习率，Adam更新模型参数，以减少损失函数。</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131732346.webp" alt="image-20230813173225176"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131733345.webp" alt="image-20230813173310075"></p><p>​<strong>选择Adam优化器：</strong> 在实例化神经网络模型时，选择Adam作为优化器。这可以在许多深度学习框架中通过一行代码来完成。</p><h3 id="其他的网络层类型"><a href="#其他的网络层类型" class="headerlink" title="其他的网络层类型"></a>其他的网络层类型</h3><h3 id="密集层"><a href="#密集层" class="headerlink" title="密集层"></a>密集层</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308132000365.webp" alt="image-20230813200009101"></p><p>​密集层（Dense Layer），也被称为全连接层（Fully Connected Layer），是神经网络中的一个基本构建模块，尤其在前馈神经网络和人工神经网络中常见。它是一种层，其中该层中的每个神经元（或节点）与前一层和后一层中的每个神经元都相连接。</p><p>​在密集层中，每个神经元从前一层的所有神经元接收输入，并产生一个输出，该输出传递给下一层中的所有神经元。密集层中每个神经元的输出是通过对其输入进行加权求和，然后应用激活函数来计算的。</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输出 = 激活函数(加权求和(输入) + 偏置)</span><br></pre></td></tr></table></figure><p>​密集层中神经元的数量和激活函数的选择通常由神经网络的架构和特定问题的要求决定。通过训练过程中调整权重和偏置，密集层用于学习数据中的复杂模式和关系。密集层的参数（权重和偏置）通过梯度下降等优化技术来学习。</p><p>​在神经网络架构中，密集层通常与其他类型的层一起堆叠，例如卷积层（用于处理像素排列的数据，如图像）或循环层（用于处理序列数据）。这种层类型在将输入数据转换为用于进行预测或分类的高级表示方面起着关键作用。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>​卷积层（Convolutional Layer）是神经网络中的一种重要层，主要用于处理具有<strong>网格状结构（如图像）</strong>的数据。卷积层通过使用卷积操作来从输入数据中提取特征，并且在图像处理和计算机视觉任务中取得了巨大的成功。</p><p>​卷积操作是一种数学运算，通过在输入数据上滑动一个称为<strong>卷积核（或滤波器）</strong>的小窗口，将窗口中的数据与<strong>卷积核的权重</strong>进行逐元素相乘，然后将结果相加，最终生成输出的一个值。通过在输入数据的不同位置应用卷积操作，卷积层可以捕获输入数据的局部特征，如边缘、纹理等。</p><blockquote><p>卷积层的主要特点包括：</p><ol><li><strong>权值共享</strong>：卷积层中的卷积核在整个输入数据上共享相同的权重。这导致卷积层具有对平移不变性，即模型可以识别相同特征的不同位置。</li><li><strong>局部连接</strong>：每个卷积核仅与输入数据的一小部分区域连接，从而减少了参数数量，降低了计算成本。</li><li><strong>池化层</strong>：卷积层之后通常会加入池化层，用于减少特征图的尺寸，并增强模型对平移和尺度变化的鲁棒性。</li><li><strong>多通道输入与输出</strong>：卷积层可以处理多通道的输入数据（例如彩色图像的RGB通道），并且可以产生多个输出通道的特征图。</li></ol><p>卷积神经网络（CNNs）是使用卷积层作为主要组件的深度学习模型，在图像识别、物体检测、语义分割等计算机视觉任务中表现出色。通过层层堆叠卷积层、池化层和全连接层，CNNs能够学习到数据的多层次抽象特征，从而实现高效的特征提取和模式识别。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308132007771.webp" alt="image-20230813200725547"></p><p>​每个神经元只查看部分像素，在卷积神经网络（CNN）中，让神经元只查看部分像素而不是全部像素有几个重要的原因：</p><ol><li><strong>减少参数数量，加快计算速度：</strong> 图像通常非常大，如果每个神经元都连接到整个图像的每个像素，网络的参数数量会变得极其庞大，导致计算和内存开销巨大。通过让神经元只查看部分像素，可以大大减少参数数量，从而使网络更轻量化且更容易训练，也不太容易过拟合。</li><li><strong>局部特征提取：</strong> 图像中的信息通常是局部相关的，即相邻区域的像素之间存在相关性。通过让神经元只查看部分像素，卷积神经网络可以更有效地捕获图像的局部特征，如边缘、纹理等。这有助于提取更有意义的特征并降低不相关信息的影响。</li><li><strong>平移不变性：</strong> 图像中的特征通常在不同位置都有出现，但位置可能不同。通过使用卷积操作，卷积神经网络可以实现平移不变性，即学习到的特征在图像中的不同位置都能够被识别，而不需要重新训练。</li><li><strong>稀疏连接：</strong> 让神经元只连接到局部区域的像素可以视为一种稀疏连接。稀疏连接有助于减少计算复杂性，使神经网络能够更高效地处理大规模输入数据。</li></ol><p>​<strong>总之，让神经元只查看部分像素而不是全部像素是为了降低计算和内存开销、捕获局部特征、实现平移不变性以及提高网络的有效性。这些策略在卷积神经网络的设计中起到了至关重要的作用，使其能够更好地处理图像数据并在计算机视觉任务中取得优异的性能。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/image-20230813201436920.webp" alt="image-20230813201436920"></p><p>​使用卷积神经网络（CNN）进行心电图（EKG信号）的分类是一个常见的任务，可以用于自动识别不同心脏疾病、心律失常等。</p><p>​第一个卷积层的神经元都是只查看部分EKG信号，隐藏层第二层也可以是卷积层，同理只查看部分激活，最后是一个sigmoid单元的输出层，以便对是否有心脏病进行二分类。</p><h3 id="反向传播如何计算导数？（optional）"><a href="#反向传播如何计算导数？（optional）" class="headerlink" title="反向传播如何计算导数？（optional）"></a>反向传播如何计算导数？（optional）</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308141635967.webp" alt="image-20230814163544710"></p><p>​我们得出导数的<strong>非正式定义</strong>，即每当 <em><strong>w</strong></em> 上升一个微小的<em><strong>Epsilon</strong></em>时，就会导致 <em><strong>w</strong></em> 的 <em><strong>」</strong></em>上升 <em><strong>k</strong></em> 倍 <em><strong>Epsilon</strong></em>。</p><p><em><strong>k</strong></em>  即为在此处的导数。</p><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308141710696.webp" alt="image-20230814171013485"></p><p>​前向传播是从左到右计算，但导数的计算是从右到左，这就是 <em><strong>BP</strong></em> 反向传播。反向传播（Backpropagation）是训练神经网络的核心算法之一，用于计算损失函数关于网络权重的梯度，反向传播的关键是<strong>链式法则</strong>，它允许将网络的输出误差分解成每个权重的贡献。这允许网络根据损失的梯度调整每个权重，以便在训练数据上获得更好的预测结果。</p><blockquote><p><strong>如何理解：</strong> 当神经网络进行前向传播时，输入通过多个层传递并产生预测输出。但是，我们的目标是通过调整网络权重，使得预测输出更接近实际目标值，从而减小损失。为了做到这一点，我们需要知道每个权重对输出误差的影响有多大，以便适当地调整它们。</p><p>链式法则在这里的作用是，它允许我们将输出误差从输出层回传到每一层的权重。具体而言，对于每个权重，<strong>我们可以计算输出误差关于该权重的偏导数，即该权重对误差的影响有多大。通过乘以这个偏导数，我们可以知道如何微调每个权重以减小输出误差。</strong>(****)</p><p>这个过程可以被看作是将整个损失函数中的变化传递回到每个神经元的激活值和权重，从而使得网络能够逐步调整自己以最小化误差。这就是反向传播的核心思想：<code>通过链式法则，将误差从输出层传播回到网络的各个层，以便计算梯度并进行权重更新，最终使得网络能够更好地拟合训练数据，提高预测能力。</code></p></blockquote><p>​反向传播计算导数的原理是：如上图   <em><strong>d</strong></em>   变化微小，如  <em><strong>Epsilon</strong></em>  ， <em><strong>」</strong></em>如何变化？</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308141732319.webp" alt="image-20230814173256090"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308141735670.webp" alt="image-20230814173525449"></p><blockquote><p>以下是为什么反向传播适合计算导数的几个关键原因：</p><ol><li><strong>链式法则的应用：</strong> 反向传播基于链式法则，这是一种将复合函数的导数分解为各个组成部分导数的方法。在神经网络中，每一层的输出都是由前一层的输出和权重相乘得到的，通过逐层应用链式法则，我们可以将整个网络的导数分解为各层之间的局部导数。</li><li><strong>计算效率：</strong> 反向传播通过有效地传播误差信号和局部梯度，从输出层向输入层逐层计算梯度。这种计算的顺序与前向传播的顺序相反，因此称为“反向”传播。这种逐层计算梯度的方式使得计算效率更高，尤其在深层网络中。</li><li><strong>复用中间结果：</strong> 在前向传播过程中，神经元的激活值和权重相乘的中间结果被保留下来，以便在反向传播时使用。这样，可以避免重复计算，提高计算效率。</li><li><strong>自动计算：</strong> 反向传播允许计算机自动地计算网络中各个层的导数，而无需手动计算。这对于深层网络和复杂的函数非常有用，因为手动计算导数可能会非常繁琐和容易出错。</li></ol><p>总之，反向传播利用了链式法则，将导数的计算分解为网络的不同部分，从而实现高效的梯度计算。这使得神经网络能够在训练过程中自动调整权重以最小化损失函数，从而提高模型的性能。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308141742473.webp" alt="image-20230814174208256"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29147/"/>
      <url>/posts/29147/</url>
      
        <content type="html"><![CDATA[<h1 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a><p align="center">多分类</p></h1><h2 id="多分类-1"><a href="#多分类-1" class="headerlink" title="多分类"></a>多分类</h2><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a><strong>Softmax</strong></h3><p>​（软最大值）是一种在机器学习和神经网络中经常使用的数学函数，特别是在分类任务的上下文中。<strong>它用于将一组实数向量转换为概率分布</strong>，其中向量中的每个元素都被转换为介于 0 和 1 之间的值，表示该元素是最可能的选择的概率。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308152122898.webp" alt="image-20230815212231672"></p><p>​Softmax 函数以任意实数向量作为输入，并计算向量中每个元素 <em>x</em><sub> <em>i</em> </sub> 的如下值：<br>$$<br>softmax(x_i)&#x3D;\frac {e^{x_i}}  {∑_{j&#x3D;1}^N e^{x_j}}<br>$$<br>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308152133569.webp" alt="image-20230815213340440"></p><p>这里：</p><ul><li><em>N</em> 是输入向量中的元素总数。</li><li><em>e</em> 是自然对数的底数，即欧拉数。</li><li><em>x</em><sub> <em>i</em> </sub> 是输入向量的第 <em>i</em> 个元素。</li></ul><p>​Softmax 函数对输入向量的每个元素进行指数化，然后通过将每个指数化值除以向量中所有指数化值的和来进行归一化。这种归一化确保输出值在 [0, 1] 范围内，并且它们的总和为 1，形成一个有效的概率分布。</p><p>​Softmax 经常用于神经网络的输出层，用于多类分类任务。给定网络产生的原始分数（logits），Softmax 函数将这些分数转换为不同类别上的概率分布，从而更容易解释和比较模型的预测结果。</p><p>​从数学上讲，Softmax 函数放大了输入向量中元素之间的差异，给予较大值更多的权重，抑制较小值。因此，输入向量中的最大值将主导结果概率，成为最可能的类别选择。</p><p>​Softmax 具有一些特性：</p><ol><li>由于指数化步骤，它对异常值比较敏感。</li><li>它是单调递增函数，因此较高的输入值将对应于较高的概率。</li><li>它是可微分的，适用于使用基于梯度的优化方法来训练神经网络。</li></ol><h3 id="Cost"><a href="#Cost" class="headerlink" title="Cost"></a>Cost</h3><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308122234613.webp" alt="image-20230812223452347"></p><p>​在机器学习和神经网络中，Softmax 通常与交叉熵损失（Cross-Entropy Loss）一起使用作为代价函数，特别是在多类别分类问题中。Softmax 用于将模型的原始分数转换为概率分布，而交叉熵损失用于衡量模型的预测与真实标签之间的差异。</p><p>​假设有一个具有 <em>C</em> 个类别的分类问题，每个类别用一个指标 <em><strong>y<sub> i </sub></strong></em> 表示，其中 <em><strong>i</strong></em> 是类别的索引。此外，假设模型的输出经过 Softmax 函数处理后得到的概率分布为 ***p<sub> i </sub>***，表示模型预测属于类别 <em><strong>p<sub> i </sub></strong></em> 的概率。</p><p>​交叉熵损失（Cross-Entropy Loss）用于衡量模型的预测与真实标签之间的差异，计算方式如下：<br>$$<br>L &#x3D; -∑_{i&#x3D;1}^{C} y_i * log(p_i)<br>$$<br><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308152135219.webp" alt="image-20230815213502096"></p><p>​这里：</p><ul><li><em><strong>y<sub> i </sub></strong></em> 是真实标签中类别 <em><strong>i</strong></em> 的指标，通常是一个 0 或 1。</li><li><em><strong>p<sub> i </sub></strong></em> 是模型预测属于类别 <em><strong>i</strong></em> 的概率，由 Softmax 函数计算得出。</li></ul><p>​交叉熵损失可以看作是模型预测分布与真实分布之间的差异的度量。在分类问题中，真实分布中只有一个类别的指标为 1，其他为 0，因此损失函数只关注模型对正确类别的预测。</p><p>​通过最小化交叉熵损失，模型将被鼓励更准确地预测样本的类别。训练过程中，优化算法（如梯度下降）会调整模型的参数，使损失逐渐减小，从而提升模型的分类性能。</p><h2 id="神经网络的Softmax输出"><a href="#神经网络的Softmax输出" class="headerlink" title="神经网络的Softmax输出"></a>神经网络的Softmax输出</h2><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131019403.webp" alt="image-20230813101904158"></p><p>当你在 TensorFlow 中使用 <code>softmax</code> 激活函数和 <code>SparseCategoricalCrossentropy</code> 损失函数时，你通常会按照以下步骤进行：</p><ol><li>定义模型的输出层，并在输出层使用 <code>softmax</code> 激活函数，以将原始分数转换为概率分布。</li><li>定义损失函数为 <code>SparseCategoricalCrossentropy</code>，并将 logits（原始分数）作为参数传递给它，而不需要显式地进行 softmax 转换。</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131023257.webp" alt="image-20230813102316988"></p><h2 id="Softmax的改进实现"><a href="#Softmax的改进实现" class="headerlink" title="Softmax的改进实现"></a>Softmax的改进实现</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308152123813.webp" alt="image-20230815212344571"></p><p>​如果您将输出层的激活函数更改为线性（linear)，那么您应该在使用 <code>SparseCategoricalCrossentropy</code> 损失函数时，将参数 <code>from_logits=True</code> 设置为确保正确的损失计算。</p><p>​在神经网络中，将输出层的激活函数设置为线性意味着网络输出不再是经过 softmax 转换的概率分布，而是原始分数（logits）。这时，为了计算交叉熵损失，您需要在损失函数中执行 softmax 转换，以将 logits 转换为概率分布。</p><p>​<strong>这样做会确保在计算损失函数时，将 logits 转换为概率分布，从而使损失计算更稳定，避免数值不稳定性问题，并且可以更好地优化模型以逼近真实标签分布。</strong></p><p>​<code>**从概念上讲，两种代码做的是同一件事，但后者数字更为准确。**</code></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131104759.webp" alt="image-20230813110435532"></p><p>​现在只有一个细节，那就是我们现在已经将神经网络更改为使用<strong>线性激活函数</strong>而不是softmax激活函数。神经网络的最后一层不再输出 a<sub> 1</sub><del>a<sub> 10</sub>，而是 z<sub> 1</sub></del>z<sub> 10</sub> </p><h3 id="逻辑回归的改进"><a href="#逻辑回归的改进" class="headerlink" title="逻辑回归的改进:"></a>逻辑回归的改进:</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131110775.webp" alt="image-20230813111053537"></p><h2 id="多标签分类问题"><a href="#多标签分类问题" class="headerlink" title="多标签分类问题"></a>多标签分类问题</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131619390.webp" alt="image-20230813161953110"></p><p>​多标签分类是一种机器学习任务，其中算法为输入实例分配多个标签。在传统的二元分类中，一个实例被分配到两个类别中的一个（例如，垃圾邮件或非垃圾邮件）。在多标签分类中，一个实例可以与多个类别同时关联。</p><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308131630467.webp" alt="image-20230813163012256"></p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29146/"/>
      <url>/posts/29146/</url>
      
        <content type="html"><![CDATA[<h1 id="训练、激活函数的选择"><a href="#训练、激活函数的选择" class="headerlink" title="训练、激活函数的选择"></a><p align="center">训练、激活函数的选择</p></h1><h2 id="1、Tensorflow实现"><a href="#1、Tensorflow实现" class="headerlink" title="1、Tensorflow实现"></a>1、Tensorflow实现</h2><ul><li><h3 id="回忆：回到识别手写数字0和1的例子："><a href="#回忆：回到识别手写数字0和1的例子：" class="headerlink" title="回忆：回到识别手写数字0和1的例子："></a>回忆：回到识别手写数字0和1的例子：</h3></li></ul><ol><li><strong>指定模型</strong>，设置隐藏层，神经网络单元，激活函数，如何计算推理；</li><li><strong>编译模型</strong>，指定损失函数（图为分类交叉熵）；</li><li><strong>训练模型</strong>，调用<code>fit</code>函数，使用指定代价函数的损失来拟合指定数据集；</li></ol><p>​<code>epochs</code>表示学习梯度下降等算法设置的<strong>步数</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308102214269.webp" alt="image-20230810221419946"></p><ul><li><h3 id="模型训练细节："><a href="#模型训练细节：" class="headerlink" title="模型训练细节："></a>模型训练细节：</h3></li></ul><ol><li>给定输入，给定参数，计算输出；</li><li>指定损失和代价；</li><li>梯度下降，最小化逻辑回归的代价函数。</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111050156.webp" alt="image-20230811105040880"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111055621.webp" alt="image-20230811105550350"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111058327.webp" alt="image-20230811105832078"></p><p>​keras中有很多损失函数，但是恐怕只有chatgpt能记得它有哪些。</p><p>​回归：MSE，均方误差</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111104515.webp" alt="image-20230811110447328"></p><p>​TensorFlow所做的，实际上也是神经网络训练的标准，是使用一种称为反向传播（BP）的算法来计算这些偏导数项，调用<code>fit</code>函数。</p><h2 id="2、激活函数的替代方案"><a href="#2、激活函数的替代方案" class="headerlink" title="2、激活函数的替代方案"></a>2、激活函数的替代方案</h2><ul><li><h3 id="Sigmoid激活函数的替代方案"><a href="#Sigmoid激活函数的替代方案" class="headerlink" title="Sigmoid激活函数的替代方案"></a>Sigmoid激活函数的替代方案<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111517810.webp" alt="image-20230811151730550"></h3></li></ul><blockquote><p>线性整流函数（Rectified Linear Unit，简称<strong>ReLU</strong>）是一种常用的人工神经网络激活函数，通常用于神经网络的隐藏层。ReLU 函数定义如下：</p><p>​<strong><em>f</em>(<em>x</em>)&#x3D;max(0,<em>x</em>)</strong></p><p>其中，<em><strong>x</strong></em> 是输入值，而 <em><strong>f(x)</strong></em> 则是输出值。换句话说，当 <em><strong>x</strong></em> 大于等于零时，ReLU 函数返回 <em><strong>x</strong></em>；当 <em><strong>x</strong></em>小于零时，返回零。这就是为什么它被称为线性整流函数，因为它将负数的部分截断为零，而保留正数部分不变。</p></blockquote><ul><li><h3 id="如何选择激活函数？"><a href="#如何选择激活函数？" class="headerlink" title="如何选择激活函数？"></a>如何选择激活函数？</h3></li></ul><p>​<code>针对目标 y 选择:</code></p><ol><li>对于二分类问题，<em><strong>y</strong></em>   分为两种，可以选择   <em><strong>sigmiod</strong></em>   函数。</li><li>对于预测股票价格如何变化的问题，<em><strong>y</strong></em>   可正可负，可以选择线性激活函数。</li><li>对于预测房屋价格，<em><strong>y</strong></em>   为非负，选择   <em><strong>ReLU</strong></em>   函数。</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111533834.webp" alt="image-20230811153329560"></p><p>​现如今，隐藏层几乎都是用  <em><strong>ReLU</strong></em>  。</p><blockquote><p>  <em><strong>ReLU</strong></em> 在大多数情况下都是更优的选择，因为它具有更好的计算效率、训练速度和收敛性能，而且通常不容易遇到梯度消失问题。然而，  <em><strong>ReLU</strong></em> 也有一些问题，比如神经元死亡问题，可能需要通过使用其他激活函数的变体来解决。在特定情况下，如循环神经网络中，   <em><strong>sigmiod</strong></em>   仍然可能有一定的用途。最终的选择应该考虑到具体问题和网络结构的需求。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111554152.webp" alt="image-20230811155359937"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308111603587.webp" alt="image-20230811160307343"></p><blockquote><p>将隐藏层使用 ReLU 激活函数，而输出层使用 Sigmoid 激活函数是一种常见的组合，适用于许多机器学习和深度学习任务，尤其是二元分类问题。这种组合在实际应用中很常见，有一些优点和适用场景：</p><ol><li><strong>隐藏层使用 ReLU</strong>：<ul><li>ReLU 激活函数在正值部分保持线性关系，对梯度消失问题有一定的缓解作用，因此通常用于隐藏层。</li><li>ReLU 是计算高效的激活函数，对于深层神经网络具有加速训练的效果。</li></ul></li><li><strong>输出层使用 Sigmoid</strong>：<ul><li>Sigmoid 激活函数将输出映射到 (0, 1) 的范围内，非常适用于表示概率值，尤其是二元分类问题的概率预测。</li><li>对于输出层使用 Sigmoid，可以将网络的输出解释为样本属于某一类的概率。</li></ul></li></ol></blockquote><ul><li><h3 id="为什么模型需要激活函数？"><a href="#为什么模型需要激活函数？" class="headerlink" title="为什么模型需要激活函数？"></a>为什么模型需要激活函数？</h3></li></ul><p>​需求检测示例：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308112111767.webp" alt="image-20230811211151453"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308112112628.webp" alt="image-20230811211215382"></p><p>​线性函数的线性函数，还是线性函数</p><blockquote><p>激活函数在神经网络中扮演着重要的角色，它们引入非线性变换，使得网络能够捕捉复杂的模式和关系，从而提高网络的表达能力和性能。以下是一些关键原因，解释了为什么要使用激活函数：</p><ol><li><strong>引入非线性性</strong>： 神经网络的堆叠层本质上是一系列线性变换，如果没有激活函数，多个线性层的组合仍然只能表示线性关系。激活函数引入非线性性，使得网络可以对输入数据进行非线性变换，从而更好地捕捉数据中的复杂模式和特征。</li><li><strong>增强网络的表达能力</strong>： 激活函数允许神经网络模型更好地拟合各种不同的函数，包括非线性函数。这使得神经网络能够处理各种复杂的任务，如图像识别、自然语言处理等，从而提高网络的表达能力。</li><li><strong>解决线性组合的问题</strong>： 激活函数将输入的线性组合转化为非线性输出。这在处理数据时非常关键，因为现实世界中的数据通常是非线性的。激活函数能够让网络学习更加复杂的数据变换和模式。</li><li><strong>实现决策边界</strong>： 在分类问题中，激活函数可以帮助网络学习到适当的决策边界，从而正确地分类不同类别的样本。这对于图像分类、情感分析等任务非常重要。</li><li><strong>引入稀疏激活性</strong>： 一些激活函数（如 ReLU）可以在激活值为零时产生稀疏激活性，即部分神经元保持非激活状态。这有助于网络进行特征选择，减少冗余信息，提高模型的泛化能力。</li><li><strong>缓解梯度消失问题</strong>： 激活函数的非线性性质有助于缓解梯度消失问题。某些激活函数（如 ReLU）在正值部分具有常数梯度，有助于梯度在反向传播时保持一定的大小，避免梯度逐渐减小。</li></ol><p>综上所述，激活函数是神经网络中不可或缺的部分，它们赋予网络非线性能力，使得网络能够处理复杂的问题和数据。通过适当选择和组合激活函数，可以提高神经网络的性能、泛化能力和训练效果。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29145/"/>
      <url>/posts/29145/</url>
      
        <content type="html"><![CDATA[<h1 id="前向传播、向量化神经网络"><a href="#前向传播、向量化神经网络" class="headerlink" title="前向传播、向量化神经网络"></a><p align="center">前向传播、向量化神经网络</p></h1><h2 id="1、单个网络层上的前向传播"><a href="#1、单个网络层上的前向传播" class="headerlink" title="1、单个网络层上的前向传播"></a>1、单个网络层上的前向传播</h2><p>​阅读代码，理解内容，继续使用咖啡烘培模型</p><p>​<strong>前向传播参数随机初始化，后面进行反向传播调参修正。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308061730483.webp" alt="image-20230806173016220"></p><blockquote><p>​w2_1 &#x3D; np.array ( [-7，8，<strong>1</strong>] )        #应该为3个元素的 1D</p><p>​               –</p><p>​a2_1 &#x3D; sigmod ( z2_1 )   # z2_1</p></blockquote><p>​这就是如何使用<code>python</code>和<code>numpy</code>实现前向传播。</p><h2 id="2、前向传播的一般实现"><a href="#2、前向传播的一般实现" class="headerlink" title="2、前向传播的一般实现"></a>2、前向传播的一般实现</h2><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308062044889.webp" alt="image-20230806204441625"></p><blockquote><ol><li><p><code>units = W.shape[1]</code>: 获取权重矩阵 <code>W</code> 的输出维度（神经元数量），保存在变量 <code>units</code> 中。</p><p>通过将 <code>w.shape[1]</code> 赋值给 <code>units</code>，你可以根据权重矩阵 <code>w</code> 的形状自动设置神经网络层的输出维度，使其与权重矩阵的列数相匹配。这样在构建神经网络时就不需要手动指定输出维度，更加方便和灵活。</p></li><li><p><code>a_out = np.zeros(units)</code>: 创建一个一维数组 <code>a_out</code>，用于保存输出结果，初始值全部设为 0。</p></li><li><p><code>for j in range(units):</code>: 遍历每个输出神经元的索引 <code>j</code>。</p></li><li><p><code>w = W[:, j]</code>: 从权重矩阵 <code>W</code> 中获取第 <code>j</code> 列，即与第 <code>j</code> 个神经元连接的权重向量。</p></li><li><p><code>z = np.dot(w, a_in) + b[j]</code>: 计算加权输入 <code>z</code>，使用权重向量 <code>w</code> 与输入向量 <code>a_in</code> 的点积，并加上对应的偏置 <code>b[j]</code>。</p></li><li><p><code>a_out[j] = g(z)</code>: 将加权输入 <code>z</code> 经过激活函数 <code>g</code> 进行非线性变换，得到第 <code>j</code> 个输出神经元的输出值，并保存在 <code>a_out[j]</code> 中。</p></li><li><p>循环结束后，<code>a_out</code> 中存储了整个层的输出结果，即一个由 <code>units</code> 个元素组成的一维数组，表示该层所有神经元的输出。</p></li></ol><p></p></blockquote><p>​后面的代码时将几个隐藏层串在一起，以便在神经网络中实现前向传播。</p><p>​<strong>这里使用   W  ,因为根据线性代数，矩阵使用大写字母，小写字母代表向量和标量。</strong></p><p>​       这就是底层工作原理， <code>在实验室中练习。</code></p><hr><p>​神经网络与AI、AGI、通用人工智能之间是什么关系？</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308062119965.webp" alt="image-20230806211930637"></p><p>​实验表明人类的大脑具有惊人的适应性，具有惊人的可塑性，它意味着能够适应令人眼花缭乱的传感器输入范围，问题是，如果同一块脑组织可以学会看、摸，甚至其他东西，我们可以复制这个算法并在计算机上实现吗？</p><p>​在短期内，我认为即使不追求AGI，机器学习和神经网络也是非常强大的工具，即使不尝试去构建人类水平的智能，我认为你会发现神经网络非常强大，以及您可能构建的应用程序的有用工具集。</p><h2 id="3、神经网络的高效"><a href="#3、神经网络的高效" class="headerlink" title="3、神经网络的高效"></a>3、神经网络的高效</h2><p>​神经网络可以<code>矢量化</code>，下面第一段代码缺少对units的定义。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308101720807.webp" alt="image-20230810172006544"></p><blockquote><p>在NumPy库中，<code>dot</code>和<code>matmul</code>都用于矩阵乘法操作，但在某些情况下它们有一些细微的差异。下面我会详细解释它们的不同之处。</p><ol><li><strong><code>dot</code>函数：</strong> <code>numpy.dot(a, b)</code>函数用于计算两个数组的点积（内积）。对于二维数组（矩阵）来说，它执行矩阵乘法操作。对于一维数组，它计算两个数组的内积。对于高维数组，它会将最后两个维度视为矩阵，然后进行矩阵乘法。</li><li><strong><code>matmul</code>函数：</strong> <code>numpy.matmul(a, b)</code>函数也用于计算两个数组的矩阵乘法。与<code>dot</code>不同的是，<code>matmul</code>在处理高维数组时会更加严格。它只能用于两个至少是2维的数组，并且在进行矩阵乘法时会忽略其他维度。</li></ol><p>总结：</p><ul><li>当处理一维或多维数组时，<code>dot</code>和<code>matmul</code>在执行矩阵乘法时的行为可能会略有不同。</li><li>对于二维数组，<code>dot</code>和<code>matmul</code>执行相同的矩阵乘法操作。</li><li>对于高维数组，<code>dot</code>会在最后两个维度上执行矩阵乘法，而<code>matmul</code>只会在两个至少为2维的数组上执行矩阵乘法。</li></ul><p>在绝大多数情况下，对于二维数组，<code>dot</code>和<code>matmul</code>的行为是相同的，但在处理高维数组时，需要注意它们之间的差异。</p></blockquote><h2 id="4、矩阵乘法的学习与代码实现"><a href="#4、矩阵乘法的学习与代码实现" class="headerlink" title="4、矩阵乘法的学习与代码实现"></a>4、矩阵乘法的学习与代码实现</h2><p>​点乘，对应相乘相加   &#x3D;    矩阵转置相乘，实现点积。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308101720192.webp" alt="image-20230810172043022"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308101721579.webp" alt="image-20230810172102414"></p><p>​<strong>向量矩阵相乘推广矩阵矩阵相乘</strong></p><hr><p>​下面实现矩阵乘代码：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308101721410.webp" alt="image-20230810172130165"></p><p>​注图中少 “[ ]”。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308101721089.webp" alt="image-20230810172146907"></p><p>​<strong>A-T我们将其称为A-in</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308101723420.webp" alt="image-20230810172325110"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29144/"/>
      <url>/posts/29144/</url>
      
        <content type="html"><![CDATA[<h1 id="搭建神经网络"><a href="#搭建神经网络" class="headerlink" title="搭建神经网络"></a><p align="center">搭建神经网络</p></h1><h2 id="1、如何用代码实现推理"><a href="#1、如何用代码实现推理" class="headerlink" title="1、如何用代码实现推理"></a>1、如何用代码实现推理</h2><p>​神经网络的一个显着特点是可以将相同的算法应用于许多不同的应用程序。下面使用一个例子来说明推理。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051709246.webp" alt="image-20230805170902992"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051717349.webp" alt="image-20230805171740094"></p><p>​这行代码是使用 Keras API 创建一个神经网络的一层。在这个例子中，它创建了一个具有 3 个神经元的全连接层（也称为稠密层），并且使用 sigmoid 激活函数。</p><ul><li><p><code>Dense</code>：这是 Keras 中创建全连接层的类。</p></li><li><p><code>units=3</code>：指定这一层有 3 个神经元。这意味着该层的输出将是一个包含 3 个值的向量。</p></li><li><p><code>activation=&#39;sigmoid&#39;</code>：这是设置该层的激活函数，这里使用的是 sigmoid 函数。Sigmoid 函数将输入的值映射到 0 到 1 之间的范围，通常用于处理二分类问题。</p><p>以下是一个简单的示例，演示如何使用这个层来构建一个简单的 Keras 模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 Sequential 模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加第一层</span></span><br><span class="line">model.add(Dense(units=<span class="number">3</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, input_shape=(input_dim,)))  <span class="comment"># 替换 input_dim 为你的输入数据维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加其他层（这里省略）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型并进行训练</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">10</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>以上代码只是一个简单示例，实际构建的模型可能更复杂，并且在编译和训练模型时需要使用适当的优化器、损失函数和评估指标。</p><p>这就是使用TensorFlow在神经网络中进行推理的方式。</p></li></ul><hr><p>​例子：手写数字分类问题</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051728039.webp" alt="image-20230805172849814"></p><h2 id="2、Tensorflow-中的数据形式"><a href="#2、Tensorflow-中的数据形式" class="headerlink" title="2、Tensorflow 中的数据形式"></a>2、Tensorflow 中的数据形式</h2><p>​在  <strong>Numpy</strong>  和  <strong>TensorFlow</strong>  中表示数据，  <strong>Numpy</strong>  和<strong>TensorFlow</strong>  中的数据表示方式存在一些不一致。</p><p>​先看在  <strong>TensorFlow</strong>  中表示数据：利用上次的  <strong>coffee</strong>  数据，<strong>为什么是双方括号？</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051749685.webp" alt="image-20230805174906489"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051932344.webp" alt="image-20230805193231104"></p><p>​矩阵维数为行数<em>列数，每行数据一个</em>*[ ]<strong>，整个矩阵表示完后加</strong>（[ ]）**。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051933971.webp" alt="image-20230805193326774"></p><p>​搞清楚生成的矩阵为什么样的形式。行向量，列向量，单个方括号则表示一维向量，没有行或者列的一维数组。一维向量可以用于表示一组单一维度的数据，例如：</p><ul><li><p>温度的时间序列数据：[25.5, 26.3, 24.8, 26.0, …]</p></li><li><p>学生的成绩：[90, 85, 78, 92, …]</p></li><li><p>产品的价格：[10.5, 15.2, 12.0, 9.8, …]</p><blockquote><p>在编程语言中，一维数组通常是一种简单的线性数据结构，可以通过列表（list）来表示。二维数组通常是嵌套的一维数组，可以通过列表的列表（list of lists）来表示。例如，Python中的列表可以用于表示一维数组和二维数组。</p><ol><li>一维数组（一维向量）：<ul><li>定义：一维数组是一组按顺序排列的数据元素，每个元素都有唯一的索引。</li><li>表示：一维数组是一个线性结构，数据按照单一维度组织，并且可以用一个索引来访问其中的每个元素。</li><li>示例：[1, 2, 3, 4, 5] 或者 [‘apple’, ‘banana’, ‘orange’]</li></ul></li><li>二维数组（二维矩阵）：<ul><li>定义：二维数组是一组按行和列排列的数据元素，每个元素由两个索引（行索引和列索引）来确定。</li><li>表示：二维数组是一个矩形结构，数据按照行和列的方式组织，每个元素需要两个索引来定位。</li><li>示例：[[1, 2, 3], [4, 5, 6], [7, 8, 9]] 或者 [[‘a’, ‘b’, ‘c’], [‘d’, ‘e’, ‘f’]]</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维数组（列表）</span></span><br><span class="line">one_dimensional_array = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组（列表的列表）</span></span><br><span class="line">two_dimensional_array = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在数据分析、机器学习和深度学习中，二维数组常用于表示数据集、图像、矩阵等数据结构。一维数组通常用于表示单一维度的特征向量或标签。</p></blockquote></li></ul><p>​</p><p>​<strong>处理线性回归和逻辑回归的课程中，使用一维向量来表示输入特征  <em>x。</em></strong></p><p>​<strong>TensorFlow</strong>  中表示数据的惯例是使用矩阵来表示数据， 事实证明，<strong>TensorFlow</strong>  旨在处理非常大的数据集，并且通过在矩阵而不是一维数组中表示数据，它让<strong>TensorFlow</strong>  在内部的计算效率更高一些。</p><p>​所以：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051954453.webp" alt="image-20230805195427271"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308051959072.webp" alt="image-20230805195912840"></p><p>​</p><blockquote><ol><li><code>x = np.array([[200.0, 17.0]])</code>：这行代码创建了一个包含两个元素的二维 NumPy 数组，并将其赋值给变量 <code>x</code>。这个数组实际上是一个包含两个浮点数的一维向量（或一维数组）。注意，这里的代码应该是没有问题的，我之前提到的问题已经修复了。</li><li><code>layer_1 = Dense(units=3, activation=&#39;sigmoid&#39;)</code>：这行代码定义了一个全连接层（Dense Layer），该层有 3 个神经元，并使用 Sigmoid 激活函数。这里的 <code>units</code> 参数表示该层的输出维度，即输出具有 3 个值的向量。</li><li><code>a1 = layer_1(x)</code>：这行代码将输入向量 <code>x</code> 通过 <code>layer_1</code> 这个定义好的神经网络层进行前向传播。这就是将输入数据通过神经网络进行处理得到输出的过程。<code>a1</code> 将保存经过 <code>layer_1</code> 处理后的输出结果。</li><li><code>tf.Tensor([[0.2 0.7 0.3]], shape=(1, 3), dtype=float32)</code>：这是代码的输出结果。这表示经过前向传播后，<code>layer_1</code> 处理输入向量 <code>x</code> 后得到的输出向量 <code>a1</code>。这里的输出向量 <code>a1</code> 是一个包含三个浮点数的一维向量，具体数值是 <code>[0.2, 0.7, 0.3]</code>。</li></ol></blockquote><p>​</p><p><strong>Tensor</strong>：张量，这里的张量是TensorFlow团队为了有效地存储和执行矩阵计算而创建的一种数据类型。</p><blockquote><p><code>tf.Tensor</code> 是 TensorFlow 中表示张量（tensor）的数据类型。张量是多维数组，可以看作是标量（0 维）、向量（1 维）、矩阵（2 维）等的扩展。</p><p>在 TensorFlow 中，所有的数据都以张量的形式进行处理。张量可以包含任意维度的数据，并且可以存储数值、字符串等类型的数据。它是 TensorFlow 的核心数据结构之一，用于表示计算图中的数据流动。</p><p><code>tf.Tensor</code> 是不可变的，意味着创建后无法直接修改其值。在 TensorFlow 中，所有的操作（例如张量运算）都会返回新的 <code>tf.Tensor</code> 对象，而不会修改原始张量。</p></blockquote><p>​事实上，如果你想获取张量a1并将其转换回NumPy数组，你可以使用函数  <strong>a1.numpy</strong>  来实现。它将获取相同的数据并以NumPy数组的形式返回，而不是以TensorFlow数组或TensorFlow矩阵的形式返回。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052013348.webp" alt="image-20230805201312150"></p><h2 id="3、搭建一个神经网络"><a href="#3、搭建一个神经网络" class="headerlink" title="3、搭建一个神经网络"></a>3、搭建一个神经网络</h2><ol><li>顺序框架张量流（Sequential Model in TensorFlow）是 TensorFlow 中一种简单且常用的神经网络模型构建方式。它是 TensorFlow 的高级 API，旨在帮助用户快速构建神经网络模型，特别适用于那些层按顺序堆叠的简单模型。</li><li>顺序框架张量流的主要特点是它按照顺序一层一层地添加神经网络层，形成一个线性的模型结构。每一层都连接到上一层，并且数据从第一层流动到最后一层。这种顺序的层叠结构使得模型的构建和理解更加直观和简单。</li></ol><p>​以下是一个简单的例子，展示了如何使用顺序框架张量流构建一个简单的全连接神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建顺序框架</span></span><br><span class="line">model = tf.keras.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加第一层（输入层）</span></span><br><span class="line">model.add(Dense(units=<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(input_dim,)))  <span class="comment"># 替换 input_dim 为输入数据的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加第二层</span></span><br><span class="line">model.add(Dense(units=<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加输出层</span></span><br><span class="line">model.add(Dense(units=num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>))  <span class="comment"># 替换 num_classes 为分类类别的数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​上述代码中，我们创建了一个顺序框架 <code>model</code>，并按照顺序添加了三个全连接层（输入层、隐藏层和输出层）。最后，<strong>我们通过 <code>compile</code> 方法对模型进行编译，指定优化器、损失函数和评估指标，以便在训练时进行模型优化和评估。</strong></p><p>​通过顺序框架张量流，我们可以快速构建和训练神经网络模型，适用于很多简单的深度学习任务。对于更复杂的模型结构或需要非顺序连接的情况，可以使用函数式 API 或子类化 API 来构建定制化的模型。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052049104.webp" alt="image-20230805204953874"></p><ul><li>如果你想训练这个神经网络，你需要做的就是调用你需要用一些参数调用模型点编译的函数   <em><strong>compile</strong></em>   。</li><li><strong>fit</strong>  告诉张量流采用这个神经网络在数据  <strong>x</strong>  上对  <strong>y</strong>  进行训练。</li><li><strong>predict</strong>  进行预测。</li></ul><p>​模型预测使用顺序函数编译的神经网络进行前向传播并为您进行推理。</p><hr><p>​为数字分类示例重做此操作：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052102013.webp" alt="image-20230805210237748"></p><p>​<strong>通常可以将隐藏层直接放入顺序函数中</strong>：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052103009.webp" alt="image-20230805210336777"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308052106426.webp" alt="image-20230805210654045"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29142/"/>
      <url>/posts/29142/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合、正则化"><a href="#过拟合、正则化" class="headerlink" title="过拟合、正则化"></a><p align="center">过拟合、正则化</p></h1><h2 id="1、过拟合问题"><a href="#1、过拟合问题" class="headerlink" title="1、过拟合问题"></a>1、过拟合问题</h2><ul><li>模型对训练数据 <strong>欠拟合</strong> (  <em><strong>underfit</strong></em>  ) or 算法具有 <strong>高偏差</strong>（ <em><strong>high bias</strong></em> ）</li><li>希望  <em><strong>generalization</strong></em> :  <strong>泛化</strong></li><li><strong>过拟合</strong>（ <em><strong>overfitting</strong></em> ） or  <strong>高方差</strong>（ <em><strong>high variance</strong></em> ）</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291641912.webp" alt="image-20230729164112592"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291650553.webp" alt="image-20230729165008193"></p><h2 id="2、解决过拟合问题"><a href="#2、解决过拟合问题" class="headerlink" title="2、解决过拟合问题"></a>2、解决过拟合问题</h2><ol><li>收集更多的训练集数据。</li><li>不要使用太多特征，使用特征的子集即选择一部分最合适的特征。</li><li><em><strong>Regularization</strong></em>  : 正则化</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291657447.webp" alt="image-20230729165758247"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291658119.webp" alt="image-20230729165852938"></p><h2 id="3、正则化"><a href="#3、正则化" class="headerlink" title="3、正则化"></a>3、正则化</h2><ul><li><p>正则化是一种更<strong>温和地减少某些特征影响</strong>的方法，而不像彻底消除它那样严厉。</p></li><li><p>正则化的作用是<strong>鼓励学习算法缩小参数值</strong>，而不是把参数变为0。</p></li></ul><p>​基于正则化为学习算法设计的代价函数，通过调整代价函数来调整参数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291711733.webp" alt="image-20230729171147458"></p><p>​<strong>正则化的典型实现方式是惩罚所有的特征，惩罚所有的 <em>w</em> 参数</strong>。最后一项可以忽略。</p><ul><li><em><strong>λ</strong></em> 设置为0，则根本没有使用正则化项。</li><li><em><strong>λ</strong></em> 设置非常大，参数非常接近0，例如对于线性回归模型则   <em><strong>f&#x3D;b</strong></em>   ，拟合水平直线，欠拟合。</li></ul><h2 id="4、用于线性回归的正则方法"><a href="#4、用于线性回归的正则方法" class="headerlink" title="4、用于线性回归的正则方法"></a>4、用于线性回归的正则方法</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292206887.webp" alt="image-20230729220649628"></p><blockquote><p><strong>注意：正则化项求偏导后无求和符号。</strong></p><p>求导是对某一项的参数求导，此时其他参数为常数。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292212627.webp" alt="image-20230729221220333"></p><h3 id="高数"><a href="#高数" class="headerlink" title="高数"></a><p align="center">高数</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292217945.webp" alt="image-20230729221710700"></p><h2 id="5、用于逻辑回归的正则化方法"><a href="#5、用于逻辑回归的正则化方法" class="headerlink" title="5、用于逻辑回归的正则化方法"></a>5、用于逻辑回归的正则化方法</h2><p><strong><p align="center">正则化逻辑与线性回归相似</p></strong></p><p>​修改代价函数实现正则化：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292222803.webp" alt="image-20230729222206533"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292224550.webp" alt="image-20230729222403346"></p><p>​<strong>完全相同的方程，除了   <em>f</em>   的定义不再是线性函数，而是应用于   <em>z</em>   的逻辑函数。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292229255.webp" alt="image-20230729222925905"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29141/"/>
      <url>/posts/29141/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a><p align="center">逻辑回归</p></h1><p>​分类：输出变量为少数可能值中的一个，而不是无限范围内的任何数字。</p><h3 id="1-2逻辑回归（用于二元分类）"><a href="#1-2逻辑回归（用于二元分类）" class="headerlink" title="1.2逻辑回归（用于二元分类）"></a>1.2逻辑回归（用于二元分类）</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061024791.webp" alt="image-20230706102408627"></p><p>​Sigmoid函数是一个在生物学中常见的<a href="https://baike.baidu.com/item/S%E5%9E%8B%E5%87%BD%E6%95%B0/19178062?fromModule=lemma_inlink">S型函数</a>，也称为<a href="https://baike.baidu.com/item/S%E5%9E%8B%E7%94%9F%E9%95%BF%E6%9B%B2%E7%BA%BF/5581189?fromModule=lemma_inlink">S型生长曲线</a>。 在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的<a href="https://baike.baidu.com/item/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2520792?fromModule=lemma_inlink">激活函数</a>，将变量映射到0,1之间。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061059720.webp" alt="image-20230706105911494"></p><p>​<strong>多变量整合为单变量，再用sigmoid分类</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061104571.webp" alt="image-20230706110452346"></p><h3 id="1-3逻辑回归、决策边界"><a href="#1-3逻辑回归、决策边界" class="headerlink" title="1.3逻辑回归、决策边界"></a>1.3逻辑回归、决策边界</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081441602.webp" alt="image-20230708144100042"></p><h3 id="2-1逻辑回归中的代价函数"><a href="#2-1逻辑回归中的代价函数" class="headerlink" title="2.1逻辑回归中的代价函数"></a>2.1逻辑回归中的代价函数</h3><p>​<strong>代价函数提供了一种衡量特定参数与训练数据拟合程度的方法</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081448958.webp" alt="image-20230708144831744"></p><hr><p>​<strong>单个训练示例的损失函数</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101450133.webp" alt="image-20230710145008957"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081510170.webp" alt="image-20230708151030003"></p><p>​<em>f</em>  的值总是在0-1之间，越接近于1，损失越小。<strong>上图损失函数漏掉了一个负号</strong></p><hr><p>​<strong>代价函数是整个训练集的函数，因此是单个训练示例的损失函数之和的平均值</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081519147.webp" alt="image-20230708151933785"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes cost</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)) : target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)) : model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)       : model parameter</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      cost (scalar): cost</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">m = X.shape[<span class="number">0</span>]</span><br><span class="line">cost = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    z_i = np.dot(X[i],w) + b</span><br><span class="line">    f_wb_i = sigmoid(z_i)</span><br><span class="line">    cost +=  -y[i]*np.log(f_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-f_wb_i)</span><br><span class="line">         </span><br><span class="line">cost = cost / m</span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="2-2简化逻辑回归代价函数"><a href="#2-2简化逻辑回归代价函数" class="headerlink" title="2.2简化逻辑回归代价函数"></a>2.2简化逻辑回归代价函数</h3><p>​目的：使用梯度下降来拟合逻辑回归模型的参数时，实现简单。</p><p>​二元<strong>交叉熵损失函数</strong>，多分类只需要多乘几个多项式即可</p><p> <img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101459266.webp" alt="image-20230710145927025"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101504651.webp" alt="image-20230710150413331"></p><p>​这个特定的成本函数是使用最大似然估计的统计原理推导出来的，这是统计学中关于如何有效的找到不同模型的参数的想法。<strong>它有一个很好的特性，它是凸的。</strong></p><h3 id="3-1实现梯度下降"><a href="#3-1实现梯度下降" class="headerlink" title="3.1实现梯度下降"></a>3.1实现梯度下降</h3><p>​为了拟合逻辑回归模型的参数，将尝试找到使 <em>w</em> 和 <em>b</em> 的成本  <em>J</em>  最小的参数值，使用梯度下降来实现。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121410281.webp" alt="image-20230712141007965"></p><p>​<strong>与线性回归模型类似，注意同时更新。</strong></p><p>推导过程：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121500696.webp" alt="image-20230712150021478"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121519309.webp" alt="image-20230712151912073"></p><p>​<strong>尽管线性回归和逻辑回归所写的算法看起来相同，但实际上它们是两种截然不同的算法，因为 <em>f</em> 的定义不同。</strong></p><p>​<strong>ps：上述逻辑回归的 <em>f</em> 的负号位置错了</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121526385.webp" alt="image-20230712152606243"></p><hr><p>​<strong>梯度函数实现代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the gradient for linear regression </span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)): target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)): model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)      : model parameter</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. </span></span><br><span class="line"><span class="string">      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))                           <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i],w) + b)          <span class="comment">#(n,)(n,)=scalar</span></span><br><span class="line">        err_i  = f_wb_i  - y[i]                       <span class="comment">#scalar</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      <span class="comment">#scalar</span></span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw/m                                   <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = dj_db/m                                   <span class="comment">#scalar</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw  </span><br></pre></td></tr></table></figure><p>​<strong>检查梯度函数的实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_tmp = np.array([[<span class="number">0.5</span>, <span class="number">1.5</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1.5</span>, <span class="number">0.5</span>], [<span class="number">3</span>, <span class="number">0.5</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2.5</span>]])</span><br><span class="line">y_tmp = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">w_tmp = np.array([<span class="number">2.</span>,<span class="number">3.</span>])</span><br><span class="line">b_tmp = <span class="number">1.</span></span><br><span class="line">dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dj_db: <span class="subst">&#123;dj_db_tmp&#125;</span>&quot;</span> )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dj_dw: <span class="subst">&#123;dj_dw_tmp.tolist()&#125;</span>&quot;</span> )</span><br></pre></td></tr></table></figure><p>​<strong>结果为：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dj_db: <span class="number">0.49861806546328574</span></span><br><span class="line">dj_dw: [<span class="number">0.498333393278696</span>, <span class="number">0.49883942983996693</span>]</span><br></pre></td></tr></table></figure><hr><p>​<strong>梯度下降代码:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w_in, b_in, alpha, num_iters</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs batch gradient descent</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)   : Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,))   : target values</span></span><br><span class="line"><span class="string">      w_in (ndarray (n,)): Initial values of model parameters  </span></span><br><span class="line"><span class="string">      b_in (scalar)      : Initial values of model parameter</span></span><br><span class="line"><span class="string">      alpha (float)      : Learning rate</span></span><br><span class="line"><span class="string">      num_iters (scalar) : number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      w (ndarray (n,))   : Updated values of parameters</span></span><br><span class="line"><span class="string">      b (scalar)         : Updated value of parameter </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span></span><br><span class="line">    J_history = []</span><br><span class="line">    w = copy.deepcopy(w_in)  <span class="comment">#avoid modifying global w within function</span></span><br><span class="line">    b = b_in</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># Calculate the gradient and update the parameters</span></span><br><span class="line">        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update Parameters using w, b, alpha and gradient</span></span><br><span class="line">        w = w - alpha * dj_dw               </span><br><span class="line">        b = b - alpha * dj_db               </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Save cost J at each iteration</span></span><br><span class="line">        <span class="keyword">if</span> i&lt;<span class="number">100000</span>:      <span class="comment"># prevent resource exhaustion </span></span><br><span class="line">            J_history.append( compute_cost_logistic(X, y, w, b) )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span></span><br><span class="line">        <span class="keyword">if</span> i% math.ceil(num_iters / <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;i:4d&#125;</span>: Cost <span class="subst">&#123;J_history[-<span class="number">1</span>]&#125;</span>   &quot;</span>)</span><br><span class="line">    </span><br><span class="line">     <span class="keyword">return</span> w, b, J_history         <span class="comment">#return final w,b and J history for graphing</span></span><br></pre></td></tr></table></figure><p>​<strong>在数据集上运行：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_tmp  = np.zeros_like(X_train[<span class="number">0</span>])</span><br><span class="line">b_tmp  = <span class="number">0.</span></span><br><span class="line">alph = <span class="number">0.1</span></span><br><span class="line">iters = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nupdated parameters: w:<span class="subst">&#123;w_out&#125;</span>, b:<span class="subst">&#123;b_out&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>​<strong>结果：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Iteration    <span class="number">0</span>: Cost <span class="number">0.684610468560574</span>   </span><br><span class="line">Iteration <span class="number">1000</span>: Cost <span class="number">0.1590977666870457</span>   </span><br><span class="line">Iteration <span class="number">2000</span>: Cost <span class="number">0.08460064176930078</span>   </span><br><span class="line">Iteration <span class="number">3000</span>: Cost <span class="number">0.05705327279402531</span>   </span><br><span class="line">Iteration <span class="number">4000</span>: Cost <span class="number">0.04290759421682</span>   </span><br><span class="line">Iteration <span class="number">5000</span>: Cost <span class="number">0.03433847729884557</span>   </span><br><span class="line">Iteration <span class="number">6000</span>: Cost <span class="number">0.02860379802212006</span>   </span><br><span class="line">Iteration <span class="number">7000</span>: Cost <span class="number">0.02450156960879306</span>   </span><br><span class="line">Iteration <span class="number">8000</span>: Cost <span class="number">0.02142370332569295</span>   </span><br><span class="line">Iteration <span class="number">9000</span>: Cost <span class="number">0.019030137124109114</span>   </span><br><span class="line"></span><br><span class="line">updated parameters: w:[<span class="number">5.28</span> <span class="number">5.08</span>], b:-<span class="number">14.222409982019837</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29140/"/>
      <url>/posts/29140/</url>
      
        <content type="html"><![CDATA[<h1 id="多元线性回归，矢量化"><a href="#多元线性回归，矢量化" class="headerlink" title="多元线性回归，矢量化"></a><p align="center">多元线性回归，矢量化</p></h1><h2 id="1-多维特征"><a href="#1-多维特征" class="headerlink" title="1.多维特征"></a>1.多维特征</h2><p><strong>多元线性回归模型</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100942878.webp" alt="image-20230610094203639"></p><hr><hr><h2 id="2-矢量化"><a href="#2-矢量化" class="headerlink" title="2.矢量化"></a>2.矢量化</h2><p><strong>缩短代码，提高运行效率</strong></p><p><strong>NumPy</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100951696.webp" alt="image-20230610095105404"></p><p><strong>用空间换时间，用连续的结构可以省去查找数据的时间</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002307.webp" alt="image-20230610100241038"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002970.webp" alt="image-20230610100256599"></p><hr><hr><h2 id="3-用于多元线性回归的梯度下降算法"><a href="#3-用于多元线性回归的梯度下降算法" class="headerlink" title="3.用于多元线性回归的梯度下降算法"></a>3.用于多元线性回归的梯度下降算法</h2><p><strong>带有矢量化的多元线性回归实现梯度下降</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101013171.webp" alt="image-20230610101334864"></p><p><strong>核心</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101018363.webp" alt="image-20230610101851161"></p><hr><hr><h2 id="4-特征缩放-数据预处理"><a href="#4-特征缩放-数据预处理" class="headerlink" title="4.特征缩放(数据预处理)"></a>4.特征缩放(数据预处理)</h2><p><strong>归一化</strong> ：拥有不同特征时，它们得取值范围非常不同时，可能会导致梯度下降运行缓慢，重新缩放不同得特征，使它们都具有可比较的取值范围，效果更显著。</p><ol><li><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110919219.webp" alt="image-20230611091955007"></li><li>均值归一化：特征值减平均值（样本平均值）再除以方差<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110925805.webp" alt="image-20230611092559538"></li><li>Z-score 归一化：需要计算每个特征的标准差。<strong>概率论</strong></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110938294.webp" alt="image-20230611093829964"></p><p><strong>按需缩放</strong>：在一个数量级上的特征可以不考虑缩放</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110941080.webp" alt="image-20230611094136864"></p><h2 id="5-判断梯度下降是否收敛"><a href="#5-判断梯度下降是否收敛" class="headerlink" title="5.判断梯度下降是否收敛"></a>5.判断梯度下降是否收敛</h2><p>如果有 <em>J</em> 在一次迭代后增加，意味着 Alpha选择不当，通常意味着Alpha太大，或者代码中可能存在错误。</p><ol><li><strong>迭代次数</strong>，创建学习曲线尝试找出。</li><li>自动收敛测试：假设 <em>epsilon</em> 是一个代表小数变量，例如 0.001 或 10^-3。如果成本 <em>J</em> 在一次迭代中减少的幅度小于这个数字 <em>epsilon</em> ，那么曲线可能趋于平坦。</li></ol><h2 id="6-选择合适的学习率"><a href="#6-选择合适的学习率" class="headerlink" title="6.选择合适的学习率"></a>6.选择合适的学习率</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120950291.webp" alt="image-20230612095038996"></p><hr><p>技巧：如果学习率足够小，成本函数应该在每次迭代中减少。通过将Alpha设置为一个非常小的数字，看看这是否会导致每次迭代时成本降低。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120956944.webp" alt="image-20230612095616640"></p><p><strong>省流：凭感觉去试，调参。</strong></p><h2 id="7-特征工程"><a href="#7-特征工程" class="headerlink" title="7.特征工程"></a>7.特征工程</h2><p><strong>特征衍生</strong>：通常通过转换或组合原始特征来使学习算法更容易做出准确的预测。</p><p><strong>多项式回归</strong></p><p><strong>Scikit-learn</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121042502.webp" alt="image-20230612104233179"></p><p>不仅可以拟合直线，还可以拟合曲线、非线性函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121050229.webp" alt="image-20230612105041878">、</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29139/"/>
      <url>/posts/29139/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归模型、梯度下降"><a href="#线性回归模型、梯度下降" class="headerlink" title="线性回归模型、梯度下降"></a><p align="center">线性回归模型、梯度下降</p></h1><h2 id="两种主要类型"><a href="#两种主要类型" class="headerlink" title="两种主要类型"></a><p align="left">两种主要类型</p></h2><h3 id="1-监督学习-Supervised-Learning"><a href="#1-监督学习-Supervised-Learning" class="headerlink" title="1.监督学习(Supervised Learning)"></a><p align="left">1.监督学习(Supervised Learning)</p></h3><p><img src="https://api2.mubu.com/v3/document_image/3413a5cb-b522-42e8-ae40-3a988a00321d-12774614.jpg" alt="image"></p><p>1、Regression-回归：从无限多种可能输出数字中预测数字。</p><p>2、Classfication-分类：预测类别，可能的输出都是一小组。</p><hr><h3 id="2-无监督学习-Unsupervised-Learning"><a href="#2-无监督学习-Unsupervised-Learning" class="headerlink" title="2.无监督学习(Unsupervised Learning)"></a><p align="left">2.无监督学习(Unsupervised Learning)</p></h3><p>没有试图监督算法而为了给每个输入提供一些正确的答案，相反，为了弄清数据中有什么模式或者结构。</p><p><img src="https://api2.mubu.com/v3/document_image/ab575484-7afb-4726-b350-0697e0a4cf12-12774614.jpg" alt="image"></p><p>1、Clustering-聚类：获取没有标签的数据并尝试将他们自动分组到集群中。例如相似推荐，就是把相似的内容归类后处理。</p><p>2、Anomaly detection-异常检测：用于检测异常事件。</p><p>3、Dimensionality reduction-降维：压缩大数据集。</p><hr><h3 id="3-线性回归模型-linear-regression"><a href="#3-线性回归模型-linear-regression" class="headerlink" title="3.线性回归模型(linear regression)"></a><p align="left">3.线性回归模型(linear regression)</p></h3><p>Training set-训练集、features-输入变量 <em>x</em>（特征或输入特征）、targets-目标变量  <em>y</em></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021634673.webp" alt="image-20230602163455564"></p><p>Univariate linear regression-单变量线性回归</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021647760.webp" alt="image-20230602164734648"></p><p><strong>1.Cost function-代价函数</strong></p><p>平方误差代价函数</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021702346.webp" alt="image-20230602170247191"></p><p>如何使用代价函数为模型找到最佳参数？</p><p>使  <em>J</em>  越小越好。  </p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021719541.webp" alt="image-20230602171953431"></p><p>先找最优的权重 <em>w</em> ，令 <em>b</em> 为 0。做代价函数图—二维</p><p><strong>多元函数求极值的问题。</strong></p><p>可视化代价函数：</p><p>回到原始问题，<em>b</em> 不为 0 时：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021801888.webp" alt="image-20230602180127719"></p><hr><h3 id="4-梯度下降（Gradient-descent）"><a href="#4-梯度下降（Gradient-descent）" class="headerlink" title="4.梯度下降（Gradient descent）"></a><p align="left">4.梯度下降（Gradient descent）</p></h3><p>高效算法：代码编写自动找到 <em>w</em> 和 <em>b</em>，实现最好的拟合。</p><p>梯度下降是一种尝试最小化任何函数的方法。而不局限于线性回归的成本函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031154023.webp" alt="image-20230603115414788"></p><p><strong>同时更新参数（代码顺序）</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031745415.webp" alt="image-20230603174503106"></p><p><strong>学习率</strong></p><p>太小：下降步幅小，速度慢。</p><p>太大：步幅大，但可能会使结果更糟，在最低点附近震荡，过充，甚至离最低点越来越远，发散。</p><hr><h3 id="5-线性回归算法"><a href="#5-线性回归算法" class="headerlink" title="5.线性回归算法"></a><p align="left">5.线性回归算法</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306071640865.webp" alt="image-20230607164007657"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081554888.webp" alt="image-20230608155457007"></p><p><strong>使用线性回归的平方误差成本函数时，成本函数永远不会有多个局部最小值。凸函数</strong></p><hr><h3 id="6-运行梯度下降"><a href="#6-运行梯度下降" class="headerlink" title="6.运行梯度下降"></a><p align="left">6.运行梯度下降</p></h3><p><strong>C1_W2_Lab03_Gradient_Descent_Soln</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081609512.webp" alt="image-20230608160911171"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
