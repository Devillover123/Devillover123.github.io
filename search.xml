<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深度学习</title>
      <link href="/posts/29143/"/>
      <url>/posts/29143/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a><p align="center">深度学习</p></h1><h2 id="1、神经元与大脑"><a href="#1、神经元与大脑" class="headerlink" title="1、神经元与大脑"></a>1、神经元与大脑</h2><p>​一些废话：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022202498.webp" alt="image-20230802220229291"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022203876.webp" alt="image-20230802220306686"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022203564.webp" alt="image-20230802220323363"></p><h2 id="2、需求检测"><a href="#2、需求检测" class="headerlink" title="2、需求检测"></a>2、需求检测</h2><p>​eg：销售T恤，想知道一件特定的T恤是否会成为畅销品。以最简单的逻辑回归为例。</p><p>​a：activation，激活。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022211456.webp" alt="image-20230802221110210"></p><p>​鉴于对单个神经元的这种描述，构建神经网络只需要将这些神经元串在一起并将它们连接在一起。</p><p>​以下复杂案例：输入为四个特征；但T恤是否畅销实际取决于 </p><ul><li>负担能力： <em><strong>affordability</strong></em>   </li><li>认可度：   <em><strong>awareness</strong></em></li><li>感知质量：   <em><strong>perceive quality</strong></em></li></ul><p>​创建一个人工神经网络来估计这件T恤被认为非常实惠的概率。其中   <em><strong>affordability</strong></em>   是   <em><strong>price</strong></em>   与  <em><strong>shipping cost</strong></em>  的函数。 这里使用一个小神经元，一个逻辑回归单元来输入   <em><strong>price</strong></em>   和  <em><strong>shipping cost</strong></em>  来预测    <em><strong>affordability</strong></em>；同理， <em><strong>awareness</strong></em>   主要是   <em><strong>marketing</strong></em>   的函数。   <em><strong>perceive quality</strong></em>   是   <em><strong>material</strong></em>   与  <em><strong>price</strong></em>   的函数。最后将三个输出的数字作为输入连接到右侧的一个逻辑回归单元的神经元，并输出这件T恤成为畅销品的概率。</p><blockquote><p><strong>中间的三个神经元组合在一起，形成层，称为隐藏层。右边的单个神经元也是一层，称为输出层。</strong></p><p>在神经网络的术语中，将 <em><strong>affordability</strong></em> 、 <em><strong>awareness</strong></em>、<em><strong>perceive quality</strong></em> 称为<strong>激活</strong>。所代表的数字为相应神经元的激活值。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308022227563.webp" alt="image-20230802222720365"></p><hr><p>​<strong>神经网络所做的是不需要我们自己手动设计特征，它可以学习其要制作的功能。</strong>尽管之前将此神经网络描述为： <em><strong>affordability</strong></em> 、 <em><strong>awareness</strong></em>、<em><strong>perceive quality</strong></em>，但神经网络应该会自行计算出它想在隐藏层中使用的特征是哪些。多层神经网络称为<strong>多层感知器</strong>。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308031701601.webp" alt="image-20230803170154420"></p><p>​<strong>当构建自己的神经网络时，需要做出的决定之一是想要多少个隐藏层以及希望每个隐藏层有多少个神经元。即神经网络要选择合适的架构。选择正确的隐藏层数和每层隐藏单元数也会对学习算法的性能产生影响。</strong></p><h2 id="3、图像感知（数字图像处理）"><a href="#3、图像感知（数字图像处理）" class="headerlink" title="3、图像感知（数字图像处理）"></a>3、图像感知（数字图像处理）</h2><p>​人脸识别的应用程序，需要训练一个神经网络将图片作为输入，人名作为输出。</p><p>​输入为百万像素亮度值的特征向量，输出为人名。</p><blockquote><p>图为吴恩达老师的老婆</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041513381.webp" alt="image-20230804151308079"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041545360.webp" alt="image-20230804154526002"></p><p>​在神经网络的最早层中，可能会发现神经元正在寻找图像中非常短的线或非常短的边缘。如果查看下一个隐藏层，会发现这些神经元可能会学习将许多小的短线和小的短边段组合在一起，以寻找面部的各个部分。当查看此示例中的下一个隐藏层时，神经网络会聚合面部的不同部分，然后尝试检测是否存在更大、更粗糙的面部形状。</p><h2 id="4、神经网络中的网络层"><a href="#4、神经网络中的网络层" class="headerlink" title="4、神经网络中的网络层"></a>4、神经网络中的网络层</h2><p>​具体数学和实现细节关于构建一个或多个神经网络层。依旧选择需求检测中的示例，放大隐藏层查看其计算。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041613506.webp" alt="image-20230804161357270"></p><ul><li><p>隐藏层输入为四个数字，三个神经元每个都是一个小逻辑回归单元。以第一个神经元为例，有两个参数    <em><strong>w</strong></em>   和   <em><strong>b</strong></em>   ，下标表示为第几个隐藏单元，   <em><strong>a</strong></em>   为激活值，下标表示第几个激活值。激活   <em><strong>a</strong></em>   向量由三个激活值构成。然后传递到输出层，成为输出层的输入。</p></li><li><p>构建多层神经网络时，为各层指定不同的数字。形成 <strong>[n]</strong>  上表，<strong>n</strong>  表示神经网络层数。</p></li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041628082.webp" alt="image-20230804162805875"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041628216.webp" alt="image-20230804162854034"></p><p>​<strong>每一层输入数字向量并对其应用一堆逻辑回归单元，然后计算另一个数字向量，然后从一层传递到另一层，直到你到达最终的输出层计算，这就是神经网络的原理。然后可以设置阈值，得出最终预测。</strong></p><h2 id="5、复杂的神经网络"><a href="#5、复杂的神经网络" class="headerlink" title="5、复杂的神经网络"></a>5、复杂的神经网络</h2><p>​为了更加清楚的了解神经网络的符号。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308041850938.webp" alt="image-20230804185043652"></p><p>​当我们描述一个神经网络为四层时，它包括输出层和所有隐藏层，不包括输入层。</p><p>​相信对我们来说，这节的知识很容易理解。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042016788.webp" alt="image-20230804201615524"></p><h2 id="6、神经网络的前向传播"><a href="#6、神经网络的前向传播" class="headerlink" title="6、神经网络的前向传播"></a>6、神经网络的前向传播</h2><p>​让我们将我们学到的东西组合成一个算法，让神经网络进行推理或预测，称为前向传播算法。</p><p>​将使用手写数字识别作为一个激励性的例子。为了简单，只区分0和1。</p><p>​分析：这是一个二位分类的问题，我们要输入图像进行分类，区分0和1。<strong>实验室可以自己玩一下。</strong></p><p>​使用8*8即64像素强度的网格或矩阵，255白色，0黑色，不同数字即灰度。鉴于64个输入特征，使用具有两个隐藏层的神经网络。其中第一个隐藏层由25个神经元，第二个隐藏层有15个神经元，最后输出层一个神经元输出这是 “ 1 ” 的概率。以下过程按照所学展开推导：</p><p>​<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042031152.webp" alt="image-20230804203118903"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042033904.webp" alt="image-20230804203301644"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042034562.webp" alt="image-20230804203430374"></p><p>​</p><p>​进行到第三层，此时的   <em><strong>a3</strong></em>  是标量，设置其阈值为4.5以得出二位分类标签。所以计算顺序先取   <em><strong>x</strong></em>   然后计算   <em><strong>a1</strong></em>   ，然后计算   <em><strong>a2</strong></em>   ，再计算   <em><strong>a3</strong></em>   ，这也是神经网络的输出。也可写为   <em><strong>f(x)</strong></em>   , 当我们学习线性回归和逻辑回归时，我们使用   <em><strong>f(x)</strong></em>  来表示线性回归或逻辑回归的输出。</p><p>​因为这个激活计算过程是从左到右进行，从  <em><strong>x</strong></em>   到   <em><strong>a1</strong></em>   、   <em><strong>a2</strong></em>   、   <em><strong>a3</strong></em>   。所以又称<strong>前向传播</strong>。<strong>这与用于学习的称为反向传播或反向传播的不同算法形成对比。</strong></p><p>​顺便说一句，这种类型的神经网络架构最初有更多的隐藏单元，然后随着离输出层越来越近，隐藏单元的数量会减少。</p><p>​<strong>以上就是前向传播算法的神经网络推理。有了这个，你就可以下载别人训练过并发布在互联网上的神经网络参数。你将能够使用他们的神经网络对你的新数据进行推理。</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202308042046047.webp" alt="image-20230804204559700"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deeplearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29142/"/>
      <url>/posts/29142/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="1、过拟合问题"><a href="#1、过拟合问题" class="headerlink" title="1、过拟合问题"></a>1、过拟合问题</h2><ul><li>模型对训练数据 <strong>欠拟合</strong> (  <em><strong>underfit</strong></em>  ) or 算法具有 <strong>高偏差</strong>（ <em><strong>high bias</strong></em> ）</li><li>希望  <em><strong>generalization</strong></em> :  <strong>泛化</strong></li><li><strong>过拟合</strong>（ <em><strong>overfitting</strong></em> ） or  <strong>高方差</strong>（ <em><strong>high variance</strong></em> ）</li></ul><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291641912.webp" alt="image-20230729164112592"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291650553.webp" alt="image-20230729165008193"></p><h2 id="2、解决过拟合问题"><a href="#2、解决过拟合问题" class="headerlink" title="2、解决过拟合问题"></a>2、解决过拟合问题</h2><ol><li>收集更多的训练集数据。</li><li>不要使用太多特征，使用特征的子集即选择一部分最合适的特征。</li><li><em><strong>Regularization</strong></em>  : 正则化</li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291657447.webp" alt="image-20230729165758247"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291658119.webp" alt="image-20230729165852938"></p><h2 id="3、正则化"><a href="#3、正则化" class="headerlink" title="3、正则化"></a>3、正则化</h2><ul><li><p>正则化是一种更<strong>温和地减少某些特征影响</strong>的方法，而不像彻底消除它那样严厉。</p></li><li><p>正则化的作用是<strong>鼓励学习算法缩小参数值</strong>，而不是把参数变为0。</p></li></ul><p>​基于正则化为学习算法设计的代价函数，通过调整代价函数来调整参数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307291711733.webp" alt="image-20230729171147458"></p><p>​<strong>正则化的典型实现方式是惩罚所有的特征，惩罚所有的 <em>w</em> 参数</strong>。最后一项可以忽略。</p><ul><li><em><strong>λ</strong></em> 设置为0，则根本没有使用正则化项。</li><li><em><strong>λ</strong></em> 设置非常大，参数非常接近0，例如对于线性回归模型则   <em><strong>f&#x3D;b</strong></em>   ，拟合水平直线，欠拟合。</li></ul><h2 id="4、用于线性回归的正则方法"><a href="#4、用于线性回归的正则方法" class="headerlink" title="4、用于线性回归的正则方法"></a>4、用于线性回归的正则方法</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292206887.webp" alt="image-20230729220649628"></p><blockquote><p><strong>注意：正则化项求偏导后无求和符号。</strong></p><p>求导是对某一项的参数求导，此时其他参数为常数。</p></blockquote><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292212627.webp" alt="image-20230729221220333"></p><h3 id="高数"><a href="#高数" class="headerlink" title="高数"></a><p align="center">高数</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292217945.webp" alt="image-20230729221710700"></p><h2 id="5、用于逻辑回归的正则化方法"><a href="#5、用于逻辑回归的正则化方法" class="headerlink" title="5、用于逻辑回归的正则化方法"></a>5、用于逻辑回归的正则化方法</h2><p><strong><p align="center">正则化逻辑与线性回归相似</p></strong></p><p>​修改代价函数实现正则化：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292222803.webp" alt="image-20230729222206533"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292224550.webp" alt="image-20230729222403346"></p><p>​<strong>完全相同的方程，除了   <em>f</em>   的定义不再是线性函数，而是应用于   <em>z</em>   的逻辑函数。</strong></p><p>​参考代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 马上学</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307292229255.webp" alt="image-20230729222925905"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29141/"/>
      <url>/posts/29141/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><p>​分类：输出变量为少数可能值中的一个，而不是无限范围内的任何数字。</p><h3 id="1-2逻辑回归（用于二元分类）"><a href="#1-2逻辑回归（用于二元分类）" class="headerlink" title="1.2逻辑回归（用于二元分类）"></a>1.2逻辑回归（用于二元分类）</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061024791.webp" alt="image-20230706102408627"></p><p>​Sigmoid函数是一个在生物学中常见的<a href="https://baike.baidu.com/item/S%E5%9E%8B%E5%87%BD%E6%95%B0/19178062?fromModule=lemma_inlink">S型函数</a>，也称为<a href="https://baike.baidu.com/item/S%E5%9E%8B%E7%94%9F%E9%95%BF%E6%9B%B2%E7%BA%BF/5581189?fromModule=lemma_inlink">S型生长曲线</a>。 在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的<a href="https://baike.baidu.com/item/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/2520792?fromModule=lemma_inlink">激活函数</a>，将变量映射到0,1之间。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061059720.webp" alt="image-20230706105911494"></p><p>​<strong>多变量整合为单变量，再用sigmoid分类</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307061104571.webp" alt="image-20230706110452346"></p><h3 id="1-3逻辑回归、决策边界"><a href="#1-3逻辑回归、决策边界" class="headerlink" title="1.3逻辑回归、决策边界"></a>1.3逻辑回归、决策边界</h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081441602.webp" alt="image-20230708144100042"></p><h3 id="2-1逻辑回归中的代价函数"><a href="#2-1逻辑回归中的代价函数" class="headerlink" title="2.1逻辑回归中的代价函数"></a>2.1逻辑回归中的代价函数</h3><p>​<strong>代价函数提供了一种衡量特定参数与训练数据拟合程度的方法</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081448958.webp" alt="image-20230708144831744"></p><hr><p>​<strong>单个训练示例的损失函数</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101450133.webp" alt="image-20230710145008957"></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081510170.webp" alt="image-20230708151030003"></p><p>​<em>f</em>  的值总是在0-1之间，越接近于1，损失越小。<strong>上图损失函数漏掉了一个负号</strong></p><hr><p>​<strong>代价函数是整个训练集的函数，因此是单个训练示例的损失函数之和的平均值</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307081519147.webp" alt="image-20230708151933785"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost_logistic</span>(<span class="params">X, y, w, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes cost</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)) : target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)) : model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)       : model parameter</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      cost (scalar): cost</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">m = X.shape[<span class="number">0</span>]</span><br><span class="line">cost = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">    z_i = np.dot(X[i],w) + b</span><br><span class="line">    f_wb_i = sigmoid(z_i)</span><br><span class="line">    cost +=  -y[i]*np.log(f_wb_i) - (<span class="number">1</span>-y[i])*np.log(<span class="number">1</span>-f_wb_i)</span><br><span class="line">         </span><br><span class="line">cost = cost / m</span><br><span class="line"><span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="2-2简化逻辑回归代价函数"><a href="#2-2简化逻辑回归代价函数" class="headerlink" title="2.2简化逻辑回归代价函数"></a>2.2简化逻辑回归代价函数</h3><p>​目的：使用梯度下降来拟合逻辑回归模型的参数时，实现简单。</p><p>​二元<strong>交叉熵损失函数</strong>，多分类只需要多乘几个多项式即可</p><p> <img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101459266.webp" alt="image-20230710145927025"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307101504651.webp" alt="image-20230710150413331"></p><p>​这个特定的成本函数是使用最大似然估计的统计原理推导出来的，这是统计学中关于如何有效的找到不同模型的参数的想法。<strong>它有一个很好的特性，它是凸的。</strong></p><h3 id="3-1实现梯度下降"><a href="#3-1实现梯度下降" class="headerlink" title="3.1实现梯度下降"></a>3.1实现梯度下降</h3><p>​为了拟合逻辑回归模型的参数，将尝试找到使 <em>w</em> 和 <em>b</em> 的成本  <em>J</em>  最小的参数值，使用梯度下降来实现。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121410281.webp" alt="image-20230712141007965"></p><p>​<strong>与线性回归模型类似，注意同时更新。</strong></p><p>推导过程：<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121500696.webp" alt="image-20230712150021478"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121519309.webp" alt="image-20230712151912073"></p><p>​<strong>尽管线性回归和逻辑回归所写的算法看起来相同，但实际上它们是两种截然不同的算法，因为 <em>f</em> 的定义不同。</strong></p><p>​<strong>ps：上述逻辑回归的 <em>f</em> 的负号位置错了</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202307121526385.webp" alt="image-20230712152606243"></p><hr><p>​<strong>梯度函数实现代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_logistic</span>(<span class="params">X, y, w, b</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the gradient for linear regression </span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n): Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,)): target values</span></span><br><span class="line"><span class="string">      w (ndarray (n,)): model parameters  </span></span><br><span class="line"><span class="string">      b (scalar)      : model parameter</span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. </span></span><br><span class="line"><span class="string">      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    m,n = X.shape</span><br><span class="line">    dj_dw = np.zeros((n,))                           <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        f_wb_i = sigmoid(np.dot(X[i],w) + b)          <span class="comment">#(n,)(n,)=scalar</span></span><br><span class="line">        err_i  = f_wb_i  - y[i]                       <span class="comment">#scalar</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      <span class="comment">#scalar</span></span><br><span class="line">        dj_db = dj_db + err_i</span><br><span class="line">    dj_dw = dj_dw/m                                   <span class="comment">#(n,)</span></span><br><span class="line">    dj_db = dj_db/m                                   <span class="comment">#scalar</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dj_db, dj_dw  </span><br></pre></td></tr></table></figure><p>​<strong>检查梯度函数的实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_tmp = np.array([[<span class="number">0.5</span>, <span class="number">1.5</span>], [<span class="number">1</span>,<span class="number">1</span>], [<span class="number">1.5</span>, <span class="number">0.5</span>], [<span class="number">3</span>, <span class="number">0.5</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2.5</span>]])</span><br><span class="line">y_tmp = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">w_tmp = np.array([<span class="number">2.</span>,<span class="number">3.</span>])</span><br><span class="line">b_tmp = <span class="number">1.</span></span><br><span class="line">dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dj_db: <span class="subst">&#123;dj_db_tmp&#125;</span>&quot;</span> )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;dj_dw: <span class="subst">&#123;dj_dw_tmp.tolist()&#125;</span>&quot;</span> )</span><br></pre></td></tr></table></figure><p>​<strong>结果为：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dj_db: <span class="number">0.49861806546328574</span></span><br><span class="line">dj_dw: [<span class="number">0.498333393278696</span>, <span class="number">0.49883942983996693</span>]</span><br></pre></td></tr></table></figure><hr><p>​<strong>梯度下降代码:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w_in, b_in, alpha, num_iters</span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Performs batch gradient descent</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      X (ndarray (m,n)   : Data, m examples with n features</span></span><br><span class="line"><span class="string">      y (ndarray (m,))   : target values</span></span><br><span class="line"><span class="string">      w_in (ndarray (n,)): Initial values of model parameters  </span></span><br><span class="line"><span class="string">      b_in (scalar)      : Initial values of model parameter</span></span><br><span class="line"><span class="string">      alpha (float)      : Learning rate</span></span><br><span class="line"><span class="string">      num_iters (scalar) : number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      w (ndarray (n,))   : Updated values of parameters</span></span><br><span class="line"><span class="string">      b (scalar)         : Updated value of parameter </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># An array to store cost J and w&#x27;s at each iteration primarily for graphing later</span></span><br><span class="line">    J_history = []</span><br><span class="line">    w = copy.deepcopy(w_in)  <span class="comment">#avoid modifying global w within function</span></span><br><span class="line">    b = b_in</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        <span class="comment"># Calculate the gradient and update the parameters</span></span><br><span class="line">        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   </span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update Parameters using w, b, alpha and gradient</span></span><br><span class="line">        w = w - alpha * dj_dw               </span><br><span class="line">        b = b - alpha * dj_db               </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Save cost J at each iteration</span></span><br><span class="line">        <span class="keyword">if</span> i&lt;<span class="number">100000</span>:      <span class="comment"># prevent resource exhaustion </span></span><br><span class="line">            J_history.append( compute_cost_logistic(X, y, w, b) )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print cost every at intervals 10 times or as many iterations if &lt; 10</span></span><br><span class="line">        <span class="keyword">if</span> i% math.ceil(num_iters / <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Iteration <span class="subst">&#123;i:4d&#125;</span>: Cost <span class="subst">&#123;J_history[-<span class="number">1</span>]&#125;</span>   &quot;</span>)</span><br><span class="line">    </span><br><span class="line">     <span class="keyword">return</span> w, b, J_history         <span class="comment">#return final w,b and J history for graphing</span></span><br></pre></td></tr></table></figure><p>​<strong>在数据集上运行：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_tmp  = np.zeros_like(X_train[<span class="number">0</span>])</span><br><span class="line">b_tmp  = <span class="number">0.</span></span><br><span class="line">alph = <span class="number">0.1</span></span><br><span class="line">iters = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nupdated parameters: w:<span class="subst">&#123;w_out&#125;</span>, b:<span class="subst">&#123;b_out&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>​<strong>结果：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Iteration    <span class="number">0</span>: Cost <span class="number">0.684610468560574</span>   </span><br><span class="line">Iteration <span class="number">1000</span>: Cost <span class="number">0.1590977666870457</span>   </span><br><span class="line">Iteration <span class="number">2000</span>: Cost <span class="number">0.08460064176930078</span>   </span><br><span class="line">Iteration <span class="number">3000</span>: Cost <span class="number">0.05705327279402531</span>   </span><br><span class="line">Iteration <span class="number">4000</span>: Cost <span class="number">0.04290759421682</span>   </span><br><span class="line">Iteration <span class="number">5000</span>: Cost <span class="number">0.03433847729884557</span>   </span><br><span class="line">Iteration <span class="number">6000</span>: Cost <span class="number">0.02860379802212006</span>   </span><br><span class="line">Iteration <span class="number">7000</span>: Cost <span class="number">0.02450156960879306</span>   </span><br><span class="line">Iteration <span class="number">8000</span>: Cost <span class="number">0.02142370332569295</span>   </span><br><span class="line">Iteration <span class="number">9000</span>: Cost <span class="number">0.019030137124109114</span>   </span><br><span class="line"></span><br><span class="line">updated parameters: w:[<span class="number">5.28</span> <span class="number">5.08</span>], b:-<span class="number">14.222409982019837</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29140/"/>
      <url>/posts/29140/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="1-多维特征"><a href="#1-多维特征" class="headerlink" title="1.多维特征"></a>1.多维特征</h2><p><strong>多元线性回归模型</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100942878.webp" alt="image-20230610094203639"></p><hr><hr><h2 id="2-矢量化"><a href="#2-矢量化" class="headerlink" title="2.矢量化"></a>2.矢量化</h2><p><strong>缩短代码，提高运行效率</strong></p><p><strong>NumPy</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306100951696.webp" alt="image-20230610095105404"></p><p><strong>用空间换时间，用连续的结构可以省去查找数据的时间</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002307.webp" alt="image-20230610100241038"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101002970.webp" alt="image-20230610100256599"></p><hr><hr><h2 id="3-用于多元线性回归的梯度下降算法"><a href="#3-用于多元线性回归的梯度下降算法" class="headerlink" title="3.用于多元线性回归的梯度下降算法"></a>3.用于多元线性回归的梯度下降算法</h2><p><strong>带有矢量化的多元线性回归实现梯度下降</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101013171.webp" alt="image-20230610101334864"></p><p><strong>核心</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306101018363.webp" alt="image-20230610101851161"></p><hr><hr><h2 id="4-特征缩放-数据预处理"><a href="#4-特征缩放-数据预处理" class="headerlink" title="4.特征缩放(数据预处理)"></a>4.特征缩放(数据预处理)</h2><p><strong>归一化</strong> ：拥有不同特征时，它们得取值范围非常不同时，可能会导致梯度下降运行缓慢，重新缩放不同得特征，使它们都具有可比较的取值范围，效果更显著。</p><ol><li><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110919219.webp" alt="image-20230611091955007"></li><li>均值归一化：特征值减平均值（样本平均值）再除以方差<img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110925805.webp" alt="image-20230611092559538"></li><li>Z-score 归一化：需要计算每个特征的标准差。<strong>概率论</strong></li></ol><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110938294.webp" alt="image-20230611093829964"></p><p><strong>按需缩放</strong>：在一个数量级上的特征可以不考虑缩放</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306110941080.webp" alt="image-20230611094136864"></p><h2 id="5-判断梯度下降是否收敛"><a href="#5-判断梯度下降是否收敛" class="headerlink" title="5.判断梯度下降是否收敛"></a>5.判断梯度下降是否收敛</h2><p>如果有 <em>J</em> 在一次迭代后增加，意味着 Alpha选择不当，通常意味着Alpha太大，或者代码中可能存在错误。</p><ol><li><strong>迭代次数</strong>，创建学习曲线尝试找出。</li><li>自动收敛测试：假设 <em>epsilon</em> 是一个代表小数变量，例如 0.001 或 10^-3。如果成本 <em>J</em> 在一次迭代中减少的幅度小于这个数字 <em>epsilon</em> ，那么曲线可能趋于平坦。</li></ol><h2 id="6-选择合适的学习率"><a href="#6-选择合适的学习率" class="headerlink" title="6.选择合适的学习率"></a>6.选择合适的学习率</h2><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120950291.webp" alt="image-20230612095038996"></p><hr><p>技巧：如果学习率足够小，成本函数应该在每次迭代中减少。通过将Alpha设置为一个非常小的数字，看看这是否会导致每次迭代时成本降低。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306120956944.webp" alt="image-20230612095616640"></p><p><strong>省流：凭感觉去试，调参。</strong></p><h2 id="7-特征工程"><a href="#7-特征工程" class="headerlink" title="7.特征工程"></a>7.特征工程</h2><p><strong>特征衍生</strong>：通常通过转换或组合原始特征来使学习算法更容易做出准确的预测。</p><p><strong>多项式回归</strong></p><p><strong>Scikit-learn</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121042502.webp" alt="image-20230612104233179"></p><p>不仅可以拟合直线，还可以拟合曲线、非线性函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306121050229.webp" alt="image-20230612105041878">、</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/posts/29139/"/>
      <url>/posts/29139/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><p align="center">机器学习</p></h1><h2 id="两种主要类型"><a href="#两种主要类型" class="headerlink" title="两种主要类型"></a><p align="left">两种主要类型</p></h2><h3 id="1-监督学习-Supervised-Learning"><a href="#1-监督学习-Supervised-Learning" class="headerlink" title="1.监督学习(Supervised Learning)"></a><p align="left">1.监督学习(Supervised Learning)</p></h3><p><img src="https://api2.mubu.com/v3/document_image/3413a5cb-b522-42e8-ae40-3a988a00321d-12774614.jpg" alt="image"></p><p>1、Regression-回归：从无限多种可能输出数字中预测数字。</p><p>2、Classfication-分类：预测类别，可能的输出都是一小组。</p><hr><h3 id="2-无监督学习-Unsupervised-Learning"><a href="#2-无监督学习-Unsupervised-Learning" class="headerlink" title="2.无监督学习(Unsupervised Learning)"></a><p align="left">2.无监督学习(Unsupervised Learning)</p></h3><p>没有试图监督算法而为了给每个输入提供一些正确的答案，相反，为了弄清数据中有什么模式或者结构。</p><p><img src="https://api2.mubu.com/v3/document_image/ab575484-7afb-4726-b350-0697e0a4cf12-12774614.jpg" alt="image"></p><p>1、Clustering-聚类：获取没有标签的数据并尝试将他们自动分组到集群中。例如相似推荐，就是把相似的内容归类后处理。</p><p>2、Anomaly detection-异常检测：用于检测异常事件。</p><p>3、Dimensionality reduction-降维：压缩大数据集。</p><hr><h3 id="3-线性回归模型-linear-regression"><a href="#3-线性回归模型-linear-regression" class="headerlink" title="3.线性回归模型(linear regression)"></a><p align="left">3.线性回归模型(linear regression)</p></h3><p>Training set-训练集、features-输入变量 <em>x</em>（特征或输入特征）、targets-目标变量  <em>y</em></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021634673.webp" alt="image-20230602163455564"></p><p>Univariate linear regression-单变量线性回归</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021647760.webp" alt="image-20230602164734648"></p><p><strong>1.Cost function-代价函数</strong></p><p>平方误差代价函数</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021702346.webp" alt="image-20230602170247191"></p><p>如何使用代价函数为模型找到最佳参数？</p><p>使  <em>J</em>  越小越好。  </p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021719541.webp" alt="image-20230602171953431"></p><p>先找最优的权重 <em>w</em> ，令 <em>b</em> 为 0。做代价函数图—二维</p><p><strong>多元函数求极值的问题。</strong></p><p>可视化代价函数：</p><p>回到原始问题，<em>b</em> 不为 0 时：</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306021801888.webp" alt="image-20230602180127719"></p><hr><h3 id="4-梯度下降（Gradient-descent）"><a href="#4-梯度下降（Gradient-descent）" class="headerlink" title="4.梯度下降（Gradient descent）"></a><p align="left">4.梯度下降（Gradient descent）</p></h3><p>高效算法：代码编写自动找到 <em>w</em> 和 <em>b</em>，实现最好的拟合。</p><p>梯度下降是一种尝试最小化任何函数的方法。而不局限于线性回归的成本函数。</p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031154023.webp" alt="image-20230603115414788"></p><p><strong>同时更新参数（代码顺序）</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306031745415.webp" alt="image-20230603174503106"></p><p><strong>学习率</strong></p><p>太小：下降步幅小，速度慢。</p><p>太大：步幅大，但可能会使结果更糟，在最低点附近震荡，过充，甚至离最低点越来越远，发散。</p><hr><h3 id="5-线性回归算法"><a href="#5-线性回归算法" class="headerlink" title="5.线性回归算法"></a><p align="left">5.线性回归算法</p></h3><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306071640865.webp" alt="image-20230607164007657"></p><hr><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081554888.webp" alt="image-20230608155457007"></p><p><strong>使用线性回归的平方误差成本函数时，成本函数永远不会有多个局部最小值。凸函数</strong></p><hr><h3 id="6-运行梯度下降"><a href="#6-运行梯度下降" class="headerlink" title="6.运行梯度下降"></a><p align="left">6.运行梯度下降</p></h3><p><strong>C1_W2_Lab03_Gradient_Descent_Soln</strong></p><p><img src="https://blog-1318483047.cos.ap-nanjing.myqcloud.com/img%20/202306081609512.webp" alt="image-20230608160911171"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/posts/29144/"/>
      <url>/posts/29144/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
